<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><link rel="canonical" href="https://www.biosconfessions.com/collections/finding-p-values-and-false-hope/"><meta name="generator" content="Astro v5.16.0"><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-E96HPDSNG8">
        </script><script>
            window.dataLayer = window.dataLayer || [];
            window.gtag = function gtag(){ window.dataLayer.push(arguments); };

            window.gtag("js", new Date());
            window.gtag("consent", "default", {
                ad_storage: "denied",
                ad_user_data: "denied",
                ad_personalization: "denied",
                analytics_storage: "denied",
            });

            window.gtag("config", "G-E96HPDSNG8");
        </script><!-- Global Metadata --><title>Collection | BIOS Confessions</title><meta name="author" content="Quinn &#38; Gianni"><meta name="title" content="Collection | BIOS Confessions"><meta name="description" content="Clear, structured explanations of core CS topics, from fundamentals to advanced principles. Learn for free, at your own pace."><meta name="keywords" content><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Collection | BIOS Confessions" href="https://www.biosconfessions.com/rss.xml"><!-- Open Graph / Facebook --><meta property="og:title" content="Collection | BIOS Confessions"><meta property="og:description" content="Clear, structured explanations of core CS topics, from fundamentals to advanced principles. Learn for free, at your own pace."><meta property="og:type" content="website"><meta property="og:url" content="https://www.biosconfessions.com/collections/finding-p-values-and-false-hope/"><meta property="og:image" content="https://www.biosconfessions.com/assets/blog-placeholder.jpg"><!-- Twitter --><meta property="twitter:title" content="Collection | BIOS Confessions"><meta property="twitter:description" content="Clear, structured explanations of core CS topics, from fundamentals to advanced principles. Learn for free, at your own pace."><meta property="twitter:url" content="https://www.biosconfessions.com/collections/finding-p-values-and-false-hope/"><meta property="twitter:image" content="https://www.biosconfessions.com/assets/blog-placeholder.jpg"><meta property="twitter:card" content="summary_large_image"><link rel="stylesheet" href="/assets/index.BtD-1wnE.css"></head> <body class="bg-silicon-100 scroll-smooth flex flex-col min-h-screen">  <header id="header" class="py-3" role="banner"> <div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-1 w-full">  <nav class="flex flex-wrap lg:grid lg:grid-cols-12 basis-full items-center" aria-label="Primary navigation"> <!-- Logo --> <div class="lg:col-span-3 flex items-center"> <a href="/" class="flex-none rounded-xl text-2xl inline-block font-bold focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400" aria-label="BIOS Confessions homepage"> <img src="/logo.svg" alt="BIOS Confessions Logo" class="h-5 w-auto"> </a> </div> <!-- Button Group --> <div class="flex items-center gap-x-1 lg:gap-x-2 ms-auto py-1 lg:ps-6 lg:order-3 lg:col-span-3"> <!-- Search Button --> <button type="button" id="search-button" class="size-9.5 relative flex justify-center items-center rounded-xl bg-silicon-50 border border-silicon-200 text-black hover:bg-silicon-0 focus:outline-hidden focus:bg-silicon-0 disabled:opacity-50 disabled:pointer-events-none" aria-label="Open search"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2.5" stroke="currentColor" class="size-4" data-slot="icon" aria-hidden="true"> <path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path> </svg> </button> <!-- Mobile Menu Toggle --> <div class="lg:hidden"> <button type="button" id="collapse-button" class="size-9.5 flex justify-center items-center text-sm font-semibold rounded-xl bg-silicon-50 border border-silicon-200 text-black hover:bg-silicon-0 focus:outline-hidden focus:bg-silicon-0 disabled:opacity-50 disabled:pointer-events-none" aria-expanded="false" aria-controls="primary-menu" aria-label="Toggle navigation"> <!-- Hamburger Icon --> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" stroke="currentColor" class="size-5" data-slot="icon" aria-hidden="true"> <line x1="3" x2="21" y1="6" y2="6"></line> <line x1="3" x2="21" y1="12" y2="12"></line> <line x1="3" x2="21" y1="18" y2="18"></line> </svg> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" stroke="currentColor" class="hidden size-5" data-slot="icon" aria-hidden="true"> <path d="M18 6 6 18"></path> <path d="m6 6 12 12"></path> </svg> </button> </div> </div> <!-- Collapsible Navigation --> <div id="collapse" class="hidden overflow-hidden transition-all duration-300 basis-full grow lg:block lg:w-auto lg:basis-auto lg:order-2 lg:col-span-6" role="region" aria-label="Main menu" aria-hidden="true"> <ul id="primary-menu" class="flex flex-col gap-y-4 gap-x-0 mt-5 lg:flex-row lg:justify-center lg:items-center lg:gap-y-0 lg:gap-x-7 lg:mt-0" role="menubar"> <li role="none"> <a href="/" role="menuitem" class="inline-block text-black focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400 hover:text-gray-600 focus:text-gray-600"> Home </a> </li><li role="none"> <a href="/posts" role="menuitem" class="inline-block text-black focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400 hover:text-gray-600 focus:text-gray-600"> Posts </a> </li><li role="none"> <a href="/about" role="menuitem" class="inline-block text-black focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400 hover:text-gray-600 focus:text-gray-600"> Hello World </a> </li> </ul> </div> </nav>  </div> </header> <script type="module">document.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector("#collapse-button"),t=document.querySelector("#collapse");if(!e||!t)return;const[n,d]=e.querySelectorAll("svg");e.addEventListener("click",()=>{const o=e.getAttribute("aria-expanded")==="true";e.setAttribute("aria-expanded",String(!o)),t.classList.toggle("hidden"),n.classList.toggle("hidden"),d.classList.toggle("hidden")}),document.querySelector("#search-button")?.addEventListener("click",()=>{window.dispatchEvent(new Event("openModal"))})});</script> <a href="#main" aria-label="Skip to Main content" aria-disabled="false" tabindex="0" class="sr-only focus:not-sr-only focus:absolute focus:top-2 focus:left-2 focus:py-3 focus:px-6 text-silicon-50 hover:text-blue-600 bg-blue-600 hover:bg-transparent border-2 border-blue-600 rounded-xl font-medium py-3 px-6 transition duration-200 false">  
Skip to Main content
  </a> <main id="main" class="shrink-0" role="main" tabindex="-1"> <section id="title" class="mt-20" aria-labelledby="title-heading"> <div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-1 w-full">  <div class="text-center"> <h1 id="title-heading" class="font-bold text-3xl sm:text-4xl mb-3"> Collection: Finding P values and false hope </h1> <p class="text-gray-600 text-lg">All posts in collection: &quot;finding-p-values-and-false-hope&quot;</p> </div>  </div> </section>  <section id="posts" class="mb-20 mt-5" aria-labelledby="posts-heading"> <div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-1 w-full">  <h2 id="posts-heading" class="sr-only">Post list</h2> <div class="grid sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-6"> <article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/quantitative-finance-introduction-python" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-quantitative-finance-introduction-python"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/banner-p-values-and-false-hope.BbDvJEO1_Z2q0e9e.webp" alt="Banner for the Introduction" decoding="async" loading="eager" fetchpriority="auto" width="1024" height="1024" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="November 9, 2025" class="text-gray-500 text-xs inline-block"> November 9, 2025 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-quantitative-finance-introduction-python" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> Finding P Values and False Hope: Intro to Quantitative Finance </h3> <p class="text-gray-600 text-sm line-clamp-3"> Explore the intersection of math, stats, and Python in finance. Learn interest rates, compounding, risk management, and bond pricing with practical code examples. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article><article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/risk-management-variance-decisions-under-uncertainty" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-risk-management-variance-decisions-under-uncertainty"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/banner-art-of-losing-slowely.DBSc2GqA_ZJwy46.webp" alt="Banner for The Art of Losing Slowly" decoding="async" loading="eager" fetchpriority="auto" width="1584" height="672" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="November 23, 2025" class="text-gray-500 text-xs inline-block"> November 23, 2025 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-risk-management-variance-decisions-under-uncertainty" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> The Art of Losing Slowly: How to Manage Risk, Variance and Decisions Under Uncertainty </h3> <p class="text-gray-600 text-sm line-clamp-3"> Master risk management through the lens of variance and expected value. Learn how to size bets, diversify risk, and implement decision-making logic in Python. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article><article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/loans-bonds-amortization-default-risk-python" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-loans-bonds-amortization-default-risk-python"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/loans-and-bonds-explained.cwsEEMkV_2g0xIG.webp" alt="Illustration explaining how loans and bonds work, including interest, repayment, and default risk" decoding="async" loading="eager" fetchpriority="auto" width="1344" height="768" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="December 7, 2025" class="text-gray-500 text-xs inline-block"> December 7, 2025 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-loans-bonds-amortization-default-risk-python" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> Loans and Bonds: The Mechanics of Borrowing and Default Risk </h3> <p class="text-gray-600 text-sm line-clamp-3"> A quantitative guide to loan amortization and bond pricing. Learn to derive payment formulas, simulate default risk, and understand coupon rates using Python. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article><article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/cdo-tranching-senior-mezzanine-equity-python" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-cdo-tranching-senior-mezzanine-equity-python"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/tranching-in-collaterized-debt.h8tTybnR_Z17XDzs.webp" alt="Illustration explaining how loans and bonds work, including interest, repayment, and default risk" decoding="async" loading="eager" fetchpriority="auto" width="1584" height="672" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="December 21, 2025" class="text-gray-500 text-xs inline-block"> December 21, 2025 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-cdo-tranching-senior-mezzanine-equity-python" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> Life in the Tranches: Understanding CDOs and Risk Waterfalls </h3> <p class="text-gray-600 text-sm line-clamp-3"> A deep dive into Collateralized Debt Obligations (CDOs). Learn how tranching redistributes credit risk into Senior, Mezzanine, and Equity layers with a Python waterfall simulation. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article><article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/yield-curves-bootstrapping-forward-rates-python" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-yield-curves-bootstrapping-forward-rates-python"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/yield-curves-and-forward-rates-explained.Dl5Ef4Qg_ZcEciW.webp" alt="Illustration explaining yield curves and forward rates" decoding="async" loading="eager" fetchpriority="auto" width="1808" height="592" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="January 4, 2026" class="text-gray-500 text-xs inline-block"> January 4, 2026 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-yield-curves-bootstrapping-forward-rates-python" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> The Geometry of Interest: Constructing Yield Curves and Forward Rates </h3> <p class="text-gray-600 text-sm line-clamp-3"> Master the term structure of interest rates. Learn to build yield curves using bootstrapping and interpolation, and derive forward rates with practical Python examples. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article><article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/stock-analysis-python-prices-returns-distribution" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-stock-analysis-python-prices-returns-distribution"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/understanding-stock-prices-banner.eRZSuP4F_sUSRO.webp" alt="Banner illustration displaying financial stock charts and data analysis trends" decoding="async" loading="eager" fetchpriority="auto" width="3168" height="1344" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="January 18, 2026" class="text-gray-500 text-xs inline-block"> January 18, 2026 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-stock-analysis-python-prices-returns-distribution" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> Stock Analysis with Python: Modeling Prices, Returns &amp; Distributions </h3> <p class="text-gray-600 text-sm line-clamp-3"> Learn how to analyze stock market data using Python. This guide covers calculating daily returns, visualizing volatility, and modeling statistical distributions with yfinance. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article><article class="group flex flex-1 flex-col focus:outline-hidden" role="listitem"> <a href="/posts/quantitative-portfolio-allocation-markowitz-efficient-frontier" class="flex flex-col h-full rounded-xl" aria-labelledby="post-title-quantitative-portfolio-allocation-markowitz-efficient-frontier"> <div class="relative pt-[50%] sm:pt-[70%] rounded-xl overflow-hidden mb-3 shadow"> <img src="/assets/portfolio-allocation-banner.CFnovJea_1HumwJ.webp" alt="Robot explaining finance with statistics" decoding="async" loading="eager" fetchpriority="auto" width="1024" height="1024" class="size-full absolute top-0 start-0 rounded-xl object-cover group-hover:scale-105 group-focus:scale-105 transition-transform duration-500 ease-in-out"> </div> <div class="sm:px-3 mb-2"> <div class="flex items-center gap-2 mb-1"> <time datetime="February 1, 2026" class="text-gray-500 text-xs inline-block"> February 1, 2026 </time> <span class="bg-yellow-200 text-yellow-800 text-xs font-medium px-2.5 py-0.5 rounded-full"> Intermediate </span> </div> <h3 id="post-title-quantitative-portfolio-allocation-markowitz-efficient-frontier" class="text-base font-semibold text-gray-900 group-hover:text-gray-600"> Portfolio Allocation: The Math Behind the Efficient Frontier </h3> <p class="text-gray-600 text-sm line-clamp-3"> Master the quantitative core of Modern Portfolio Theory. Learn how to use Markowitz optimization and Python to build an efficient frontier for asset allocation. </p> </div> <div class="sm:px-3 mt-auto inline-flex items-center gap-x-1 text-sm text-blue-600 decoration-2 group-hover:underline group-focus:underline font-medium">
Read more
<svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20" class="size-5" data-slot="icon" aria-hidden="true"> <path d="M8.22 5.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.75.75 0 0 1-1.06-1.06L11.94 10 8.22 6.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd" fill-rule="evenodd"></path> </svg> </div> </a> </article> </div>  </div> </section> <section id="pagination"> <div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-1 w-full">  <div class="flex items-center justify-between">  <div class="hidden sm:flex sm:flex-1 sm:items-center sm:justify-between"> <div> <p class="text-sm text-gray-700">
Showing
<span class="font-medium">1</span>
-
<span class="font-medium">7</span>
of
<span class="font-medium">7</span>
results
</p> </div>  </div> </div>  </div> </section>  </main> <footer id="footer" class="mt-auto text-gray-600" role="contentinfo"> <div class="mx-auto max-w-6xl px-4 sm:px-6 lg:px-8 py-1 w-full">   <div class="flex flex-col sm:flex-row justify-between gap-4 py-6"> <!-- Site description --> <section aria-labelledby="footer-about" class="sm:max-w-2/5"> <h2 id="footer-about" class="font-bold text-gray-700 mb-2">
BIOS Confessions
</h2> <p class="text-sm text-gray-500">
Our mission is to help curious minds grow by learning,
                    experimenting, and sharing knowledge in the world of
                    computer science.
</p> </section> <!-- Navigation --> <nav aria-labelledby="footer-navigation"> <h2 id="footer-navigation" class="font-bold text-gray-700">
Navigation
</h2> <ul class="mt-3 space-y-3 text-sm" role="list"> <li> <a href="/" class="inline-flex gap-x-2 text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400"> Home </a> </li><li> <a href="/posts" class="inline-flex gap-x-2 text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400"> Posts </a> </li><li> <a href="/about" class="inline-flex gap-x-2 text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400"> About </a> </li><li> <a href="/search" class="inline-flex gap-x-2 text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400"> Search </a> </li><li> <a href="/tags" class="inline-flex gap-x-2 text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400"> Tags </a> </li><li> <a href="/collections" class="inline-flex gap-x-2 text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400"> Collections </a> </li> </ul> </nav> <!-- Contact --> <section aria-labelledby="footer-contact"> <h2 id="footer-contact" class="font-bold text-gray-700 mb-2">
Contact
</h2> <a href="mailto:biosconfessions@gmail.com" class="text-sm text-gray-500 hover:text-gray-800 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400">
biosconfessions@gmail.com
</a> </section> </div>  <div class="border-t border-silicon-200 py-6"> <div class="flex flex-col sm:flex-row justify-center sm:justify-between items-center gap-4"> <p class="text-xs text-gray-600">
&copy; 2026 BIOS Confessions.
</p> <div class="flex gap-4"> <a href="/privacy-statement" class="text-xs text-gray-500 underline hover:text-gray-800 hover:decoration-2 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400">
Privacy Statement
</a> <button type="button" class="text-xs text-gray-500 underline hover:text-gray-800 hover:decoration-2 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-400" data-cc="show-preferencesModal">
Cookie Settings
</button> </div> </div> </div>  </div> </footer>  <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).only=e;window.dispatchEvent(new Event("astro:only"));})();</script><script>(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="1ms2kW" component-url="/assets/SearchModal.CBqktBRN.js" component-export="default" renderer-url="/assets/client.BfPWZUkF.js" props="{&quot;posts&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;data-science-with-python/1-setup.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;setting-up-python-data-science-environment-vscode-colab&quot;],&quot;title&quot;:[0,&quot;The Lab Setup: Preparing Your Local and Cloud Python Environment&quot;],&quot;description&quot;:[0,&quot;A beginner-friendly guide to setting up a professional data science environment. Learn how to configure VS Code, manage virtual environments with venv, and leverage Google Colab for cloud computing.&quot;],&quot;collection&quot;:[0,&quot;Data Science with Python&quot;],&quot;chapter&quot;:[0,1],&quot;shortname&quot;:[0,&quot;Setup&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2026-01-11T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;VS Code&quot;],[0,&quot;Jupyter&quot;],[0,&quot;Google Colab&quot;],[0,&quot;Virtual Environments&quot;],[0,&quot;pip&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;python data science environment setup&quot;],[0,&quot;how to use jupyter notebooks in vs code&quot;],[0,&quot;creating a virtual environment for data science&quot;],[0,&quot;google colab tutorial for beginners&quot;],[0,&quot;installing pandas and numpy with pip&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/data-science-with-python.D2zPcuIT.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for Data Science with Python showing Python and data analysis tools&quot;],&quot;update&quot;:[3,&quot;2026-01-11T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Before we dive into data science with Python, we need a solid place to write and run our code. \n\nIn this chapter, we&#39;ll set up:\n\n- A **local environment** on your computer\n- A **cloud environment** using Google Colaboratory\n\nBoth approaches are valuable, and you’ll likely switch between them depending on your project.\n\n## Local Environment\n\nWe’ll start by preparing our local machine for Python development. If you haven’t already, make sure to download and install the latest version of Python from the official site: https://www.python.org/downloads/ \n\nMake sure to check the option **“Add Python to PATH”** during installation if you&#39;re on Windows. To verify Python is installed correctly, open a terminal on your computer and type the following command:\n\n```bash\npython --version\n```\n\nYou should see a version number like `Python 3.x.x`\n\n### Install Visual Studio Code\n\nA powerful and lightweight editor, **VS Code** is one of the most popular tools for Python and data science.\n\n1. Download it from the official website: https://code.visualstudio.com/\n2. Open VS Code and navigate to the **Extensions** panel (the icon with four squares).\n3. Install the **Python extension** (search for: `ms-python.python`)\n4. Install the **Jupyter extension** (search for: `ms-toolsai.jupyter`)\n\n### Create a Project Folder\n\nBefore creating a virtual environment, start by setting up a project folder:\n\n1. Create a new folder anywhere on your system (e.g., `my-first-ds-project`).\n2. In VS Code, go to **File → Open Folder** and select the folder.\n3. Open a terminal inside VS Code with **Ctrl + J** (or **View → Terminal**).\n\n### Virtual Environments (venv)\n\nA virtual environment isolates your project’s Python packages so they don’t interfere with other projects.\n\nIn the VS Code terminal, create a virtual environment:\n\n```bash\npython -m venv venv\n```\n\nActivate it:\n\n- **Windows:**\n    \n    ```bash\n    venv\\Scripts\\activate\n    ```\n    \n- **Mac/Linux:**\n    \n    ```bash\n    source venv/bin/activate\n    ```\n    \n\nOnce activated, you should see **`(venv)`** at the beginning of your terminal line. This confirms you are successfully operating within your isolated project environment.\n\nTo deactivate the venv is for both Windows and Mac/Linux the same, type the following in your terminal:\n\n```bash\ndeactivate\n```\n\n### Installing Packages\n\nWith our virtual environment active, we can install the core libraries for our data science project with Python&#39;s package manager (`pip`):\n\n```bash\npip install numpy pandas matplotlib seaborn scikit-learn scipy\n```\n\n- **`numpy`**: For fast numerical computations (especially arrays).\n- **`pandas`**: For efficient data handling and analysis (DataFrames).\n- **`matplotlib`**: For creating static, interactive, and animated visualisations.\n- `seaborn`: For high-level, aesthetically pleasing, and informative statistical data visualisations built on top of Matplotlib.\n- **`scikit-learn`**: The most popular library for machine learning algorithms.\n- `scipy`:  For scientific and technical computing, including optimisation, statistics, linear algebra, and signal processing.\n\nThese libraries form the core of most data science projects.\n\n### Creating a Notebook in VS Code\n\nVS Code makes working with notebooks extremely convenient once the Python and Jupyter extensions are installed.\n\nYou can create a new Jupyter notebook in two ways:\n\n- **Method 1: Using the Command Palette**\n    1. Press **Ctrl + Shift + P** to open the Command Palette.\n    2. Type **“Jupyter: Create New Jupyter Notebook”**.\n    3. Press Enter, a new `.ipynb` file opens, ready for code.\n- **Method 2: Creating a File Manually**\n    1. In the Explorer sidebar, click the **New File** icon.\n    2. Name the file with a `.ipynb` extension, e.g.: `analysis.ipynb` \n\nVS Code will automatically open it in notebook mode. Once the notebook opens, you can:\n\n- Add cells by clicking **+ Code** or **+ Markdown**\n- Run Python code directly in your project’s virtual environment\n- View charts and visualisations inline\n\nNotebooks are ideal for experimentation, visualisation, and documenting your thought process.\n\n### Linking Our Notebook with the Virtual Environment\n\nNow that you have created your Jupyter notebook (`.ipynb` file) and installed all the necessary packages within the `venv`, the final step is to tell VS Code to use that environment as the notebook&#39;s **Kernel**.\n\nIf you skip this step, the notebook will use your computer&#39;s default Python installation, which will likely result in a `ModuleNotFoundError` when you try to import your installed libraries.\n\n**How to Select the Kernel in VS Code:**\n\n1. **Open your Notebook:** Ensure your `.ipynb` file is open in the VS Code editor.\n2. **Locate the Kernel Selector:** Look for the current kernel name in the **top-right corner** of the notebook window. It might currently say something like \&quot;Python 3\&quot; or \&quot;No Kernel Selected.\&quot;\n3. **Select Your Environment:** Click on the current kernel name. A list of available Python environments will appear.\n4. **Choose Your venv:** Select the option that corresponds to the virtual environment you just created. It will typically be named something like:\n    \n    &gt; Python 3.x.x (venv) (The name of your project folder)\n    &gt; \n\nOnce selected, the name in the top-right corner should change to show your `(venv)`. You are now ready to run code in your notebook, knowing it is correctly isolated and has access to all the libraries you installed.\n\n## Google Drive (Colaboratory)\n\nIf you prefer a cloud-based setup or want to avoid installing anything locally, **Google Colab** is an excellent option. It provides:\n\n- A fully configured Python environment\n- Free GPU access\n- Automatic saving to your Google Drive\n- Easy sharing and collaboration\n\n### **Enable Colab in Google Drive**\n\n1. Go to **Google Drive**.\n2. Right-click → **More** → **Connect more apps**.\n3. Search for **“Colaboratory”**.\n4. Click **Install**.\n\nYou can now create notebooks anytime via: **Right-click → More → Colaboratory**\n\nColab notebooks run entirely in the cloud, and many common data science libraries are installed by default.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/data-science-with-python/1-setup.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/data-science-with-python.png&quot;]]],&quot;digest&quot;:[0,&quot;d05ab3cb56b0d65c&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;data-science-with-python/1-setup&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;data-science-with-python/3-visualising.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;data-visualisation-python-eda-seaborn-tutorial&quot;],&quot;title&quot;:[0,&quot;Python Data Visualisation: A Beginner&#39;s Guide to EDA with Seaborn&quot;],&quot;description&quot;:[0,&quot;Move beyond raw numbers. Learn how to perform Exploratory Data Analysis (EDA) using Python to uncover trends, correlations, and distributions in your clean data.&quot;],&quot;collection&quot;:[0,&quot;Data Science with Python&quot;],&quot;chapter&quot;:[0,3],&quot;shortname&quot;:[0,&quot;Visualising&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2026-02-08T00:00:00.000Z&quot;],&quot;update&quot;:[3,&quot;2026-02-08T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Matplotlib&quot;],[0,&quot;Seaborn&quot;],[0,&quot;EDA&quot;],[0,&quot;Data Visualization&quot;],[0,&quot;Statistics&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;exploratory data analysis python tutorial&quot;],[0,&quot;seaborn heatmap correlation example&quot;],[0,&quot;how to read a qq plot&quot;],[0,&quot;python histogram vs boxplot&quot;],[0,&quot;data visualization for data science beginners&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/data-science-with-python.D2zPcuIT.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for Data Science with Python showing Python and data analysis tools&quot;],&quot;draft&quot;:[0,false],&quot;steps&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Summarize Data Statistics&quot;],&quot;text&quot;:[0,&quot;Use df.describe() and value_counts() to understand the scale and frequency of your variables.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Check Feature Correlations&quot;],&quot;text&quot;:[0,&quot;Generate a Seaborn Heatmap to identify strong relationships between numerical columns.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Analyze Distributions&quot;],&quot;text&quot;:[0,&quot;Use Histograms for shape, Box Plots for outliers, and Q-Q plots to test for normality.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Compare Categories&quot;],&quot;text&quot;:[0,&quot;Utilize Bar Plots to see how numerical averages differ across categorical groups.&quot;]}]]]}],&quot;body&quot;:[0,&quot;Once our data is cleaned, validated, and structured, we can finally begin to **understand it**.\n\nThis is where **data visualisation** becomes one of the most powerful tools in a data scientist’s toolkit.\n\nRaw numbers in a table rarely tell a clear story on their own. Visualisation transforms those numbers into patterns, trends, and relationships that are easy to interpret and communicate. It allows us to move beyond *“what does the data contain?”* and start asking *“what is the data telling us?”*\n\nIn this chapter, we’ll explore how to use Python’s visualisation libraries to perform **exploratory data analysis (EDA)** and turn our clean data into insights that can guide decisions, analysis, and model development.\n\n## The Purpose of Visualisation\n\nThe primary purpose of data visualisation is **insight**, not decoration.\n\nVisualisation plays a critical role in the **exploratory phase** of a data science project. Before building models or concluding, we use charts to develop intuition about the data and uncover questions worth investigating further.\n\nWell-designed charts help us:\n\n- Understand the distribution of data\n- Identify trends and seasonal patterns\n- Detect anomalies and outliers\n- Compare groups and categories\n- Explore relationships between variables\n\n## Understanding Data Types and Chart Choices\n\nThe first rule of effective visualisation is: **the type of data dictates the appropriate chart.** Choosing the wrong chart can obscure the very insight you are trying to reveal.\n\n| Analysis Goal | Data Type | Key Question | Recommended Chart |\n| --- | --- | --- | --- |\n| Distribution | Single Numerical Column | How are values spread out? | Histogram, Box Plot |\n| Relationship | Two Numerical Columns | Do two variables affect each other? | Scatter Plot, Line Plot |\n| Comparison | Categorical vs. Numerical | Which group is performing better? | Bar Plot |\n\n## Basic Exploratory Data Analysis (EDA)\n\nBefore jumping into complex visuals, we use simple Pandas functions to frame our analysis. We assume our cleaned data is still loaded into the DataFrame `df`.\n\n### 1. Data Summary and Statistics\n\nTo get a quick overview of our numerical columns and to identify the mean, standard deviation, and quartiles:\n\n```python\n# Statistical summary for numerical columns\ndf.describe()\n```\n\nTo understand the counts and spread of a categorical column:\n\n```python\n# Value counts for a categorical column (e.g., \&quot;gender\&quot;)\nprint(df[\&quot;gender\&quot;].value_counts())\n```\n\n### 2. Visualising Relationships: The Correlation Heatmap\n\nCorrelation measures the strength of the relationship between two numerical variables. A value close to $1$ means a strong positive relationship (as one increases, the other increases), while a value close to $-1$ means a strong negative relationship.\n\nWe use a Seaborn **Heatmap** to visualise the entire correlation matrix at once:\n\n```python\n# Calculate correlation matrix\ncorr_matrix = df.corr(numeric_only=True)\n\n# Visualize the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap=\&quot;coolwarm\&quot;, fmt=\&quot;.2f\&quot;)\nplt.title(\&quot;Correlation Matrix of Numerical Features\&quot;)\nplt.show()\n```\n\nWith a basic statistical understanding in place, we can now use visualisation to explore these patterns more deeply.\n\n## Core Visualisation Techniques (Chart Examples)\n\nNow we move to creating specific charts to answer targeted questions about our data.\n\n### 1. Distributions (Histograms, Box Plots, and Q-Q Plots)\n\n**Question:** What is the typical range, spread, and shape of the data, and how normal is the distribution?\n\n**Histogram (Shape and Frequency)**\n\nWe use a **Histogram** to show the frequency distribution and shape (skewness) of a numerical column (e.g., “price”).\n\n```python\n# Histogram: Distribution of a single numerical variable\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\&quot;price\&quot;], bins=20, kde=True)\nplt.title(\&quot;Distribution of Product Price (Histogram)\&quot;)\nplt.xlabel(\&quot;Price\&quot;)\nplt.ylabel(\&quot;Frequency\&quot;)\nplt.show()\n```\n\n**Box Plot (Quartiles and Outliers)**\n\nThe **Box Plot** (or box-and-whisker plot) provides a clear, concise view of the data&#39;s central tendency (median), spread (IQR), and presence of outliers.\n\n```python\n# Box Plot: Visualize central tendency and outliers\nplt.figure(figsize=(8, 6))\nsns.boxplot(y=df[\&quot;price\&quot;])\nplt.title(\&quot;Distribution of Product Price (Box Plot)\&quot;)\nplt.ylabel(\&quot;Price\&quot;)\nplt.show()\n```\n\n**Q-Q Plot (Testing for Normality)**\n\nThe **Quantile-Quantile (Q-Q) Plot** is used to visually assess if the distribution of a dataset is similar to a theoretical distribution, most commonly the **normal distribution**. If the data points fall roughly alongside the straight diagonal line, the data is likely normally distributed.\n\n```python\n# Q-Q Plot: Test if the &#39;price&#39; column is normally distributed\nplt.figure(figsize=(8, 6))\nstats.probplot(df[\&quot;price\&quot;], dist=\&quot;norm\&quot;, plot=plt)\nplt.title(\&quot;Q-Q Plot of Product Price\&quot;)\nplt.show()\n```\n\n&gt;\n&gt; Pro Tip: If your Q-Q plot shows an \&quot;S\&quot; shape or curves away from the line at the ends, your data might have \&quot;heavy tails,\&quot; meaning outliers are more frequent than in a normal distribution.\n&gt;\n\n### 2. Relationships (Scatter Plots)\n\n**Question:** Is there a visible relationship between a customer&#39;s `age` and their `salary`?\n\nWe use a **Scatter Plot** to explore this relationship, adding the `hue` parameter to colour-code points by a third variable (like `gender`) for deeper insight.\n\n```python\n# Scatter Plot: Relationship between two numerical variables\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=\&quot;age\&quot;, y=\&quot;salary\&quot;, data=df, hue=\&quot;gender\&quot;)\nplt.title(\&quot;Age vs. Salary, colored by Gender\&quot;)\nplt.xlabel(\&quot;Age\&quot;)\nplt.ylabel(\&quot;Salary\&quot;)\nplt.show()\n```\n\nA visible upward trend would suggest that salary tends to increase with age, though further analysis would be needed to confirm this relationship.\n\n### 3. Comparisons (Bar Plots)\n\n**Question:** How does the average product `rating` compare across different `city` locations?\n\nWe use a **Bar Plot** (Seaborn&#39;s `barplot` defaults to showing the mean) to compare a numerical value across categories.\n\n```python\n# Bar Plot: Comparison of average rating across different cities\nplt.figure(figsize=(12, 6))\n\n# errorbar=None removes the default error bars for simplicity in this example\nsns.barplot(x=\&quot;city\&quot;, y=\&quot;rating\&quot;, data=df, errorbar=None)\nplt.title(\&quot;Average Rating by City\&quot;)\nplt.xlabel(\&quot;City\&quot;)\nplt.ylabel(\&quot;Average Rating\&quot;)\nplt.xticks(rotation=45) # Rotate labels for better readability\nplt.show()\n```\n\n## Conclusion: From Code to Insight\n\nThis completes the foundational phase of our data science journey into the world of data science with Python. We moved from an empty console to a data-driven narrative:\n\n1. **Setup** (Chapter 1): We built a reliable, isolated environment.\n2. **Cleaning** (Chapter 2): We transformed raw, messy data into a trustworthy asset.\n3. **Visualisation** (Chapter 3): We used charts to uncover key insights, such as [Insert a sample finding here based on your imaginary data, e.g., \&quot;the strong positive correlation between Age and Salary\&quot;].\n\nBy mastering these three phases, you now have the foundational skills to tackle any real-world data science project.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/data-science-with-python/3-visualising.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/data-science-with-python.png&quot;]]],&quot;digest&quot;:[0,&quot;4185bb148b8f80d9&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;data-science-with-python/3-visualising&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;data-science-with-python/2-cleaning.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;data-cleaning-python-pandas-wrangling-guide&quot;],&quot;title&quot;:[0,&quot;The Cleaning Machine: 6 Steps to Trustworthy Data&quot;],&quot;description&quot;:[0,&quot;Real-world data is chaotic. Master the 6-step framework for data wrangling using Python and Pandas to transform raw inputs into high-quality insights.&quot;],&quot;collection&quot;:[0,&quot;Data Science with Python&quot;],&quot;chapter&quot;:[0,2],&quot;shortname&quot;:[0,&quot;Cleaning&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2026-01-25T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Pandas&quot;],[0,&quot;Data Wrangling&quot;],[0,&quot;Outliers&quot;],[0,&quot;Data Quality&quot;],[0,&quot;Preprocessing&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how to clean data in pandas&quot;],[0,&quot;data wrangling steps python&quot;],[0,&quot;removing outliers with iqr method&quot;],[0,&quot;handling missing data imputation&quot;],[0,&quot;standardizing categorical data python&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/data-science-with-python.D2zPcuIT.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for Data Science with Python showing Python and data analysis tools&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2026-01-25T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Once your environment is ready and you know how to run Python code, the next crucial step in any data science project is **cleaning the data,** often called **data wrangling** or **data pre-processing**. \n\nReal-world data is messy. It’s incomplete, inconsistent, duplicated, noisy, badly formatted, or simply wrong.\n\nThis chapter will give you a practical framework, your **“Cleaning Machine”** to turn raw, chaotic data into something trustworthy.\n\n## Why Data Cleaning Matters\n\nImagine trying to build a machine-learning model with flawed data. Models learn from the data that we feed them, so bad data leads to bad results. Even the most advanced algorithms can&#39;t fix:\n\n- Duplicated entries\n- Wrong categorical labels\n- Missing or impossible values\n- Unexpected formatting\n- Outliers that distort your analysis\n\nCleaning is often **80% of the work** in data science… and also where the biggest improvements happen.\n\nIn this chapter, we will walk through the cleaning steps and learn how to apply them with Python tools like **Pandas**.\n\n## The Data\n\nBefore we begin with the cleaning steps, we first need to have data. [Kaggle](https://www.kaggle.com/datasets) is a website that offers a wide range of datasets for you to use in practising data science. For this post, we will not use a specific dataset, so feel free to choose any one you like.\n\nNow that we have downloaded a dataset, we need to start by adding our Python packages to the notebook. Add a code cell with the following import statements, and make sure to run the cell so that the packages are available in our notebook:\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom scipy import stats\n```\n\n### Loading the Dataset\n\nHow you load your dataset depends on **where you&#39;re running Python**.\n\n**1. Google Colab**\n\nTo load data from Google Drive, we first need to mount it to our notebook. We can do that by running the following code:\n\n```python\nfrom google.colab import drive\ndrive.mount(&#39;/content/drive&#39;)\n```\n\nYou will see a link. Log in to your Google account, copy the authorisation code, and paste it back into Colab.\n\nAfter this step, your Drive files are available at:\n\n```\n/content/drive/MyDrive/\n```\n\nNow we can load the dataset. If your dataset is inside a folder named **datasets** in Google Drive:\n\n```python\ndf = pd.read_csv(\&quot;/content/drive/MyDrive/datasets/data.csv\&quot;)\n\n# Quickly inspect the first few rows of the DataFrame\ndf.head()\n```\n\n**2. locally (Jupyter, VS Code, etc.)**\n\nIf your dataset file is in the **same folder** as your notebook:\n\n```python\n# Assuming the file is named data.csv and is in the same directory as the notebook\ndf = pd.read_csv(\&quot;data.csv\&quot;)\n\n# Quickly inspect the first few rows of the DataFrame\ndf.head()\n```\n\nNote: Every time you rerun the dataset loading cell:\n```python\ndf = pd.read_csv(...)\n```\nThe data is reloaded from disk into memory, and all previous modifications are lost. This is useful if you want to reset and restart the cleaning process.\n\n## The Six Steps of Data Cleaning\n\nNow that we have loaded the data into our notebook, we can begin with the six steps of data cleaning.\n\n### 1. Remove Irrelevant Data\n\nNot all collected data is useful to us, and in this step, we will remove columns or rows that do not contribute to the analysis goal. This often includes columns with a single constant value (no variance) or columns with non-essential identifiers.\n\nBefore removal, we need to assess our data. We can start by getting an idea of the columns and their data types:\n\n```python\ndf.info()\n```\n\nIf you identify a column (let&#39;s call it `irrelevant_id`) that is not needed for your analysis, you can permanently remove it using the `drop()` method:\n\n```python\n# axis=1 specifies that we are dropping a column, not a row\ndf = df.drop(columns=[\&quot;irrelevant_id\&quot;])\n```\n\n### 2. Remove Duplicate Data\n\nIn this step, we will identify and eliminate rows that are identical copies of others to prevent analysis bias. Duplicates can sneak into datasets during merging, scraping, or logging and may inflate counts and skew statistical summaries.\n\nWe first check how many duplicates exist, and then remove them. Pandas makes this a one-line process:\n\n```python\n# Check for the number of duplicate rows\nprint(f\&quot;Number of duplicates {df.duplicated().sum()}\&quot;)\n\n# Delete duplicates\ndf = df.drop_duplicates()\n```\n\n### 3. Repair Structural Errors\n\nThe aim for this step is to fix inconsistencies that arise from human error, different data formats, or typos. This is common in categorical columns (e.g., city names or gender). \n\nExamples: \n\n- “NY”, “new_york”, and “New York” in the same column\n- Mis-typed numerical values (`100O` instead of `1000`)\n- Inconsistent casing (“Male” vs “male”)\n\nFixing these often involves:\n\n**Normalise text:**\n\n```python\ndf[\&quot;city\&quot;] = df[\&quot;city\&quot;].str.lower().str.replace(\&quot;_\&quot;, \&quot; \&quot;)\n```\n\n**Standardise categories:**\n\n```python\ndf[\&quot;gender\&quot;] = df[\&quot;gender\&quot;].replace({\n    \&quot;M\&quot;: \&quot;Male\&quot;,\n    \&quot;male\&quot;: \&quot;Male\&quot;,\n    \&quot;F\&quot;: \&quot;Female\&quot;,\n    \&quot;female\&quot;: \&quot;Female\&quot;\n})\n```\n\n**Convert data types:**\n\n```python\n# Change data type if possible, else N/A\ndf[\&quot;price\&quot;] = pd.to_numeric(df[\&quot;price\&quot;], errors=\&quot;coerce\&quot;)\n```\n\n### 4. Address Missing Data / Gaps\n\nMissing data are common and can bias models or cause errors. In this step, we will decide how to handle them.\n\nThe method you choose depends on the context. Never treat missing data blindly. But before deciding on a strategy, we must first quantify *how much* data is missing and *where*.\n\n```python\n# Display the count of missing (NaN) values for every column\ndf.isnull().sum()\n```\n\n1. **Removal (Dropping)**: Removing the entire row containing the missing value. This is appropriate if only a small percentage of data is missing.\n    \n    ```python\n    # Remove rows where ANY column has a missing value (use with caution)\n    df = df.dropna()\n    \n    # Remove rows where price columns has a missing value\n    df = df.dropna(subset=[\&quot;price\&quot;])\n    ```\n    \n\n1. **Imputation (Filling)**: Replacing the missing value with a calculated value.\n    - For **numerical** columns, you can use the mean, median, or mode. The **median** is often safer as it is less sensitive to outliers.\n        \n        ```python\n        # Fill missing values in a numerical column (&#39;age&#39;) with the median\n        median_age = df[\&quot;age\&quot;].median()\n        df[\&quot;age\&quot;] = df[\&quot;age\&quot;].fillna(median_age)\n        ```\n        \n    - For **categorical** columns, you can fill with the mode (most frequent value) or a placeholder like “Unknown”.\n        \n        ```python\n        # Fill missing values in a categorical column (\&quot;status\&quot;) with \&quot;Unknown\&quot;\n        df[\&quot;status\&quot;] = df[\&quot;status\&quot;].fillna(\&quot;Unknown\&quot;)\n        ```\n        \n\nWhen working with **time-series data**, where a pattern or seasonality exists, simple imputation can destroy the underlying structure. This is where the **Fourier Transformation** comes into play. This advanced technique decomposes the time series into its core frequencies (patterns). It allows us to model and predict the missing values based on the dominant cyclical and trend components of the data, providing a much more accurate fill than simple averages.\n\nThe essential functions for this are available directly in the **NumPy library** we already imported (`numpy.fft`). It involves:\n\n1. Performing the Fast Fourier Transform (FFT) on the data.\n2. Zeroing out selected high-frequency components (often associated with noise) and potentially using the remaining signal to interpolate the missing points.\n3. Applying the Inverse Fast Fourier Transform (IFFT) to return the smoothed, complete data back to the time domain.\n\nNote: While the core tools are in NumPy, setting up the interpolation logic around the FT requires understanding time series signals and is best suited for advanced analysis.\n\n### 5. Filter Outliers\n\nOutliers can skew descriptive statistics (especially the mean and standard deviation) and distort the training of machine learning models. In this step, we will identify data points that are significantly different from other observations.\n\nA robust way to identify outliers in a numerical column is using the **Interquartile Range (IQR) method**. This method flags values that lie outside $1.5 \\times IQR$  from the first quartile ($Q1$) and third quartile ($Q3$).\n\nBefore we remove them, a good practice is to visualise them using a **Box Plot**:\n\n```python\n# Create a box plot to visually identify outliers in a column (e.g., \&quot;price\&quot;)\nplt.figure(figsize=(8, 6))\nsns.boxplot(y=df[&#39;price&#39;])\nplt.title(&#39;Box Plot of Price before Outlier Removal&#39;)\nplt.show()\n```\n\nIdentifying and Removing Outliers using IQR:\n\n```python\n# Calculate Q1 (25th percentile) and Q3 (75th percentile) for a column (\&quot;price\&quot;)\nQ1 = df[\&quot;price\&quot;].quantile(0.25)\nQ3 = df[\&quot;price\&quot;].quantile(0.75)\nIQR = Q3 - Q1\n\n# 2. Define the boundaries for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# 3. Filter the DataFrame to keep only the non-outliers\ndf = df[(df[\&quot;price\&quot;] &gt;= lower_bound) &amp; (df[\&quot;price\&quot;] &lt;= upper_bound)]\n```\n\n### 6. Validate That the Data Is Correct\n\nIn the last step, we need to perform a final check to ensure the cleaned data adheres to the expected rules and formats.\n\nData validation often involves checking logical conditions specific to your dataset. Your checks should confirm that the steps you just completed actually worked:\n\n| Check | Objective | Python Code Example |\n| --- | --- | --- |\n| No Negative Values | Ensures numerical data (like age or price) is logically positive | `(df[\&quot;age\&quot;] &gt;= 0).sum()` |\n| Within bounds  | Ensure numerical data falls within our bounds | `df[\&quot;rating\&quot;].between(1, 5).all()` |\n| Categorical Purity | Ensures standardized categories are the ONLY values present | `df[\&quot;gender\&quot;].isin([\&quot;Male\&quot;, \&quot;Female\&quot;]).all()` |\n| No New Nulls | Confirms no new missing values were created during transformations | `df[\&quot;category\&quot;].notnull().any()` |\n\nIf any of these checks return `False`, we still have remaining errors that need to be addressed. Otherwise, our data is officially considered **\&quot;clean\&quot;** and ready for the next phase: **Visualisation and Analysis!**&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/data-science-with-python/2-cleaning.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/data-science-with-python.png&quot;]]],&quot;digest&quot;:[0,&quot;a63b44df65386d2e&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;data-science-with-python/2-cleaning&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/1-losing.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;risk-management-variance-decisions-under-uncertainty&quot;],&quot;title&quot;:[0,&quot;The Art of Losing Slowly: How to Manage Risk, Variance and Decisions Under Uncertainty&quot;],&quot;description&quot;:[0,&quot;Master risk management through the lens of variance and expected value. Learn how to size bets, diversify risk, and implement decision-making logic in Python.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,1],&quot;shortname&quot;:[0,&quot;Losing&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-11-23T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Risk Management&quot;],[0,&quot;Expected Value&quot;],[0,&quot;Variance&quot;],[0,&quot;Python&quot;],[0,&quot;Probability&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;losing slowly finance&quot;],[0,&quot;variance vs expected value&quot;],[0,&quot;decision making under uncertainty python&quot;],[0,&quot;risk averse vs risk seeking&quot;],[0,&quot;expected value calculation python&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/banner-art-of-losing-slowely.DBSc2GqA.png&quot;],&quot;width&quot;:[0,1584],&quot;height&quot;:[0,672],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for The Art of Losing Slowly&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-11-23T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In a world obsessed with winning, few stop to consider the quiet art of losing. Not making reckless decisions, but instead being deliberate, patient, and purposeful. Managing risk and making decisions under uncertainty isn’t about avoiding loss; it’s about controlling its space, size, and impact. Success often belongs not to those who never fall, but to those who learn to fall well, who can endure small, calculated setbacks while keeping the bigger picture intact.\n\nLearning to “*lose slowly*” means embracing uncertainty without panic, balancing conviction with adaptability, and knowing when to press forward and when to hold back. It&#39;s playing the long game when the short term feels chaotic. By learning to lose slowly, we will be able to separate sustainable progress from mere luck.\n\n# Understanding Risk\n\nRisk is woven into almost every decision we make. We take it when we cross the street, start a new job, or trust someone’s word. Most of the time, we manage it intuitively, weighing outcomes, reading context, and adjusting as we go. Every day life trains us to navigate uncertainty, but it can create a dangerous illusion of being too comfortable with risk and of underestimating it.\n\nIn financial decisions, those illusions often break down. Even highly educated people who hold strong opinions about financial markets and financial risk struggle with it. The 2008-2009 financial crisis is a clear example of how financial risk was underestimated. It&#39;s important for us to have a deep enough understanding to manage the risk we can live with.\n\nLet&#39;s explore an example of risk evaluation in a financial decision involving gambling. Imagine this: you pay **€1** to draw a single card from a standard deck. If you pull the **ace of spades**, you win **€50**; otherwise, you get nothing. Would you take the bet?\n\nMost people would since it feels like a small risk for a high reward. But let&#39;s change the game. Would you play the banker? So now you get **€1** if no **ace of spades** is drawn, but you lose **€49** if an ace is drawn.\n\nWe can make this game more fun by upping the stake by ***x100***. Most of us would probably hesitate to lose **€4900** just to gain **€100**. This just feels like a bad deal, even though the odds are the same as with the lower amounts. We can calculate the expected value of this bet as:\n\n$$\nE(\\text{Bet}) = \\frac{1}{52}(-€4900) + \\frac{51}{52}(€100) = \\frac{100}{26} \\text{ or about €4}\n$$\n\nSo the bet still has a positive outcome, but what if we split it into 50 even smaller bets? With each round, we risk losing **€98** and have a chance to gain **€2**. Our maximum possible loss is still **€4900,** and the maximum possible gain is still **€100**. Did you notice what happened when we changed the game like this? The probability of us losing everything just changed to almost zero (about **10^-86**), while the probability of obtaining the maximum gain is (**51/52)^50 = 37.87%**.  \n\nWhat changed? Not the expected outcome—what stayed the same—but the variance did change. Variance captures how spread out the possible outcomes are, and it turns out to be a powerful way to distinguish a good from a bad bet.\n\nThe essence of variance is dividing a single large, risky bet into many smaller, uncorrelated ones. In finance, diversification transforms uncertainty into something manageable. It’s why investors might prefer to hold a **1%** stake in **100** **mortgages** than a **100% stake in one.** The total exposure is the same, but the risk behaviour is completely different. This is a great example of managing risk by leveraging variance. \n\n# Making decisions under uncertainty\n\nEvery meaningful decision we make lives in the space between what we know and what we can only guess. Whether we’re allocating capital or choosing when to exit a trade, uncertainty is the constant backdrop. Yet decisions must still be made. Good decision-making under uncertainty is not only about the probability of an event, but also about the consequences if it occurs. A rare event with a large impact can matter far more than a frequent one with small effects. However, in practice, this can be very difficult, since there are an infinite number of possible outcomes for certain events. For simplicity, let&#39;s imagine we have an event with a finite number of possibilities. Imagine we have a company that runs online ads. We need to decide whether to place the ad online. Based on historical data, we know the probability and outcome of each decision we can make:   \n\n| **Decsion** | **Probablity** | **Outcome** |\n| --- | --- | --- |\n| *Place the ads* | 20% | Customer buys → **€5 profit** |\n|  | 30% | Customer does not buy → **€1 loss** |\n| *Do not place the ads* | 50% | Customer shows interest but doesn’t buy → **€0 profit** |\n\nThere are several ways to evaluate the risk and the actions we should take. Let&#39;s discuss two simple methods to understand the principle of decision-making. The first one is the expected value decision. With this decision, we will calculate the expected value of each outcome with this formula:\n\n$$\n\\text{EV(decision)} = \\sum (\\text{Probability of outcome} \\times \\text{Value of outcome})\n$$\n\nWe can easily automate this decision-making process with Python. By defining the possible outcomes and their probabilities for each decision, the script will compute the expected value and compare your options programmatically.\n\n```python\ndef expected_value(outcomes):\n    \&quot;\&quot;\&quot;\n    outcomes: list of (probability, payoff) pairs.\n    \&quot;\&quot;\&quot;\n    return sum(p * v for p, v in outcomes)\n\ndef evaluate_decisions(decisions):\n    \&quot;\&quot;\&quot;\n    decisions: dict where key = decision name,\n               value = list of (probability, payoff) pairs.\n    \&quot;\&quot;\&quot;\n    results = {}\n    for name, outcomes in decisions.items():\n        results[name] = expected_value(outcomes)\n    return results\n\nif __name__ == \&quot;__main__\&quot;:\n    # All the options and their possible outcomes\n    decisions = {\n        \&quot;Place the ad\&quot;: [(0.2, 5), (0.3, -1)],\n        \&quot;Do not place the ad\&quot;: [(1, 0)],\n    }\n\n    ev_results = evaluate_decisions(decisions)\n    # Identify the best decision\n    best_decision = max(ev_results, key=ev_results.get)\n    print(f\&quot;Best decision under the Expected Value criterion: {best_decision}\&quot;)\n```\n\n**The script will give us this output:**\n\n```\nBest decision under the Expected Value criterion: Place the ad\n```\n\nSo we should place the ads based on the expected value decision. In general, we should make decisions based on expected value if the decision is made often enough for the law of large numbers to apply. The decision in our example is for every person who interacts with the ad. So in the end, we should make the most money by placing the ad.\n\nNot all decision makers will find this approach appropriate for this situation. A risk-averse decision maker will be more interested in minimising their loss. They will use the worst-case decision-making method. The decision based on the worst-case scenario should be made when it is a one-time occurrence and the worst-case scenario has a catastrophic effect. In this case, we should always cover the worst case. The worst-case scenario calculation does not account for the probability of an event. We simply look at what it would the worst possible outcome is.\n\nImagine we are going on holiday and you get the option to buy travel insurance. The total trip costs you **€5000**, and you can buy additional travel insurance for **€200**. So what could happen?\n\n| **Decision** | **Outcome** |\n| --- | --- |\n| *Buy Insurance* | Trip is fine → €200 loss |\n|  | Trip is canceled → €200 loss |\n| *Do NOT buy Insurance* | Trip is fine → €0 loss |\n|  | Trip is canceled → €5000 loss |\n\nThe worst thing that can happen is us losing **€5000** if we don&#39;t buy the insurance and the trip gets cancelled. Therefore, a risk-averse decision maker will choose to buy the insurance when going on the trip.\n\n# Closing thoughts\n\nLearning to **lose slowly** is not about avoiding failure; it’s about respecting uncertainty. It’s recognising that progress is rarely a straight line and that the cost of staying in the game is sometimes accepting small, intentional losses along the way.\n\nWe don’t control outcomes, but we *do* control our exposure: how much we risk, how often, and in what context. By diversifying, by sizing our decisions to our tolerance, and by keeping a long-term perspective even when the short term feels chaotic, we can build a path where setbacks are manageable rather than catastrophic.\n\nIn the end, winning isn’t about making the right decision every time. It’s about creating a system in which even the wrong decisions don’t break you.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/1-losing.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/banner-art-of-losing-slowely.png&quot;]]],&quot;digest&quot;:[0,&quot;886d02423565f394&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/1-losing&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/2-loans.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;loans-bonds-amortization-default-risk-python&quot;],&quot;title&quot;:[0,&quot;Loans and Bonds: The Mechanics of Borrowing and Default Risk&quot;],&quot;description&quot;:[0,&quot;A quantitative guide to loan amortization and bond pricing. Learn to derive payment formulas, simulate default risk, and understand coupon rates using Python.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,2],&quot;shortname&quot;:[0,&quot;Loans&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-12-07T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Fixed Income&quot;],[0,&quot;Bond Pricing&quot;],[0,&quot;Python&quot;],[0,&quot;Loan Amortization&quot;],[0,&quot;Credit Risk&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;loan amortization formula derivation&quot;],[0,&quot;bond coupon rate calculation&quot;],[0,&quot;default risk simulation python&quot;],[0,&quot;difference equation for loans&quot;],[0,&quot;risk-free rate and bonds&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/loans-and-bonds-explained.cwsEEMkV.png&quot;],&quot;width&quot;:[0,1344],&quot;height&quot;:[0,768],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Illustration explaining how loans and bonds work, including interest, repayment, and default risk&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-12-07T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Imagine a young couple, Sara and Tom, who have been dreaming of owning their first home. With a small savings account and a lot of determination, they finally step into a bank to discuss a mortgage loan. This moment marks a significant turning point in their lives, as the financial leverage they gain through the loan will enable them to move into their dream house, a place where they will raise their family. Financial loans sit at the core of how modern economies work. Whether it&#39;s a family buying a house, a company expanding its operations, or the government funding a public project, loans provide the upfront capital needed to make things happen. Yet, while most people understand the idea of borrowing and paying back with interest, the broader mechanics behind loans and how they evolve into more complex financial instruments are far less understood. In this post, we will start breaking down what a loan really is and how lenders manage the risk of loaning money. From there, we will work towards understanding bonds, which can be seen as loans packaged, traded, and scaled for markets. By understanding these concepts, we are building the right mental framework to start working with more complicated financial instruments.\n\n# Loans\n\nIn its essence, a loan is pretty simple: we borrow some money and pay it back over time. We can analyze the repayment of the money pretty easily. For example, we take a mortgage of $€300,000$ and plan to repay it in $300$equal monthly payments with an interest rate of $0.5\\%$ per month.\n\nMost of us can probably quantify how much we need to pay each month by using our intuition. The lowerbound of our monthly payment would be $€300,000 * 0.5\\% = €1500$ since this is the amount we would pay each month in interest. The upper bound of our payment would be $€1500 + \\frac{€300,000}{300} = €2500$, since this would cover the entire loan, including interest compounded over the full amount.\n\nBut what if we can estimate the monthly share of the principal at $€1000$ plus an interest of $€750$. We can calculate this interest by dividing $€1500$ by 2 since the interest would vanish over time. With this, our monthly payment would be $€1750$. But is it fair to assume this as our monthly payment? Let&#39;s try to calculate it.\n\n```python\ndef loan_payment(X, r, N):\n    \&quot;\&quot;\&quot;\n    Compute the fixed payment amount A for a loan.\n    X: initial loan amount\n    r: interest rate per period (e.g., 0.05 for 5%)\n    N: number of payments\n    \&quot;\&quot;\&quot;\n    if r == 0:\n        return X / N\n    return X * (r * (1 + r)**N) / ((1 + r)**N - 1)\n\ndef simulate_loan(X, r, N):\n    \&quot;\&quot;\&quot;\n    Simulate the loan balance D_k using the difference equation.\n    \&quot;\&quot;\&quot;\n    A = loan_payment(X, r, N)\n    D = [X]  # D_0 = X\n\n    for k in range(N):\n        D_next = (1 + r) * D[-1] - A\n        D.append(D_next)\n\n    return A, D\n\ndef main():\n    # Parameters\n    X = 300000.0   # initial amount borrowed\n    annual_rate = 0.06\n    payments_per_year = 12\n    years = 5\n\n    # Convert to rate per period and number of periods\n    r = annual_rate / payments_per_year\n    N = payments_per_year * years\n\n    A, D = simulate_loan(X, r, N)\n\n    print(f\&quot;Loan amount (X): {X:,.2f}\&quot;)\n    print(f\&quot;Interest rate per period (r): {r:.6f}\&quot;)\n    print(f\&quot;Number of payments (N): {N}\&quot;)\n    print(f\&quot;Payment per period (A): {A:,.2f}\\n\&quot;)\n\n    print(\&quot;k\\tOutstanding Balance (D_k)\&quot;)\n    for k, balance in enumerate(D):\n        print(f\&quot;{k}\\t{balance:,.2f}\&quot;)\n\nif __name__ == \&quot;__main__\&quot;:\n    main()\n\n```\n\nThis will output:\n\n```java\nLoan amount (X): 300000.00\nInterest rate per period (r): 0.004167\nNumber of payments (N): 300\nPayment per period (A): 1753.77\n\nk    Outstanding Balance (D_k)\n0    300000.00\n1    299496.23\n2    298990.36\n3    298482.38\n4    297972.29\n....\n298  3485.74\n299  1746.49\n300  0.00\n```\n\nAccording to this script, our monthly payments would be €1753.77. We got to this amount by first establishing the value of $€X$ (the amount borrowed). We will pay this back by paying $€A$ at time $N + \\Delta T$ ***,*** then at time $N + 2 \\Delta T$, and so on, ******until we reach time $N \\Delta T$***,*** at which point we owe no money. Let $D_{k}$ be the amount we owe at the $K_{th}$ payment. We know that $D_{0} = €X$ and $D_{N} = 0$, so we need to solve a difference equation. So to summarize:\n\n| Symbol | Meaning | Units |\n| --- | --- | --- |\n| $D_{k}$  | amount we owe at the $K_{th}$ payment | Currency |\n| $A$  | Payment per period | Currency |\n| $N$ | Number of payments |  |\n| $\\Delta T$ | Time between payments | Time |\n| $r$ | Interest rate per period |  |\n| $N$ | Principal (amount borrowed at time *t*) | Currency |\n\nWe can look at what happens between two consecutive payments. At the start of period $k$ you owe $D_{k}$. So during the next time interval $\\Delta T$:\n\n- The loan accrues interest at rate $r$, so it grows to $D_k(1+r)$.\n- Then you make a payment $A$, reducing your balance.\n\nThus the amount owed after the next payment (just after $(k+1) \\Delta T$ ) is:\n\n$$\nD_{k+1} = (1+r) D_k - A\n$$\n\nNow we need to find a closed form for $D_k$ which is a non-homogeneous linear recurrence relation. We know the boundary conditions of our scenario. The initial condition is us owing the full loan at $D_0=X$, and the final condition is us owing noting after $N$ payments at $D_N=0$. Knowing this we can solve for $A$:\n\n$$\n0 = (1 + r)^N X - A \\frac{(1 + r)^{N}-1}{r} \n$$\n\nSo,\n\n$$\nA = X \\frac{r(1+r)^N}{(1+r)^N-1}\n$$\n\nGiven the fixed payment per period we will have completely repaid the loan of amount $X$ after $N$ equal payments. \n\n# Bonds\n\nIf loans are the backbone of personal and corporate finance, then bonds are their large-scale, institutional counterpart. While a loan typically involves direct agreements between a borrower and a lender, bonds take that same idea and expand it to the financial markets. In this section, we will explore what bonds are, how they function, and why they are so important for funding governments and corporations. Understanding bonds will be an important part of our ability to understand and build more complex quantitative financial models. \n\nSimply put, a bond is a loan that is sold to an investor. Bonds have many interesting quantitative features; for example, their values fluctuate with interest rates, and they often contain embedded options that allow them to be repaid or called back. Sometimes bonds are issued in a foreign currency to investors, introducing foreign exchange risk. While these are all interesting, the most important risk we need to determine is whether the lender can repay the borrowed money.  \n\nWe can start with unraffling a simple bond for which the borrower receives $€X$. In exchange, the borrower will repay $€X(1+c)$ at time $T$**,** where $c$ ****is the coupon rate and $T$ ****is ****the bond&#39;s term. An important question we should ask here is: what is a fair value for $c$? How much should we be compensated for borrowing our money? To find the value of $c$**,** we first need to know the risk-free rate **r**. The risk-free rate is the rate that theoretically entails no financial risk, meaning the borrower is certain to repay the loan. Think of the risk-free rate as the minimum expected return on your investment. When making any investment, our expected return should exceed the risk-free rate. If this is not the case, you can not justify the risk you are taking. In a perfect world, we could even say that a fair value for $c=r$.\n\nTo find a fair value for $c$**,** we need to quantify the bond default rate. A simple model assumes that at time $T$**,** the bond either defaults or is repaid in full. When the bond defaults, we assume the investor will lose some value $R$ of their entire investment. So, with probability $p$**,** the bondholder receives $RX(1+c)$ at time $T$, and with probability $1-p$**,** they receive $X(c+1)$ at time $T$. We can say a fair rate for **c is** when the investor, on average, gets back their principal, adjusted for the time value of money. The lender must also charge a large enough coupon so that they do no worse than lending at the risk-free rate. This can be computed as follows:\n\n$$\npRX(1+c)+(1-p)X(1+c) \\geq X(1+r)\n$$\n\nOr,\n\n$$\npR(1+c)+(1-p)(1+c) \\geq (1+r)\n$$\n\nOr even simpler:\n\n$$\nc_{fair} = \\frac{r+p(1-R)}{1-p(1-R)}\n$$\n\nGiven this formula, a lender will, on average, break even after accounting for the time value of money and the risk of default. But which investor will take this risk to just break even? Many investors will ask for a far greater coupon to justify their risk and increase the chance of making some money. The difference between the fair coupon and the actual coupon is often determined during negotiations between the lender and borrower. \n\nInstead of increasing the coupon, we can also reduce our risk by finding many different borrowers, each with an independent chance of defaulting. Let me give you a one-sentence roadmap of the upcoming simulation: we will explore how varying default outcomes across ten distinct loans affects our risk profile. Let&#39;s run a simple simulation to see how our risk changes. We will work with the following fixed parameters:\n\n| **Param** | **Value** |\n| --- | --- |\n| X | €1000 |\n| r | 3% |\n| p | 5% |\n| R | 0 |\n\n```python\nimport random\nimport math\n\ndef main():\n    trials = 100\n    num_loans = 10\n    X = 1000.0\n    r = 0.03\n    p = 0.05\n    R = 0.0\n\n    # Coupon rate\n    c = (r + p * (1 - R)) / (1 - p * (1 - R))\n\n    returns = []\n\n    for _ in range(trials):\n        portfolio_return = 0.0\n\n        for _ in range(num_loans):\n            defaults = random.random() &lt; p  # Bernoulli(p)\n\n            if defaults:\n                portfolio_return += 0.0\n            else:\n                portfolio_return += X * (1.0 + c)\n\n        total_initial = num_loans * X\n        yearly_return = (portfolio_return - total_initial) / total_initial\n        returns.append(yearly_return)\n\n    # Statistics\n    mean = sum(returns) / trials\n    best = max(returns)\n    worst = min(returns)\n\n    variance = sum((r - mean) ** 2 for r in returns) / trials\n    stddev = math.sqrt(variance)\n\n    # Output\n    print(f\&quot;Bond Portfolio Simulation Results after {trials} trials with coupon: {c * 100:.2f}%\&quot;)\n    print(\&quot;--------------------------------------------\&quot;)\n    print(f\&quot;Average return:      {mean * 100:.2f} %\&quot;)\n    print(f\&quot;Best-case return:    {best * 100:.2f} %\&quot;)\n    print(f\&quot;Worst-case return:   {worst * 100:.2f} %\&quot;)\n    print(f\&quot;Standard deviation:  {stddev * 100:.2f} %\&quot;)\n\nif __name__ == \&quot;__main__\&quot;:\n    main()\n```\n\nThis will output:\n\n```\nBond Portfolio Simulation Results after 100 trials with coupon: 8.42105%\n--------------------------------------------\nAverage return:      3.75895 %\nBest-case return:    8.42105 %\nWorst-case return:   -24.1053 %\nStandard deviation:  7.55143 %\n```\n\nFrom this result, we can see that, on average, our return exceeds the risk-free rate, but not by much. Considering that if we had one lender, our potential worst-case scenario would be 100% loss. Having 10 different lenders has changed this to a worst-case loss of only -24%. However, this setup invites us to reflect on additional risk-mitigation strategies. How might tools like collateral or credit default swaps provide extra layers of protection against worst-case outcomes? Incorporating such alternatives could further enhance our ability to manage financial risk and potentially reduce it.\n\n# Closing thoughts\n\nLoans and bonds may look like everyday financial tools, but beneath the surface, they are built on clear mathematical structures and carefully managed risk. By breaking down how a simple loan works, how payments are determined, and how interest shapes repayment over time, we lay the foundations for understanding larger, more complex financial instruments.\n\nBonds follow the same principle as a loan, but scale it to markets. Their pricing depends on many factors, some of which we discussed, such as credit risk, recovery assumptions, and exposure diversification across many borrowers. As the simulation showed, spreading risk across a portfolio of independent loans dramatically reduces the worst-case outcomes, illustrating why diversification is central to modern finance.\n\nUltimately, understanding loans and bonds gives us more than just the ability to calculate payments or yields. It builds the mental framework needed to navigate the financial system as a whole. With this foundation, we can start exploring some more complex financial instruments.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/2-loans.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/loans-and-bonds-explained.png&quot;]]],&quot;digest&quot;:[0,&quot;a72e812223cbf277&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/2-loans&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/0-introduction.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;quantitative-finance-introduction-python&quot;],&quot;title&quot;:[0,&quot;Finding P Values and False Hope: Intro to Quantitative Finance&quot;],&quot;description&quot;:[0,&quot;Explore the intersection of math, stats, and Python in finance. Learn interest rates, compounding, risk management, and bond pricing with practical code examples.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,0],&quot;shortname&quot;:[0,&quot;Intro&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-11-09T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Featured&quot;],[0,&quot;Data Science&quot;],[0,&quot;Introduction&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Python&quot;],[0,&quot;Financial Math&quot;],[0,&quot;Risk Management&quot;],[0,&quot;Bond Pricing&quot;],[0,&quot;Statistics&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;quantitative finance python&quot;],[0,&quot;financial engineering basics&quot;],[0,&quot;interest rate compounding math&quot;],[0,&quot;risk management tools python&quot;],[0,&quot;bond pricing formulas&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/banner-p-values-and-false-hope.BbDvJEO1.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for the Introduction&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-11-09T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Welcome to this series, where we’ll explore how mathematics, statistics, and programming come together to understand —and sometimes predict —the movements of financial markets. Whether you&#39;re a curious learner or data enthusiast, this series will explore some essential quantitative finance concepts with practical Python examples.\n\nWe start this series by understanding some fundamentals of financial mathematics, including interest rates, compounding, and present value. We will discuss risk management—how to measure and mitigate financial uncertainty —and show how to implement some risk management tools.\n\nIn this series, we will also learn about bond pricing, understand dynamic stock price fluctuations, explore different models for option pricing, and much more.\n\nEach post in the series will combine clear financial intuition with Python-based implementations, helping you move from theory to practical application. By the end, you’ll have a solid foundation in both the quantitative concepts that drive modern finance and the computational tools used to bring them to life.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/0-introduction.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/banner-p-values-and-false-hope.png&quot;]]],&quot;digest&quot;:[0,&quot;002429249a0c2f89&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/0-introduction&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/3-tranching.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;cdo-tranching-senior-mezzanine-equity-python&quot;],&quot;title&quot;:[0,&quot;Life in the Tranches: Understanding CDOs and Risk Waterfalls&quot;],&quot;description&quot;:[0,&quot;A deep dive into Collateralized Debt Obligations (CDOs). Learn how tranching redistributes credit risk into Senior, Mezzanine, and Equity layers with a Python waterfall simulation.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,3],&quot;shortname&quot;:[0,&quot;Tranching&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-12-21T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;CDO&quot;],[0,&quot;Tranching&quot;],[0,&quot;Python&quot;],[0,&quot;Risk Management&quot;],[0,&quot;Fixed Income&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how CDO tranching works&quot;],[0,&quot;senior vs mezzanine vs equity tranches&quot;],[0,&quot;credit risk waterfall python&quot;],[0,&quot;collateralized debt obligation simulation&quot;],[0,&quot;loss absorbed by junior tranche&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/tranching-in-collaterized-debt.h8tTybnR.png&quot;],&quot;width&quot;:[0,1584],&quot;height&quot;:[0,672],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Illustration explaining how loans and bonds work, including interest, repayment, and default risk&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-12-21T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Few financial products capture the imagination and confusion of investors quite like Collateralized Debt Obligations (CDOs). Born of the desire to turn illiquid debt into tradable securities, CDOs bundle cash flows and slice them into layers of risk and reward. This “tranching” process doesn’t just redistribute exposure; it fundamentally changes how investors think about credit, yield, and safety.\n\nAt their core, CDOs are all about tranching: taking a pool of underlying loans or bonds and splitting the cash flows into different paths. Each path has its own payment priority; senior tranches are at the top, protected from defaults by junior tranches at the bottom. These junior tranches might be the first to absorb any defaults, but they will also return eye-watering returns. The structure of CDOs allows the same pool of bonds/loans to serve many different investors willing to take different levels of risk.\n\nIn this post, we’ll see how a CDO is built from different layers, how collateral is assembled, how tranching is done, and why a simple change in structure can make the same assets appear much safer or riskier. This post will be an introduction to living life in the tranches.\n\n# Collateralized Debt Obligations\n\nThere are investors who like to take a lot of risk in exchange for the chance of a high return, and there are investors who might not be as willing to take that same level of risk. A pension fund might not be willing to take a high risk and will settle for a lower return and coupon. Other investors seeking higher returns might be willing to take greater risk. Companies can use CDOs to meet this specific demand ratio. Let’s explore a simple example to see how this would work.\n\nAssume we have two very simple bonds that at time $t=0$ cost $€X$. At time $t=1$, they will either repay $€X+c*€X$  with probability $p$, or they will repay $€0$ with probability $1-p$. Assume these two bonds have an independent chance of defaulting. In this situation, we have a pension fund that is legally prohibited from taking on more than a certain level of risk. The two bonds we have are currently too risky for the pension fund to purchase. How can we issue a new financial bond so the pension fund can make this investment?\n\n![Explains a CDO structure](assets/visualizing-a-cdo-structure.png)\n\nWe do this by buying the two bonds and putting them into a legal structure that protects them from the outside world. The cash flow arising from this new structure is divided between investors in a special way. We create two new fixed-income securities: a junior and senior tranche. These junior and senior tranches refer to the order in which investors are paid out in case of a default. The junior tranche will only get paid after the senior tranche has been paid. If our CDO is split evenly between junior and senior tranches, what would happen if one of the bonds in the CDO defaulted? Well, the senior tranche would be paid with the principal supplied by the bond that did not default. However, the junior tranche would lose its principal, as there is no further cash flow available. Only in the unlikely event that all bonds in our CDO default would the senior tranche not get back its principal. \n\nTo offset this risk for junior tranche holders, they will receive a higher coupon. So, if neither of the bonds in our simple CDO defaults, both the junior and senior tranches will receive their principal, and on top of that principal, the junior tranche will receive a higher coupon. We need to find a coupon high enough so that investors in both tranches are willing to buy the bond.\n\nLet&#39;s assume that the junior tranches have a probability *p* of defaulting; therefore, the senior tranche has a probability $p^2$ of defaulting. In the event of a bond default, the recovery rate is $R=0$. Given this, the minimum platatable coupon rate we need to pay the senior tranche is:\n\n$$\nc_A = \\frac{p^2+r}{1-p^2}\n$$\n\nThe interest remaining to pay the junior tranche can be found using the formula:\n\n$$\nc_B = \\frac{2p(1+r)+r+p^2}{1-p^2}\n$$\n\nBecause it is highly unlikely that both bonds in our CDO default, the junior tranche actually has a small recovery rate if only one bond defaults. The bond that did not default will still pay its coupon, allowing for some recovery rate. The recovery rate *R_2* for the junior tranche in this case can be calculated as:\n\n$$\nR_2 = \\frac{p(1+r)}{1+r+2p(1+r)} = \\frac{p(1+r)}{(1+r)(1+2p)} = \\frac{p}{1+2p}\n$$\n\n# CDO Simulations\n\nHaving a CDO consisting of just two bonds is a good example to get a clear understanding of how they work. However, this kind of CDO is not very realistic in the actual world. Luckily, even CDOs composed of many different bonds share the same cash flow principles as the small CDO we have discussed so far.\n\nOur new CDO will consist of three tranches: A, B, and C. Tranche A is the senior tranche, and tranche C is the junior tranche. In this simulation, we do not consider the bonds&#39; ability to pay a fair coupon rate, as we did before. All the parameters for this simulation are listed in the table below.\n\n| $r$ | 3% |\n| --- | --- |\n| $X$ | €1000 |\n| $p$ | 5% |\n| $R$ | 0% |\n| **$c_{fair}$** | 8% |\n| $c$ | 12% |\n\nThe thicknesses of tranches A, B and C in our CDO are shown in the table below. Tranche A and B will be the biggest tranches, and our junior tranche C will simply receive whatever is left after tranches A and B have been paid. \n\n| **Tranche** | **Tickeness** | **Coupon** | **Coupon %** |\n| --- | --- | --- | --- |\n| Tranche A | 70% | $c_{A}$ | 3% |\n| Tranche B | 20% | $c_{B}$ | 8% |\n| Tranche C | 10% | $c_{C}$ | 15% |\n\nIn the previous post about *default risk,* we wrote a small script to simulate the result of a simple bond structure. Let&#39;s adjust this script a bit so we can simulate a more complicated CDO structure with different tranches, ticknesses, and coupons. \n\n```python\nimport random\nimport math\nfrom typing import List\n\nclass BondPricing:\n    def __init__(\n        self,\n        trials: int,\n        num_loans: int,\n        loan_principal: float,\n        interest_rate: float,\n        default_prob: float,\n        tranche_thickness: List[float],\n        tranche_coupons: List[float],\n        tranche_min_recovery: List[float],\n    ):\n        self.trials = trials\n        self.num_loans = num_loans\n        self.X = loan_principal\n        self.r = interest_rate\n        self.p = default_prob\n        self.tranche_thickness = tranche_thickness\n        self.tranche_coupons = tranche_coupons\n        self.tranche_min_recovery = tranche_min_recovery\n\n        self.validate_tranches()\n\n    def validate_tranches(self) -&gt; None:\n        if not (\n            len(self.tranche_thickness)\n            == len(self.tranche_coupons)\n            == len(self.tranche_min_recovery)\n        ):\n            raise RuntimeError(\&quot;All tranche vectors must have identical sizes.\&quot;)\n\n        total = 0.0\n        for t in self.tranche_thickness:\n            if t &lt;= 0.0:\n                raise RuntimeError(\&quot;Tranche thickness must be positive.\&quot;)\n            total += t\n\n        if abs(total - 1.0) &gt; 1e-6:\n            raise RuntimeError(\&quot;Tranche thickness must sum to 1.0\&quot;)\n\n        for rr in self.tranche_min_recovery:\n            if rr &lt; 0.0 or rr &gt; 1.0:\n                raise RuntimeError(\n                    \&quot;Tranche minimum recovery must be between 0 and 1\&quot;\n                )\n\n    def get_portfolio_principal(self) -&gt; float:\n        return self.num_loans * self.X\n\n    def get_num_tranches(self) -&gt; int:\n        return len(self.tranche_thickness)\n\n    def simulate_bond_pricing(self) -&gt; None:\n        n_tranches = self.get_num_tranches()\n        portfolio_notional = self.get_portfolio_principal()\n\n        tranche_returns = [[] for _ in range(n_tranches)]\n\n        for _ in range(self.trials):\n            default_count = 0\n            performing_count = 0\n            total_loss = 0.0\n            coupon_pool = 0.0\n\n            # Simulate loan defaults\n            for _ in range(self.num_loans):\n                is_default = random.random() &lt; self.p\n                if is_default:\n                    loan_recovery = 0.40\n                    default_count += 1\n                    total_loss += self.X * (1.0 - loan_recovery)\n                else:\n                    performing_count += 1\n                    coupon_pool += self.X * self.r\n\n            # Initialize tranches\n            tranche_init = [\n                t * portfolio_notional for t in self.tranche_thickness\n            ]\n            tranche_remaining = tranche_init.copy()\n\n            remaining_loss = total_loss\n\n            # Loss waterfall (junior → senior)\n            for i in range(n_tranches):\n                if remaining_loss &lt;= 1e-12:\n                    break\n                absorbed = min(remaining_loss, tranche_remaining[i])\n                tranche_remaining[i] -= absorbed\n                remaining_loss -= absorbed\n\n            # Coupon waterfall (senior → junior)\n            tranche_coupons_paid = [0.0] * n_tranches\n            for i in reversed(range(n_tranches)):\n                if tranche_remaining[i] &gt; 1e-12:\n                    due = tranche_init[i] * self.tranche_coupons[i]\n                    paid = min(coupon_pool, due)\n                    tranche_coupons_paid[i] = paid\n                    coupon_pool -= paid\n\n            # Compute returns\n            for i in range(n_tranches):\n                end_value = tranche_remaining[i] + tranche_coupons_paid[i]\n                if tranche_init[i] &gt; 0.0:\n                    ret = (end_value - tranche_init[i]) / tranche_init[i]\n                else:\n                    ret = 0.0\n                tranche_returns[i].append(ret)\n\n        # Output results\n        print(\&quot;\\n================= TRANCHE RESULTS =================\&quot;)\n\n        for i in range(n_tranches):\n            rets = tranche_returns[i]\n\n            mean = sum(rets) / self.trials\n            best = max(rets)\n            worst = min(rets)\n\n            variance = sum((r - mean) ** 2 for r in rets) / self.trials\n            stddev = math.sqrt(variance)\n\n            print(f\&quot;\\n--- Tranche {i + 1} ---\&quot;)\n            print(f\&quot;Thickness: {self.tranche_thickness[i] * 100:.2f}%\&quot;)\n            print(f\&quot;Coupon:    {self.tranche_coupons[i] * 100:.2f}%\&quot;)\n            print(f\&quot;Min Recov: {self.tranche_min_recovery[i] * 100:.2f}%\&quot;)\n            print(f\&quot;Avg Return: {mean * 100:.2f}%\&quot;)\n            print(f\&quot;Best Case:  {best * 100:.2f}%\&quot;)\n            print(f\&quot;Worst Case: {worst * 100:.2f}%\&quot;)\n            print(f\&quot;Std Dev:    {stddev * 100:.2f}%\&quot;)\n\n        print(\&quot;===================================================\&quot;)\n\n```\n\nCompared to the previous script, we added some extra variables and methods to this header. We added methods to validate the tranches&#39; configuration, to calculate the total principal of the CDO portfolio, and to count the number of tranches in the CDO. When running this class, we will get the following output:\n\n```cpp\n================= TRANCHE RESULTS =================\n\n--- Tranche 1 ---\nThickness: 10%\nCoupon:    15%\nMin Recov: 30%\nAvg Return: -0.955%\nBest Case:  15%\nWorst Case: -100%\nStd Dev:    29.3405%\n\n--- Tranche 2 ---\nThickness: 20%\nCoupon:    8%\nMin Recov: 10%\nAvg Return: 7.6%\nBest Case:  8%\nWorst Case: -62%\nStd Dev:    3.41174%\n\n--- Tranche 3 ---\nThickness: 70%\nCoupon:    3%\nMin Recov: 0%\nAvg Return: 3%\nBest Case:  3%\nWorst Case: 3%\nStd Dev:    3.81639e-14%\n===================================================\n```\n\nThe simulation demonstrates how tranche design fundamentally shapes the risk-return profile of a CDO. Even though the underlying loan pool is homogeneous and relatively simple. The distribution of losses and coupons across tranches produces dramatically different outcomes.\n\nTo better understand these outcomes, we can analyze each tranche&#39;s performance using a Sharpe-like ratio, which provides insight into how returns compare to volatility. The junior tranche absorbs losses first and carries substantial risk, exhibiting high volatility with outcomes ranging from full principal loss to full coupon payment. Its average negative return suggests that it effectively sells insurance against portfolio defaults, highlighting a low risk-adjusted return.\n\nThe mezzanine tranche offers a more balanced profile, with lower volatility than the junior tranche. It presents a smaller coupon and a slightly lower exposure to losses, meaning it would require significant CDO-wide losses before it incurs losses. As such, its risk-adjusted return is more stable than that of the junior tranche.\n\nFinally, the senior tranche resembles a nearly risk-free instrument, with losses rarely affecting it. Its consistent returns, even in worst-case scenarios, reflect a high risk-adjusted return, demonstrating the stability that appeals to conservative investors. By relating returns to volatility in this manner, investors can make more informed decisions when assessing the appeal of each tranche. This type of risk-adjusted analysis is highly relevant when evaluating CDO investments.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/3-tranching.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/tranching-in-collaterized-debt.png&quot;]]],&quot;digest&quot;:[0,&quot;62ffd015930f335d&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/3-tranching&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/4-yields.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;yield-curves-bootstrapping-forward-rates-python&quot;],&quot;title&quot;:[0,&quot;The Geometry of Interest: Constructing Yield Curves and Forward Rates&quot;],&quot;description&quot;:[0,&quot;Master the term structure of interest rates. Learn to build yield curves using bootstrapping and interpolation, and derive forward rates with practical Python examples.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,4],&quot;shortname&quot;:[0,&quot;Yields&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2026-01-04T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Yield Curve&quot;],[0,&quot;Forward Rates&quot;],[0,&quot;Python&quot;],[0,&quot;Fixed Income&quot;],[0,&quot;Bootstrapping&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;yield curve bootstrapping python&quot;],[0,&quot;calculate forward rate from spot rate&quot;],[0,&quot;linear interpolation yield curve&quot;],[0,&quot;term structure of interest rates&quot;],[0,&quot;zero coupon bond pricing formula&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/yield-curves-and-forward-rates-explained.Dl5Ef4Qg.png&quot;],&quot;width&quot;:[0,1808],&quot;height&quot;:[0,592],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Illustration explaining yield curves and forward rates&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2026-01-04T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In the world of fixed-income investing, few tools are as misunderstood as yield curves and forward rates. Whether you’re analysing macroeconomic trends, pricing bonds, or assessing interest rate risk, these two concepts are the backbone of modern financial analysis. Despite their importance, many investors struggle to scratch the surface of what they really mean and how to interact with them.\n\nIn the previous chapters, we have seen how loans and bonds work, and how we can create more complex financial instruments from them. So far, we have worked with fixed interest rates, but this is not representative of the real world. In more realistic settings, interest rates differ across terms. This variation in interest rate is called the “term structure” of interest rates. With the term structure, we can directly construct the forward rates. The forward rates will help us interpret what the yield means about future interest rates.\n\nIn this post, we’ll break down how yield curves work, what forward rates really represent, and why understanding both is essential for interpreting the market’s view of the future. Then we’ll put theory into practice by running simulations, analyzing different curve shapes, and generating visualizations with Python. By the end, you’ll understand these concepts and have hands-on experience exploring them with real data and code.\n\n# **Constructing Yield Curves**\n\nThe yield curve is a foundational object in fixed-income markets. It summarizes how interest rates vary with maturity and serves as a critical input for bond pricing, risk management, and the valuation of more complex financial instruments. Despite its importance, the yield curve is not directly observed in the market. Instead, it must be constructed from the prices of traded securities at different points in time. Each of these points on the yield curve represents the interest rate required to exactly reproduce the observed price of a zero-coupon bond ($ZSB$) at a given maturity. The price of a zero-coupon bond maturing at time $T$ with par value $X$ can be written as: \n\n$$\nP_{T}= \\frac{X}{(1+r_{T})^T}\n$$\n\nIn this notation $r_{T}$ is the yield corresponding to maturity $T$. It would be easy to construct the yield curve if we had $ZSB$s at every maturity. We would simply observe each $ZSB$ price and back out the corresponding yield. In practice, however, this ideal situation does not exist. There are two main challenges:\n\n1. **Incomplete maturities:** Bonds do not mature on every possible date.\n    1. *Solution:* Estimate missing maturities using interpolation\n2. **Coupon bonds dominate the market:** Many traded bonds pay coupons rather than being pure zeros.\n    1. *Solution:* Synthetically extract $ZSB$ prices from coupon bond prices using a procedure known as bootstrapping.\n\nThe overall approach to yield curve construction can be summarized as follows:\n\n1. **Bootstrap zero-coupon prices** from observed bond prices for as many maturities as possible.\n2. **Interpolate** these zero-coupon prices to obtain values at maturities that are not directly observed.\n3. **Address practical implementation issues,** such as market conventions and irregular cash-flow dates.\n\nTo understand bootstrapping, it is useful to first ignore interpolation issues and assume that we have sufficient short-dated instruments. The procedure for bootstrapping is:\n\n- Start with Treasury bills (or other short-dated instruments) at the short end of the curve, which are effectively zero-coupon bonds.\n- Use the prices of these short-maturity $ZSB$s to compute the present value of coupon payments on longer-dated bonds.\n- Subtract the present value of all coupon payments from the observed price of a coupon bond.\n- Divide by the principal repayment at maturity to obtain the implied price of a zero-coupon bond for that maturity.\n\nOnce we have a collection of $ZSB$ prices across maturities, we can invert them to obtain the corresponding yields. This produces the yield curve from short to long maturities.\n\n### Using interpolation\n\nBootstrapping works best when prices are available at many well-spaced maturities. As already mentioned, zero-coupon bonds are rarely issued on every coupon payment date. To make the procedure workable, we must estimate prices at intermediate maturities using interpolation. While many sophisticated interpolation methods exist, for now we will use the basic variant of linear interpolation. Linear interpolation works as follows: Suppose we have observed two points on the curve ($x_{1}$, $y_{1}$) and ($x_{2}$, $y_{2}$), and wish to estimate the value of $y$ at some $x$ between $x_{1}$ and $x_{2}$. Linear interpolation gives: \n\n$$\ny(x) = y_{1} + \\frac{y_{2} - y_{1}}{x_{2} - x_{2}} (x-x_{1})\n$$\n\nThis formula simply traces the straight line connecting the two known data points. Substituting $x=x_{1}$ yields $y=y_{1}$ and substituting  $x=x_{2}$ yields $y=y_{2}$. Although it&#39;s a basic form of interpolation, it&#39;s also easy to understand and often sufficient for illustratice purpose. Linear interpolation is also what we will use in our example. \n\n### Yield curve example\n\nFor our example, we will use the bootstrapping approach. Proceed from short to long maturities using already bootstrapped $ZCB$ prices. With the short $ZCB$s, we can turn longer-dated coupon bonds into $ZCB$s. For example:\n\n- A zero-coupon bond with par value $X$ and maturity of 6 months has price $P_{0.5}$.\n- A coupon bond with par value $X$, annual coupon rate $c$, and maturity of 1 year pays coupons of $\\frac{cX}{2}$ at 6 months and 1 year, and has a market price $P$.\n\nThe price of a synthetic 1-year $ZCB$ with par $X$ is then obtained by subtracting the present value of the 6-month coupon from the bond price:\n\n$$\n\\frac{P_{1}^c -0.5cP_{0.5}}{1+0.5c}\n$$\n\nThe price today of a par $X$ $ZCB$ maturing at $T$ is $P_{T}$.\n\n$$\nX=P_{T}(1+\\frac{r}{2})^{2T}\n$$\n\nSo\n\n$$\nr = 2 [(\\frac{X}{P_{T}})^{-2T}-1]\n$$\n\nSo we can get the maturity $T$ **$ZCB$ price with an analytic expression. This method works well if you have many prices, well spaced across maturities. We can perform these calculations and construct the yield curve using a Python script. \n\n```python\nimport numpy as np\nimport pandas as pd\n\n# Input market data\nPAR = 100.0\n\n# Zero-coupon bonds directly observed\n# maturity (years) -&gt; price\nzcb_prices = {\n    0.5: 98.0\n}\n\n# Coupon bonds:\n# maturity (years), annual coupon rate, market price\ncoupon_bonds = [\n    (1.0, 0.04, 99.0),\n    (2.0, 0.05, 101.0),\n    (3.0, 0.055, 102.0)\n]\n\n# Bootstrapping ZCB prices\ndef bootstrap_zcb_prices(zcb_prices, coupon_bonds):\n    \&quot;\&quot;\&quot;\n    Bootstrap zero-coupon bond prices from coupon bond prices.\n    \&quot;\&quot;\&quot;\n    zcb = dict(zcb_prices)  # copy so we can add to it\n\n    for maturity, coupon_rate, price in coupon_bonds:\n        coupon = coupon_rate * PAR\n\n        pv_coupons = 0.0\n        for t in sorted(zcb.keys()):\n            if t &lt; maturity:\n                pv_coupons += coupon * zcb[t]\n\n        # Price of synthetic ZCB at maturity\n        zcb_price = (price - pv_coupons) / (PAR + coupon)\n        zcb[maturity] = zcb_price\n\n    return zcb\n\nzcb_prices = bootstrap_zcb_prices(zcb_prices, coupon_bonds)\n\n# Convert ZCB prices to yields\ndef zcb_price_to_yield(price, maturity, par=PAR):\n    return (par / price) ** (1.0 / maturity) - 1.0\n\nyields = {\n    t: zcb_price_to_yield(p, t)\n    for t, p in zcb_prices.items()\n}\n\n# Linear interpolation\ndef linear_interpolate(x, x1, y1, x2, y2):\n    return y1 + (y2 - y1) * (x - x1) / (x2 - x1)\n\ndef interpolate_curve(curve, target_maturities):\n    known = sorted(curve.items())\n    result = {}\n\n    for x in target_maturities:\n        for (x1, y1), (x2, y2) in zip(known[:-1], known[1:]):\n            if x1 &lt;= x &lt;= x2:\n                result[x] = linear_interpolate(x, x1, y1, x2, y2)\n                break\n\n    return result\n\n# Interpolate yields at quarterly maturities\ntarget_maturities = np.arange(0.5, 3.01, 0.25)\ninterpolated_yields = interpolate_curve(yields, target_maturities)\n\n# Output\ndf = pd.DataFrame({\n    \&quot;Maturity (Years)\&quot;: sorted(interpolated_yields.keys()),\n    \&quot;Yield\&quot;: [interpolated_yields[t] for t in sorted(interpolated_yields)]\n})\n\nprint(\&quot;Bootstrapped Zero-Coupon Prices:\&quot;)\nfor t in sorted(zcb_prices):\n    print(f\&quot;T={t:.2f}  Price={zcb_prices[t]:.4f}\&quot;)\n\nprint(\&quot;\\nInterpolated Yield Curve:\&quot;)\nprint(df.to_string(index=False))\n```\n\nThis script will output\n\n```python\nBootstrapped Zero-Coupon Prices:\nT=0.50  Price=98.0000\nT=1.00  Price=-2.8173\nT=2.00  Price=-3.5706\nT=3.00  Price=-3.8092\n\nInterpolated Yield Curve:\n Maturity (Years)                 Yield\n             0.50   0.041233+ 0.000000j\n             0.75 -18.226824+ 0.000000j\n             1.00 -36.494881+ 0.000000j\n             1.25 -27.621160+ 1.323028j\n             1.50 -18.747440+ 2.646057j\n             1.75  -9.873720+ 3.969085j\n             2.00  -1.000000+ 5.292113j\n             2.25  -0.628493+ 4.612554j\n             2.50  -0.256986+ 3.932994j\n             2.75   0.114521+ 3.253435j\n             3.00   0.486028+ 2.573875j\n```\n\nThe script starts at short maturities with the observed 6-month ZCB. We then bootstrap longer maturities by discounting known coupon payments using previously bootstrapped ZCB prices. Subtracting their present value from the bond price and solving for the implied ZCB price at maturity. With this, we can invert ZCB prices to yields and use interpolation to fill in any missing maturities. From the output of this script, we can see that yields increase with bond maturity. \n\n# Finding forward rates\n\nWhile the yield curve summarizes the cost of borrowing over different maturities as of today, it does not directly tell us how the market prices future borrowing between two dates. Forward rates fill this gap. A forward rate is the interest rate implied by today&#39;s yield curve for a loan that starts at a future date and ends at a later date. So, forward rates translate the information embedded in the yield curve into expectations about the term structure of interest rates over time. In this section, we show how forward rates are derived from zero-coupon bond prices and yields. \n\nIt is often known if money will be needed at some time in the future. But this also means we should know what the interest rate will be at that point in the future. For example, somebody may have arranged to buy a new house at time $t_{1}$ **in the future and sell an old house at time $T &lt; t_{1}$. They will need the sale of the old house to finance the purchase of the new house. So, at time $t_{0} &lt; t_{1}$*,* they can request a bridging loan. The interest on this loan will be the forward rate. \n\nWe can infer the forward rate from the yield curve using simple no-arbitrage arguments. The main idea is to investigate over a long horizon where in one step must yield the same return as if we were to take shorter steps of that same horizon. If this is not the case, arbitrage opportunities would exist. \n\nOnce more let $P_{t}$ **denote the price today of a $ZCB$ that pays out at maturity $T$. Consider two maturities $T_{1} &lt; T_{2}$*.* A forward rate $f(T_{1}, T_{2})$ **is the interest rate, agreed upon today, for a loan that starts at $T_{1}$ **and matures at time  $T_{2}$. We can obtain the no arbitrage relationship by combining two strategies. \n\n1. **Invest directly to $T_{2}$:**\n    1. Buy a $ZCB$ maturing at $T_{2}$  at price $P_{T_{2}}$*.*\n2. **Invest to $T_{1}$*,* then reinvest forward to $T_{2}$*:***\n    1. Buy a $ZCB$ maturing at $T_{1}$ **at price $P_{T_{1}}$*,* and at time $T_{1}$ **reinvest the proceeds at the forward rate $f(T_{1}, T_{2})$*.*\n\nNo arbitrage implies that the value today of both strategies must be equal. This leads to:\n\n$$\nP_{T_{2}} = P_{T_{1}} * \\frac{1}{1+f(T_{1}, T_{2})}^{(T_{2} - T_{1})}\n$$\n\nRearranging, the forward rate implied by the yield curve is\n\n$$\nf(T_{1}, T_{2}) = (\\frac{P_{T_{1}}}{P_{T_{2}}})^{\\frac{1}{T_{2} - T_{1}}} - 1\n$$\n\nThus, forward rates are fully determined by zero-coupon bond prices. We can write a small python script that takes a bootstrapped yield curve as input to calculate the forward rate. \n\n```python\nimport numpy as np\nimport pandas as pd\n\n# Input: spot yield curve\n# maturity (years) -&gt; spot rate\nspot_rates = {\n    0.5: 0.020,\n    1.0: 0.025,\n    1.5: 0.028,\n    2.0: 0.030,\n    2.5: 0.032,\n    3.0: 0.033\n}\n\n# Forward rate calculation\ndef forward_rate(T1, T2, r1, r2):\n    \&quot;\&quot;\&quot;\n    Computes the forward rate f(T1, T2)\n    using annual compounding.\n    \&quot;\&quot;\&quot;\n    return ((1 + r2) ** T2 / (1 + r1) ** T1) ** (1 / (T2 - T1)) - 1\n\n# Compute consecutive forward rates\nmaturities = sorted(spot_rates.keys())\nforwards = []\n\nfor i in range(len(maturities) - 1):\n    T1 = maturities[i]\n    T2 = maturities[i + 1]\n    fwd = forward_rate(T1, T2, spot_rates[T1], spot_rates[T2])\n    forwards.append((T1, T2, fwd))\n\n# Output\ndf = pd.DataFrame(\n    forwards,\n    columns=[\&quot;Start (Years)\&quot;, \&quot;End (Years)\&quot;, \&quot;Forward Rate\&quot;]\n)\n\nprint(df.to_string(index=False))\n\n```\n\nThis outputs\n\n```python\n Start (Years)  End (Years)  Forward Rate\n           0.5          1.0      0.030025\n           1.0          1.5      0.034026\n           1.5          2.0      0.036023\n           2.0          2.5      0.040039\n           2.5          3.0      0.038015\n```\n\nSince $ZCB$ prices themselves function as spot zero yields, we can also express forward rates directly in terms of yields. Recall that:\n\n$$\nP_{T_{2}} = \\frac{1}{(1 + r_{T})^T}\n$$\n\nWhere $r_{T}$ **is the spot rate for maturity $T$*.* So with this in the script we have substituted into the forward rate formula giving:\n\n$$\nf(T_{1}, T_{2}) = (\\frac{(1 + r_{T_{2}})^{T_{2}}}{(1 + r_{T_{1}})^{T_{1}}})^{\\frac{1}{T_{2} - T_{1}}} - 1\n$$\n\nForward rates are implied by current market prices; they are not forecasts in statistical sense. Instead, they represent the rates that make investors indifferent. Under additional assumptions such as risk neutrality we can interpret forward rates as expectations of future spot rates.\n\nOnce a smooth yield curve has been constructed via bootstrapping and interpolation, forward rates can be computed for any pair of maturities. This makes forward rates a powerful tool for analyzing the slope and dynamics of the term structure of interest rates.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/4-yields.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/yield-curves-and-forward-rates-explained.png&quot;]]],&quot;digest&quot;:[0,&quot;5ffa1d080e93e404&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/4-yields&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/5-stocks.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;stock-analysis-python-prices-returns-distribution&quot;],&quot;title&quot;:[0,&quot;Stock Analysis with Python: Modeling Prices, Returns &amp; Distributions&quot;],&quot;description&quot;:[0,&quot;Learn how to analyze stock market data using Python. This guide covers calculating daily returns, visualizing volatility, and modeling statistical distributions with yfinance.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,5],&quot;shortname&quot;:[0,&quot;Stocks&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2026-01-18T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Python&quot;],[0,&quot;Finance&quot;],[0,&quot;Data Visualization&quot;],[0,&quot;Statistics&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;stock analysis&quot;],[0,&quot;daily returns&quot;],[0,&quot;normal distribution&quot;],[0,&quot;yfinance&quot;],[0,&quot;volatility&quot;],[0,&quot;AAPL&quot;],[0,&quot;pandas&quot;],[0,&quot;stationarity&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/understanding-stock-prices-banner.eRZSuP4F.png&quot;],&quot;width&quot;:[0,3168],&quot;height&quot;:[0,1344],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner illustration displaying financial stock charts and data analysis trends&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2026-01-18T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In previous posts in this series, we have looked at loans, bonds, and other financial securities built around these products. Many of the securities we have not discussed yet depend on stock pricing. To understand the behavior of different investment strategies, we will discuss it is crucial that we understand how stock pricing works.\n\nBefore we can understand how pricing works, we need to know that a stock represents fractional ownership in a company, with limited liability. The value of a stock can be estimated by combining a traditional balance sheet analysis with day-to-day fluctuations in response to market buy and sell pressures.\n\n# **Modeling and analyzing real stock data**\n\nTo better understand what a stock is and how it behaves, we will model and analyze the price of a real stock. Let’s analyse Apple&#39;s historical data.Inc. For our analysis, we will use daily data from the last 5 years, including Open, Close, Low, Volume, and Adjusted Close for each day. The difference between the Close and Adjusted Close value is that the Adjusted Close incorporates the effect of dividends on the value of the stock. For our analysis, we will focus on the Adjusted Close. First lets plot all our data points. If you want to know how to do that, have a look at the post *“Data science with Python”*.\n\n```python\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# &#39;Adj Close&#39; is preferred as it accounts for dividends and stock splits\naapl_data = yf.download(&#39;AAPL&#39;, period=&#39;5y&#39;)\ndf = aapl_data[[&#39;Adj Close&#39;]].copy()\n\n# Plot the Price History\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df[&#39;Adj Close&#39;], label=&#39;AAPL Adjusted Close&#39;, color=&#39;blue&#39;)\nplt.title(&#39;Apple (AAPL) Stock Price - Last 5 Years&#39;)\nplt.xlabel(&#39;Date&#39;)\nplt.ylabel(&#39;Price (USD)&#39;)\nplt.grid(True, linestyle=&#39;--&#39;, alpha=0.5)\nplt.legend()\nplt.show()\n```\n\nWhich give the plot:\n\n![AAPL_Adjusted_Close.png](assets/AAPL_Adjusted_Close.png)\n\nBased on this plot, you probably think that the trade price looks pretty random, so how will we model this? To get a useful model its a good idea to look at the stock returns instead of the actual stock prices. The rationale is that the most important thing to an investor is the return on their investment. For example, a €10 stock changing €1 is very different than a €1000 stock changing €10. We can calculate the daily stock return using the formula:\n\n$$\nR_{t} = \\frac{P_{t} - P_{t-1}}{P_{t-1}}\n$$\n\nWhere:\n\n- $R_{t}$ = is the return at time $t$\n- $P_{t}$ = is the price at time $t$\n- $P_{t-1}$ = is the price of the previous day\n\nWhen we apply this formula to our Apple data and plot the results, the graph changes drastically. Instead of a line that wanders upwards over time, we see a graph that oscillates around zero. \n\n```python\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# pct_change() calculates (Price_t - Price_t-1) / Price_t-1\naapl_data = yf.download(&#39;AAPL&#39;, period=&#39;5y&#39;)\ndf[&#39;Daily Return&#39;] = df[&#39;Adj Close&#39;].pct_change()\n\n# Drop the first row (NaN) created by the calculation\ndf.dropna(inplace=True)\n\n# 4. Plot Daily Returns (Volatility)\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df[&#39;Daily Return&#39;], label=&#39;Daily Returns&#39;, color=&#39;orange&#39;, linewidth=0.7)\nplt.axhline(0, color=&#39;black&#39;, linewidth=0.5) # Add a line at 0 for reference\nplt.title(&#39;Apple (AAPL) Daily Stock Returns&#39;)\nplt.xlabel(&#39;Date&#39;)\nplt.ylabel(&#39;Return&#39;) # e.g., 0.02 means 2%\nplt.grid(True, linestyle=&#39;--&#39;, alpha=0.5)\nplt.legend()\nplt.show()\n\n```\n\nWhich give the plot:\n\n![AAPL_Daily_Returns.png](assets/AAPL_Daily_Returns.png)\n\nThis transformation is critical for modeling because it makes the data stationary. Or in simple terms, while the price of Apple can go anywhere, the daily return usually stays within a predictable range. Looking at the Apple data, we can observe two key characteristics. \n\n1. **Clustering:** Notice how the spikes tend to bunch together? This tells us that if Apple stock is volatile today, it is likely to be volatile tomorrow.\n2. **The range:** For a stable giant like Apple, the vast majority of daily moves fall between -2% and +2%.\n\nTo model the probability of making or losing money, we can use these daily return values to create a histogram. This histogram reveals the stock&#39;s “personality.” \n\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\naapl_data = yf.download(&#39;AAPL&#39;, period=&#39;5y&#39;)\ndf[&#39;Daily Return&#39;] = df[&#39;Adj Close&#39;].pct_change()\n\nplt.figure(figsize=(10, 6))\n\n# Plot the actual data histogram\n# bins=50 breaks the data into 50 \&quot;buckets\&quot;\nplt.hist(df[&#39;Daily Return&#39;], bins=50, density=True, alpha=0.6, color=&#39;green&#39;, label=&#39;Actual Returns&#39;)\n\n# Plot the Normal Distribution curve for comparison\nmu, std = df[&#39;Daily Return&#39;].mean(), df[&#39;Daily Return&#39;].std()\nx = np.linspace(df[&#39;Daily Return&#39;].min(), df[&#39;Daily Return&#39;].max(), 100)\np = stats.norm.pdf(x, mu, std)\nplt.plot(x, p, &#39;k&#39;, linewidth=2, label=&#39;Normal Distribution&#39;)\n\nplt.title(&#39;Distribution of Apple Daily Returns&#39;)\nplt.xlabel(&#39;Daily Return&#39;)\nplt.ylabel(&#39;Density&#39;)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Optional: Print statistics\nprint(f\&quot;Mean Return: {mu:.4f}\&quot;)\nprint(f\&quot;Standard Deviation (Volatility): {std:.4f}\&quot;)\n```\n\nWhich give the plot:\n\n![AAPL_Distribution.png](assets/AAPL_Distribution.png)\n\nFor Apple, our analysis shows a curve that looks very similar to a Normal Distribution, but with a few distinct features. \n\n- **The Mean:** The center of the curve is slightly to the right of zero. This reflects Apples log-term upward trend over the last 5 years.\n- **Fat Tails:** Unlike a perfect mathematical bell curve, real stock data has “fat tails”. This means extreme events, like a 5% drop in a single day due to a missed earnings event, happen more often than a standard normal distribution would predict.\n\nBy understanding that Apple’s return follows this distribution curve, we can start calculating risk. We know that roughly 68% of the time, tomorrows return will fall within one standard deviation of the average. This statistical model serves as the basis for more advanced models, such as Monte Carlo simulations and Value at Risk.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/5-stocks.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/understanding-stock-prices-banner.png&quot;]]],&quot;digest&quot;:[0,&quot;dd46efc929d14434&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/5-stocks&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;finding-p-values-and-false-hope/6-allocation.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;quantitative-portfolio-allocation-markowitz-efficient-frontier&quot;],&quot;title&quot;:[0,&quot;Portfolio Allocation: The Math Behind the Efficient Frontier&quot;],&quot;description&quot;:[0,&quot;Master the quantitative core of Modern Portfolio Theory. Learn how to use Markowitz optimization and Python to build an efficient frontier for asset allocation.&quot;],&quot;collection&quot;:[0,&quot;Finding P values and false hope&quot;],&quot;chapter&quot;:[0,6],&quot;shortname&quot;:[0,&quot;allocation&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2026-02-01T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Data Science&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Portfolio Optimization&quot;],[0,&quot;Markowitz Model&quot;],[0,&quot;Python&quot;],[0,&quot;Quantitative Finance&quot;],[0,&quot;Matrix Algebra&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;Mean-Variance Optimization&quot;],[0,&quot;Efficient Frontier&quot;],[0,&quot;Lagrange Multipliers&quot;],[0,&quot;Asset Allocation Python&quot;],[0,&quot;Portfolio Variance Matrix&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/portfolio-allocation-banner.CFnovJea.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Robot explaining finance with statistics&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2026-02-01T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In the previous posts, we have already dipped our toes into diversifying our portfolio. But since that post, we have looked at different financial investment instruments. We can invest in stock, government bonds, corporate bonds, and even options. We can also buy these on many markets in countries such as Japan, Germany, the United Kingdom, the United States, and Canada. With all these options, a sensible investor will probably try to diversify their investment portfolio. \r\n\r\nWhen faced with all these investment choices, what do we do? In this post, we will explore a quantitative model for portfolio allocation. It is a model that provides the conceptual core for modern asset allocation. This model was developed by Markowitz and refined by Sharpe, and earned the Nobel Prize in Economics. \r\n\r\n# **Foundations of Portfolio Allocation**\r\n\r\nBefore we dive into the mechanics of quantitative allocation, we must establish *the rules of the game*. We begin with a set of simplifying assumptions that allow us to build a mathematically tractable model.\r\n\r\n### Core Assumptions\r\n\r\nWe assume a one-period investment decision. You invest your entire capital at $t=0$ and “cash out” entirely at time $T$. Within this universe, we assume:\r\n\r\n- **$N$ Assets**: The investment universe consists of *N assets*. This could be the entire market or some subset of the market after we have done some kind of screening (e.g., liquidity filters or ethical criteria).\r\n- **Joint Normality:** The simple returns of these assets are jointly normally distributed.\r\n    - **Mean Returns:** Represented by an  $N \\times 1$ column $\\mu$.\r\n    - **Risk:** Represented by an $N \\times N$ variance-covariance matrix $V$.\r\n- **Frictionless Trading:** An investor with initial wealth $W$ can invest in any asset without restriction and without impacting market prices.\r\n- **Unrestricted Short Selling:** Investors can short any asset and use the proceeds to fund other positions.\r\n\r\n### **Why the One-Period Model?**\r\n\r\nWe use a one-period model for its simplicity. This one-period model does ignore the reality that investors can rebalance their portfolios in response to new information. However, this approach dramatically simplifies the calculus, bypassing the headache of transaction costs, and still provides a clear baseline for understanding diversification. \r\n\r\n### **Simple returns**\r\n\r\nIn derivatives pricing, we often prefer log returns because they exhibit better long-term behavior. However, in portfolio construction, simple returns are superior because they are additive across assets. Suppose you invest A dollars in stock $S_{1}$ with an annualized simple return $\\mu_{1}$. Over time, $T$*:*\r\n\r\n- **Value at $t=0: V_{0} = A$**\r\n- **Value at $t=T: V_{t} = A(1+\\mu_{1}T)$**\r\n\r\nIf we split our wealth across two assets ($A_{1}$ and $A_{2}$), our total portfolio value at time $T$ is:\r\n\r\n$$\r\nV_{t} = A_{1}(1+\\mu_{1}T) + A_{2}(1+\\mu_{2}T) \r\n$$\r\n\r\nIf we define our weight as $x_{1} = \\frac{A_{1}}{A}$ and $x_{2} = \\frac{A_{2}}{A}$*,* the total portfolio return $R_{p}$ becomes a clean linear combinations:\r\n\r\n$$\r\nR_{p} = x_{1}\\mu{1} + x_{2}\\mu_{2}\r\n$$\r\n\r\nIf we try the same thing with log returns *( $\\beta$ ),* the math gets ugly fast. The portfolio value would be:\r\n\r\n$$\r\nAe^{\\beta T} = A_{1}e^{\\beta_{1}T}+A_{2}e^{\\beta_{2}T}\r\n$$\r\n\r\nSolving for the portfolio return $\\beta$*:*\r\n\r\n$$\r\n\\beta = \\frac{1}{2}ln(x_{1}e^{\\beta_{1}T} + x_{2}e^{\\beta_{2}T})\r\n$$\r\n\r\nThis nonlinear relationship is incredibly difficult to optimize. Unless $T$ is so small that we can use a Taylor series approximation (where $e^x \\approx 1 + x$ ), log returns make portfolio aggregation mathematically messy.\r\n\r\nWe assume returns are normally distributed primarily for mathematical tractability. While fat tails of real-world markets often challenge this assumption, the normal hypothesis allows us to define a portfolio&#39;s entire return profile using the mean and the variance.\r\n\r\n# **Building the Risk and Return Model**\r\n\r\nNow that we have established the core assumptions of our model, let&#39;s move to the mathematical implementation. To optimize a portfolio, we must first define our variables and the mathematical model that balances the desire for profit with the reality of risk.\r\n\r\nOur primary control variable is the **allocation vector**, denoted as $N \\times 1$  column vector $X.$ Each element $x_{1}$represents the fraction of total wealth invested in asset $i.$ Because we are dealing with proportions of a whole, our first physical constraint is that the sum of these fractions must equal 1 (100% of our capital). In matrix notation, this is expressed using a vector of ones:\r\n\r\n$$\r\n1^TX=1\r\n$$\r\n\r\nWhile we could add a constraint that no short selling is allowed ($x_{i} \\ge 0$), we will first solve the unconstrained version to see the market&#39;s pure geometry. \r\n\r\nWe know we want a high Expected Return, calculated simply as $R_{p} = \\mu^TX$*.* However, maximizing return alone is dangerous without considering risk; the model would simply tell you to put 100% of your money into the single most volatile, high-growth stock. To measure risk, we use the portfolio&#39;s variance. If we have two stocks, the variance is influenced not just by their individual volatility ( $\\sigma_{1}, \\sigma_{2}$), but also by how they move together. So, for a portfolio of $N$ **assets, we capture these relationships in the Variance-Covariance Matrix (V). This symmetric matrix allows us to express the total portfolio risk as a quadratic form:\r\n\r\n$$\r\nPortfolio Variance = X^TVX\r\n$$\r\n\r\nThe goal of the Mean-Variance Optimization is to find the specific allocation $X$ that minimizes risk for a target level of return $R$. So, mathematically, we want to:\r\n\r\n- **Minimize: $X^TVX$**\r\n- **Subject to:**\r\n    - **$\\mu^TX = R$ (T**he portfolio must hit our target return)\r\n    - $1^TX=1$ (The budget constraint)\r\n\r\nTo solve this, we use the method of Lagrange Multipliers. By introducing two multipliers ( $\\lambda_{1}$  and $\\lambda_{2}$) for our two constraints, we can transform this into an unconstrained problem. Taking first-order derivatives and solving the linear system yields a crucial discovery: the relationship between risk and return is not linear; it is hyperbolic. The solution for the variance of the optimal portfolio can be simplified to the following form:\r\n\r\n$$\r\n\\sigma_{p}^2 = \\frac{aR^2 - 2bR + c}{ac - b^2}\r\n$$\r\n\r\nThis equation describes a hyperbola in the risk-return space, famously known as the **Efficient Frontier**. Every point on this curve represents a portfolio that offers the lowest possible risk for that specific level of return. In the formula, $a$*, $b$,* and $c$ are so-called “summary statistics” of the entire market’s investment opportunity set. They depend strictly on the expected returns and the risk relationships ($V^{-1}$) of the assets you’ve chosen.\r\n\r\n| **Constant** | **Formula** | **Conceptual Meaning** |\r\n| --- | --- | --- |\r\n| $a$ | $1^TV^{-1}1$ | **Information Density:** Represents the sum of al elements in the inverse covariance matrix. It scales the total precision of the market. |\r\n| $b$ | $\\mu^TV^{-1}1$ | **Return-Weighted Risk:** Measures the relationship between the assets expected returns and their risk structure. |\r\n| $c$ | $\\mu^TV^{-1}\\mu$ | **Return Intensity:** Indicates the potential for return relative to the risk structure of the assets. |\r\n\r\nWe can now translate the matrix algebra into functional code. We can perform all the matrix operations using numpy and pandas. In this script, we will use four hypothetical assets, calculate the $a$*, $b$,* and $c$ constants, and then derive the Optimal Allocation Vector ($X$) for a target return.\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ndef optimize_portfolio(returns, cov_matrix, target_return):\r\n    \&quot;\&quot;\&quot;\r\n    Calculates the optimal weights using the Lagrange Multiplier analytical solution.\r\n    \&quot;\&quot;\&quot;\r\n    # Prepare the inputs\r\n    V = np.matrix(cov_matrix)\r\n    V_inv = V.I\r\n    mu = np.matrix(returns).T\r\n    ones = np.matrix(np.ones(len(returns))).T\r\n    R = target_return\r\n\r\n    # Calculate the Market Constants (a, b, c)\r\n    a = (ones.T * V_inv * ones).item()\r\n    b = (mu.T * V_inv * ones).item()\r\n    c = (mu.T * V_inv * mu).item()\r\n    det = a * c - b**2\r\n\r\n    # Solve for Lagrange Multipliers (lambda 1 and lambda 2)\r\n    # Based on the system:\r\n    # lambda1 * a + lambda2 * b = 1\r\n    # lambda1 * b + lambda2 * c = R\r\n    lambda_2 = (a * R - b) / det\r\n    lambda_1 = (c - b * R) / det\r\n\r\n    # Calculate Optimal Weights (X)\r\n    # Equation: X = V^-1 * (lambda_1 * 1 + lambda_2 * mu)\r\n    weights = V_inv * (lambda_1 * ones + lambda_2 * mu)\r\n\r\n    return np.array(weights).flatten(), a, b, c\r\n\r\ndef plot_efficient_frontier(cov_matrix, a, b, c):\r\n\t\t\&quot;\&quot;\&quot;\r\n\t\tPlot the efficient frontier chart\r\n\t\t\&quot;\&quot;\&quot;\r\n    # Define the Frontier range (around the Global Min Variance Portfolio)\r\n    r_min = b / a\r\n    target_returns = np.linspace(r_min - 0.05, r_min + 0.1, 200)\r\n\r\n    # Analytical Variance: σ² = (aR² - 2bR + c) / (ac - b²)\r\n    variances = (a * target_returns**2 - 2 * b * target_returns + c) / b**2\r\n    volatilities = np.sqrt(variances)\r\n\r\n    plt.figure(figsize=(10, 6))\r\n    plt.plot(volatilities, target_returns, color=&#39;blue&#39;, linestyle=&#39;--&#39;, label=&#39;Min-Var Frontier&#39;)\r\n\r\n    # The Efficient Frontier is the upper branch (Returns &gt;= r_min)\r\n    efficient_idx = target_returns &gt;= r_min\r\n    plt.plot(volatilities[efficient_idx], target_returns[efficient_idx],\r\n             color=&#39;darkblue&#39;, linewidth=3, label=&#39;Efficient Frontier&#39;)\r\n\r\n    # Mark the Global Minimum Variance Portfolio (GMVP)\r\n    plt.scatter(np.sqrt(1/a), r_min, color=&#39;red&#39;, label=&#39;GMVP&#39;, zorder=5)\r\n\r\n    # Plot individual assets\r\n    plt.scatter(np.sqrt(np.diag(cov_matrix)), asset_returns, marker=&#39;X&#39;, color=&#39;black&#39;, label=&#39;Assets&#39;)\r\n\r\n    plt.title(&#39;The Markowitz Efficient Frontier&#39;)\r\n    plt.xlabel(&#39;Risk (Volatility)&#39;)\r\n    plt.ylabel(&#39;Expected Return&#39;)\r\n    plt.legend()\r\n    plt.grid(True, linestyle=&#39;:&#39;, alpha=0.6)\r\n    plt.savefig(&#39;efficient_frontier_plot.png&#39;)\r\n\r\n# --- Example Usage ---\r\n\r\n# Assume 4 assets with expected annual returns\r\nasset_returns = np.array([0.08, 0.12, 0.15, 0.10])\r\n\r\n# Assume a stylized Covariance Matrix\r\ncov_data = [\r\n    [0.005, 0.002, 0.001, 0.001],\r\n    [0.002, 0.008, 0.003, 0.002],\r\n    [0.001, 0.003, 0.012, 0.002],\r\n    [0.001, 0.002, 0.002, 0.007]\r\n]\r\n\r\ntarget = 0.11  # We want an 11% return\r\nweights, a, b, c = optimize_portfolio(asset_returns, cov_data, target)\r\n\r\nprint(f\&quot;Optimal Weights for {target*100}% Return:\&quot;)\r\nfor i, w in enumerate(weights):\r\n    print(f\&quot; Asset {i+1}: {w*100:.2f}%\&quot;)\r\n\r\n# Calculate resulting portfolio variance\r\nportfolio_variance = (a * target**2 - 2 * b * target + c) / (a * c - b**2)\r\nprint(f\&quot;\\nPortfolio Volatility (Std Dev): {np.sqrt(portfolio_variance)*100:.2f}%\&quot;)\r\n\r\n# Plot the chart\r\nplot_efficient_frontier(cov_data, a, b, c)\r\n```\r\n\r\nThis script will output:\r\n\r\n```python\r\nOptimal Weights for 11.0% Return:\r\n Asset 1: 30.29%\r\n Asset 2: 20.79%\r\n Asset 3: 23.80%\r\n Asset 4: 25.11%\r\n```\r\n\r\nIn addition to this output, we also visualized the relationship between risk and return. By plotting the analytical solution derived from our constants $a$*, $b$,* and $c$ *,* we can see exactly where the best portfolios lie.\r\n\r\n![efficient_frontier.png](assets/efficient_frontier.png)\r\n\r\nIn this plot, the dashed line represents all portfolios that have the minimum possible variance for a given return. In red, the Global Minimum Variance Portfolio is marked. This is the single point on the graph with the lowest possible risk ( *$\\frac{1}{\\sqrt{a}}$*). All the individual assets are marked with “$X$” and fall inside the curve.\r\n\r\nWhile the math produces the full hyperbola, an investor would never choose a portfolio on the lower branch. Because why take the same risk for a lower return? The true Efficient Frontier is the upper part of the blue curve. This chart visually proves the point that a diversified portfolio can provide a better risk-return profile than any single asset held in isolation.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/finding-p-values-and-false-hope/6-allocation.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/portfolio-allocation-banner.png&quot;]]],&quot;digest&quot;:[0,&quot;f5e48599cf435b39&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;finding-p-values-and-false-hope/6-allocation&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;hello-world/hello-world-asm.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;x86-64-assembly-docker-guide&quot;],&quot;title&quot;:[0,&quot;x86-64 Assembly Hello World: The Complete Docker-Based Guide&quot;],&quot;description&quot;:[0,&quot;Stop fighting environment errors. Learn x86-64 Assembly with a zero-setup Docker environment. Includes NASM code, syscall explanations, and a one-click build script.&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-10-19T00:00:00.000Z&quot;],&quot;update&quot;:[3,&quot;2026-02-08T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Programming&quot;]]],&quot;tags&quot;:[1,[[0,&quot;x86-64&quot;],[0,&quot;Assembly&quot;],[0,&quot;Docker&quot;],[0,&quot;NASM&quot;],[0,&quot;Linux Syscalls&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;x86_64 assembly docker tutorial&quot;],[0,&quot;how to run assembly on apple silicon mac&quot;],[0,&quot;nasm x86_64 linux syscalls example&quot;],[0,&quot;setup assembly dev environment fast&quot;],[0,&quot;assembly programming without installing nasm&quot;],[0,&quot;dockerize assembly language development&quot;],[0,&quot;nasm elf64 hello world tutorial&quot;],[0,&quot;nasm x86-64 linux hello world example&quot;],[0,&quot;hello world in x86-64 assembly nasm linux&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/whale-behind-a-desk-with-containers.DX4KWUkX.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Whale behind a desk with containers&quot;],&quot;draft&quot;:[0,false],&quot;steps&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Build the x86-64 Docker Environment&quot;],&quot;text&quot;:[0,&quot;Create a Dockerfile using the Ubuntu base image and install NASM, GCC, and Build-Essential to handle assembly compilation.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Write the Assembly Source Code&quot;],&quot;text&quot;:[0,&quot;Draft the hello.asm file using the .data section for your string and the .text section for your logic.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Invoke Linux Syscalls&quot;],&quot;text&quot;:[0,&quot;Use the &#39;rax 1&#39; register for the write syscall and &#39;rax 60&#39; for a clean exit.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Assemble and Link the Executable&quot;],&quot;text&quot;:[0,&quot;Run NASM to generate an ELF64 object file and use the ld linker to create the final executable.&quot;]}]]]}],&quot;body&quot;:[0,&quot;We’ve all seen the classic *“Hello, World!”* programs. But let’s be honest, you’re not a true Giga Chad until you’ve written one in **pure assembly**.\n\n## Why Learn Assembly in 2026?\n\nYou might be asking yourself: *Why on earth would anyone learn assembly when high-level languages exist to make life easier?* The truth is, there are situations outside of programming a [space probe](https://science.nasa.gov/mission/voyager/frequently-asked-questions/) where assembly is still relevant. For example, imagine you’re working on a collision detection system for a self-driving car. In that scenario, every microsecond matters. You want to squeeze out every drop of performance to avoid disaster. Of course, this doesn’t mean writing the entire system in assembly. In practice, you’d only optimise the performance-critical parts, while leaving the rest in a higher-level language. And knowing how things work under the hood is always valuable\n\n## Environment\n\nBefore we start coding, we need to set up a proper environment. Let’s create a project directory. Throughout this tutorial, we’ll call it `hello-world-asm`.  To make sure our code runs anywhere (not just on my machine), we’ll use a Docker container with all the necessary tools for building and running assembly. If you don’t already have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed, go ahead and do that first. For your editor/IDE, feel free to use whatever you like; it doesn’t really matter. In this tutorial, I’ll be using [Visual Studio Code](https://code.visualstudio.com/Download), but it’s not a requirement.\n\n### Setting up the Docker Assembly Environment\n\nInside your `hello-world-asm` directory, create a file named `Dockerfile`:\n\n```docker\nFROM ubuntu:latest\n\n# Build-time variable for noninteractive installs\nARG DEBIAN_FRONTEND=noninteractive\n\n# Install only the essentials for x86_64 assembly\nRUN apt update &amp;&amp; apt upgrade -y &amp;&amp; \\\n    apt install -y --no-install-recommends \\\n    build-essential \\\n    gdb \\\n    nasm &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Create non-root user \&quot;dev\&quot;\nRUN useradd -m -s /bin/bash dev &amp;&amp; \\\n    echo \&quot;dev ALL=(ALL) NOPASSWD: ALL\&quot; &gt;&gt; /etc/sudoers\n\nUSER dev\nWORKDIR /home/dev\n```\n\nLet’s break down what we just set up:\n\n- **Base image:** We start with `ubuntu:latest`, giving us a clean Linux environment.\n- **Non-interactive installs:** The `DEBIAN_FRONTEND=noninteractive` variable ensures `apt` doesn’t get stuck asking us questions.\n- **Install essentials:** We pull in just the tools we need for x86_64 assembly programming:\n    - `build-essential` → compiler, linker, common build tools (handy for your C/C++ projects too)\n    - `gdb` → the GNU Debugger, so we can step through our assembly code.\n    - `nasm` → the Netwide Assembler, the tool we use to assemble and link our `.asm` files.\n- **Non-root user:** We add a new user `dev`, so we don’t run everything as root (a good security practice).\n- **Working directory:** Finally, we set the container to drop us into `/home/dev`, which is where we’ll link our project directory to later.\n\n### Helper Script\n\nTyping Docker commands over and over can be a pain. To make life easier, let’s create a shell script to automate container management. Inside your `hello-world-asm` directory, create a file named `docker.sh` with the following content:\n\n```bash\n#!/bin/bash\n\nIMAGE_NAME=\&quot;dev-env\&quot;\nCONTAINER_NAME=\&quot;dev-env-container\&quot;\nCONTAINER_ENTRY=\&quot;/usr/bin/bash\&quot;\nWORK_DIR=\&quot;/home/dev/hello-world-asm\&quot;\n\nPROJECT_ROOT=\&quot;$(pwd)\&quot;\n\nfunction container_stop() {\n\tsudo docker stop $CONTAINER_NAME || echo \&quot;container already stopped\&quot;\n}\n\nfunction clean() {\n\tcontainer_stop\n\tsudo docker rm $CONTAINER_NAME || echo \&quot;container already removed\&quot;\n\tsudo docker image rm $IMAGE_NAME  || echo \&quot;image already removed\&quot;\n}\n\nfunction container_start() {\n\tif [ -z \&quot;$(sudo docker ps -a | grep $CONTAINER_NAME)\&quot; ]; then\n\t\techo \&quot;&gt;&gt; creating dev container\&quot;\n\n\t\tsudo docker run -w $WORK_DIR --hostname dev -it --name $CONTAINER_NAME \\\n\t\t\t--net=host \\\n\t\t\t--cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n\t\t\t-v $PROJECT_ROOT:$WORK_DIR:z \\\n\t\t\t$IMAGE_NAME $CONTAINER_ENTRY\n\n\telse\n\t\techo \&quot;&gt;&gt; found existing dev container\&quot;\n\t\tsudo docker start $CONTAINER_NAME\n\t\tsudo docker exec -it $CONTAINER_NAME $CONTAINER_ENTRY\n\tfi\n}\n\nfunction image_build() {\n\tif [ -z \&quot;$(sudo docker images | grep $IMAGE_NAME)\&quot; ]; then\n\t\techo \&quot;&gt;&gt; building image: $IMAGE_NAME\&quot;\n\t\tsudo docker build -t $IMAGE_NAME ./\n\tfi\n}\n\nfunction print_usage() {\n\techo \&quot;Usage: ./docker.sh [container-start | container-stop | image-build | clean]\&quot;\n}\n\nfunction main() {\n\tif ! command -v docker &gt;/dev/null 2&gt;&amp;1; then\n\t\techo \&quot;Could not find docker. Please install it https://docs.docker.com/engine/install/\&quot;\n\t\texit 1\n\tfi\n\n\tcase \&quot;$1\&quot; in\n\t  container-start) container_start ;;\n\t  container-stop)  container_stop ;;\n\t  image-build)     image_build ;;\n\t  clean)           clean ;;\n\t  *)               print_usage ;;\n\tesac\n}\n\nmain $@\n```\n\nWhat this script does:\n\n- **`image-build`** Builds our Docker image from the `Dockerfile`.\n- **`container-start`** Starts a new container (or reuses an existing one) and drops you into a shell.\n- **`container-stop`** Stops the running container.\n- **`clean`** Removes the container and image, so you can start fresh.\n\nIn short, it hides the long Docker commands and gives you a simple interface.\n\n### Apple Silicon (M-Chips)\n\nIf you&#39;re on a Mac with Apple Silicon, your CPU is ARM64, but this tutorial targets x86_64 assembly. That means we must run our container using x86_64 emulation.\n\nIf you don’t do this, Docker may build the image for ARM, and the assembler/linker output won’t run correctly.\n\nBuild the image using the x86 platform:\n\n```bash\ndocker build --platform linux/amd64 -t dev-env .\n```\n\nIf you&#39;re using the helper script, update the build command in ``docker.sh``:\n\n```bash\nsudo docker build --platform linux/amd64 -t $IMAGE_NAME ./\n```\n\nAlso update the container run command. Inside ``container_start()``, change the docker run line to:\n\n```bash\nsudo docker run --platform linux/amd64 -w $WORK_DIR --hostname dev -it --name $CONTAINER_NAME \\\n```\n\n### First Run\n\nBefore we can use our container, we need to build the image. In your terminal (for Windows users, PowerShell works best), run:\n\n```bash\nsh docker.sh image-build   # Linux/macOS\nbash docker.sh image-build # Windows (PowerShell)\n```\n\nThen start the container with:\n\n```bash\nsh docker.sh container-start   # Linux/macOS\nbash docker.sh container-start # Windows (PowerShell)\n```\n\nThis will provide you with terminal access within your development environment. From here, we’re ready to write some assembly.\n\n## Assembly File Structure\n\nBefore we dive into coding, it’s important to understand how an assembly file is usually organized. Most assembly programs are divided into **sections**, each with a specific purpose. The exact details vary depending on the assembler and platform, but the structure is broadly similar everywhere.\n\nHere are the most common sections:\n\n- **`.data`** Holds variables and constants that are **initialised** when the program starts. For example, your “Hello, World!” string belongs here.\n- **`.bss`** Holds **uninitialised variables** (they start as zero by default). Think of this as a workspace where you can reserve space for data you’ll fill in later.\n- **`.text`** Contains the actual **instructions** of the program. This is where the CPU starts executing. The program’s entry point (often called `_start` on Linux or `main` in higher-level conventions) is defined in this section.\n\nA Simple Mental Model:\n\n- `.data` → “What I already know.”\n- `.bss` → “What I’ll figure out later.”\n- `.text` → “What I need to do.”\n\nFor our *Hello, World!* program, we only need two sections:\n\n- `.data` to store the message\n- `.text` to hold the instructions that display it\n\n## Writing Our First Assembly Program\n\nLet’s start building our first program! Inside your project directory, create a new file named `hello.asm`.\n\nThe first step is to set up the sections of our program. As we learned earlier, we’ll need:\n\n- a **`.data`** section for our message\n- a **`.text`** section for the instructions that display it.\n\nWe can declare these sections with the `section` keyword:\n\n```asm\nsection .data                           ; Create the data section\nsection .text                           ; Create the text section\n```\n\n### Adding Data\n\nOur program needs just two pieces of data:\n\n1. the **message** we want to print, and\n2. the **length** of that message.\n\nBecause this program is simple, we don’t need any dynamic memory management. We can store everything directly in the `.data` section. To define the message, we’ll use the assembler directive `db` (*define byte*). This instructs the assembler to allocate memory space and fill it with the values we provide. In this case, that’s the string `\&quot;Hello, World!\&quot;` followed by `0x0A`, which represents the newline character (`&#39;\\n&#39;`).\n\n```asm\n\nsection .data                           ; Create the data section\n    msg     db  \&quot;Hello, World!\&quot;, 0x0A   ; Our message plus a newline (0x0A = &#39;\\n&#39;)\n    \nsection .text                           ; Create the text section\n```\n\nNow we also need the **length of the message**. We can compute it automatically using the `equ` directive (*equate*). Unlike `db`, `equ` does not allocate memory. Instead, it defines a constant value that the assembler substitutes wherever the label is used. To get the length, we subtract the address of the beginning of the message (`msg`) from the current location counter (`$`). The symbol `$` always refers to the assembler’s current position, which in this case is right after the message we just defined.\n\n```asm\nsection .data                           ; Create the data section\n    msg     db  \&quot;Hello, World!\&quot;, 0x0A   ; Our message plus a newline (0x0A = &#39;\\n&#39;)\n    len     equ $ - msg                 ; Define a constant len with the message length\n    \nsection .text                           ; Create the text section\n```\n\n### x86-64 System Calls: Printing to the Terminal\n\nNow that our data is defined, it’s time to write the actual instructions in the **`.text`** section. This is where the CPU will start executing our program. Every program needs an **entry point**, ****a place where execution begins. In Linux assembly, this is usually the label `_start`. We’ll declare it as a global symbol so the linker knows where to begin:\n\n```asm\nsection .data                           ; Create the data section\n    msg     db  \&quot;Hello, World!\&quot;, 0x0A   ; Our message plus a newline (0x0A = &#39;\\n&#39;)\n    len     equ $ - msg                 ; Define a constant len with the message length\n\nsection .text                           ; Create the text section\n    global _start                       ; Tell the linker the entry point is _start\n\n_start:                                 ; Entry point of our program\n```\n\nTo display text on the screen, we need to ask the operating system for help. In Linux, this is done using a **system call**. A system call is like raising your hand and asking the OS: “Hey, can you do this for me?” We’ll use the `write` system call, which has this signature:\n\n```c\nwrite(fd, buf, count)\n```\n\n- `fd` = file descriptor (1 means standard output, the terminal).\n- `buf` = address of the data to write (our `msg`).\n- `count` = number of bytes to write (our `len`).\n\nIn x86-64 Linux, [system calls](https://blog.rchapman.org/posts/Linux_System_Call_Table_for_x86_64/) are made by:\n\n1. putting the syscall number into the `rax` register,\n2. putting the arguments into registers (`rdi`, `rsi`, `rdx`, …), and\n3. executing the instruction `syscall`.\n\nThe syscall number for `write` is **1**.\n\nHere’s how we set up and call `write`:\n\n```asm\nmov     rax, 1                      ; Syscall number for write\nmov     rdi, 1                      ; File descriptor 1 = stdout\nmov     rsi, msg                    ; Address of the string\nmov     rdx, len                    ; Length of the string\nsyscall                             ; Invoke the system call\n```\n\nAfter printing the message, the program still needs to exit cleanly. That’s another syscall: `exit`.\n\n- The syscall number for `exit` is **60**.\n- Its only argument is the exit code (0 means success).\n\n```asm\nmov     rax, 60                     ; syscall number for exit\nxor     rdi, rdi                    ; exit code 0 (using xor to set rdi = 0)\nsyscall                             ; Invoke the system call\n```\n\nHere’s the full `hello.asm` so far:\n\n```asm\nsection .data                           ; Create the data section\n    msg     db  \&quot;Hello, World!\&quot;, 0x0A   ; Our message plus a newline (0x0A = &#39;\\n&#39;)\n    len     equ $ - msg                 ; Define a constant len with the message length\n\nsection .text                           ; Create the text section\n    global _start                       ; Tell the linker the entry point is _start\n\n_start:                                 ; Entry point of our program\n    ; write(fd1, msg, len)\n    mov     rax, 1                      ; Syscall number for write\n    mov     rdi, 1                      ; File descriptor 1 = stdout\n    mov     rsi, msg                    ; Address of the string\n    mov     rdx, len                    ; Length of the string\n    syscall                             ; Invoke the system call\n\n    ; exit(0)\n    mov     rax, 60                     ; syscall number for exit\n    xor     rdi, rdi                    ; exit code 0 (using xor to set rdi = 0)\n    syscall                             ; Invoke the system call\n```\n\n## Assembling, Linking, and Running\n\nWe’ve written our `hello.asm` file, now it’s time to bring it to life! The process has three steps:\n\n1. **Assemble:** convert the human-readable assembly into machine code (`.o` file).\n2. **Link:** package that machine code into a proper executable (`hello`).\n3. **Run:** execute it and see the magic happen.\n\n### Option 0: Use `make` (recommended)\n\nSince we installed `make` in our container, we can automate these steps with a **Makefile**. Inside your project folder, create a file named `Makefile`:\n\n```makefile\nall: hello\n\nhello: hello.o\n\tld hello.o -o hello\n\nhello.o: hello.asm\n\tnasm -f elf64 hello.asm -o hello.o\n\nclean:\n\trm -f hello hello.o\n```\n\nNow you can build your program with a single command:\n\n```bash\nmake\n```\n\nAnd when you want to clean up all build artefacts:\n\n```bash\nmake clean\n\n```\n\n### Step 1: Assemble\n\nManually, the first step is turning your source code into an **object file**:\n\n```bash\nnasm -f elf64 hello.asm -o hello.o\n```\n\n- `f elf64` → tells NASM to generate 64-bit ELF output (Linux’s standard format).\n- `hello.asm` → your assembly source file.\n- `o hello.o` → output file (`.o` = object file).\n\n### Step 2: Link\n\nNext, we use the GNU linker `ld` to create an **executable** from the object file:\n\n```bash\nld hello.o -o hello\n\n```\n\n- `hello.o` → the object file we just created.\n- `o hello` → the name of the final executable.\n\n### Step 3: Run\n\nNow execute your program:\n\n```bash\n./hello\n```\n\nYou should see:\n\n```\nHello, World!\n```\n\nCongratulations, you’ve just written, built, and executed your first **Hello World in pure x86 assembly**!&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/hello-world/hello-world-asm.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/whale-behind-a-desk-with-containers.png&quot;]]],&quot;digest&quot;:[0,&quot;ff52dcfa1c59e9b1&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;hello-world/hello-world-asm&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;the-tough-love-architecture-guide/1-the-truth.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;the-harsh-truth-about-your-design-pattern-choices&quot;],&quot;title&quot;:[0,&quot;The Art of Saying No: The Harsh Truth About Design Patterns&quot;],&quot;description&quot;:[0,&quot;Stop pattern-worshipping and start solving problems. Learn why most developers misuse design patterns and how to pick the right architecture for real-world constraints.&quot;],&quot;collection&quot;:[0,&quot;The Tough Love Architecture Guide&quot;],&quot;chapter&quot;:[0,1],&quot;shortname&quot;:[0,&quot;The Truth&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-08-24T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Software Design Patterns&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Clean Code&quot;],[0,&quot;Engineering Mindset&quot;],[0,&quot;Design Patterns&quot;],[0,&quot;Simplicity&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;when to use design patterns&quot;],[0,&quot;overengineering in software&quot;],[0,&quot;design patterns vs simple code&quot;],[0,&quot;pattern worshiping&quot;],[0,&quot;software architecture trade-offs&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/the-tough-love-architecture-guide.CA6T7i7D.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for the \&quot;The Tough Love Architecture Guide\&quot; series&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-08-24T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Imagine you&#39;re knees-deep in a project deadline, navigating through so many lines of messy code that resemble a spaghetti factory explosion. You&#39;ve probably been there: tempted to implement a clever design pattern you recently learned about, hoping it will be the magic bullet for all your coding woes. But here&#39;s a reality check: “most developers either misuse or don&#39;t know what they are doing with design patterns”. It&#39;s not because they&#39;re bad at coding, but they&#39;ve fallen into the trap I like to call ***Pattern worshiping.***\n\nIf you&#39;re the type of developer who finds themselves forcing a Pattern into a project that didn&#39;t need one, you&#39;re not alone, and this page is for you. By the end, you&#39;ll not only understand how to stop turning your architecture into a &#39;look what I have learned in school&#39; museum, but you&#39;ll discover how to save valuable time, reduce the introduction of bugs, and improve code review outcomes by choosing solutions that truly solve your problem.\n\n---\n\n## The myth of the “Perfect” Design Pattern\n\nLet me give you a quick reality check: THERE IS NO UNIVERSAL BEST PATTERN.\n\nThe right pattern in one project could be the wrong pattern in another. Yet, including myself at times, we developers often pick a pattern simply because a tutorial made it look sexy. This leads to bizarre overengineering, such as creating a simple object using a Builder, Factory, and Prototype pattern simply because the textbook made it look cool. Consider this: a startup once spent a month implementing an intricate Builder-Factory architecture, only to realize that it could be refactored in a single day using straightforward functions. This real-world example illustrates the potential pitfall of blindly applying complex patterns where simplicity would suffice.\n\nA pattern should never be your objective; it should only make your solution simpler, not more complex.\n\n---\n\n## Harsh Truth #1 - Patterns Are Tools, Not Goals\n\nHammers are a great tool when you have a bunch of nails, but they are awful when you try to fix a leaky pipe.\n\nDesign patterns are similar; they are tools that can be great for the right task. If you start a project and think, *“I want to use a singleton pattern,”* You are not addressing the underlying problem, but rather your desires\n\nA code base can quickly become overcomplicated when we use patterns that do not fit the task. Sometimes, a simple if statement will do a better job.\n\nBefore you begin implementing a new pattern, ask yourself these three questions:\n\n1. Does using a strategy pattern here reduce complexity, or are we simply rearranging it?\n2. Will the code be easier to maintain after implementing the pattern?\n3. Will future developers understand and appreciate the added complexity?\n\n---\n\n## Harsh Truth #2 - Ignoring Constraints will lead to wrong choices\n\nFor most of us, we are not writing code on supercomputers with unlimited memory, crazy performance, and super high concurrency. We work in a world where we are restricted by boundaries and constraints. When selecting a pattern, we must consider all our constraints, including the team&#39;s experience, deadlines, business requirements, and performance. To help ensure these constraints are at the forefront of consideration, here is a checklist of typical project limitations you should evaluate:\n\n- Memory availability\n- Latency requirements\n- Team skill levels\n- Project deadlines\n- Business needs\n- Performance targets\n- Scalability demands\n\nIf we ignore our constraint, we&#39;ll end up with mismatched solutions. The right pattern is the one that fits within our boundaries and doesn&#39;t violate our constraints.\n\n---\n\n## Harsh Truth #3 - You&#39;re Probably Copying, Not Designing\n\nThis one might be a bit awkward, but many developers don&#39;t apply patterns; they simply copy them. They find some pattern, copy it into their codebase, change some class name, and call it a solid architecture. The problem with this is that textbook examples all live in a perfect world where requirements never change and performance is someone else&#39;s problem. To truly embrace the creative process of design, ask yourself: &#39;Which parts of this pattern can we safely drop or modify while still achieving our goals?&#39; This mindset encourages adaptation rather than blind replication, allowing for solutions that are tailored to the specific challenges of your project.\n\nThe code we write will not live in that perfect world. This means we need to adapt and modify the pattern to make it fit within our domain. We need to tweak the pattern to ensure it operates within our boundaries and does not cross any constraints. Blindly copying a perfect example will fall apart when reality comes knocking.\n\n---\n\n## Patterns don&#39;t make you smart; good choices do\n\nA well-chosen pattern can make your code elegant and maintainable, but the wrong choice leads to unnecessary complexity. Choosing a pattern should always serve your problem, not overshadow it.\n\nThe harsh truth is: just knowing design patterns does not make you a good developer, but knowing when **not** to use them will bring you pretty close to being one.\n\nInstead of defaulting to whatever is trending, we can follow a simple framework. Next time you&#39;re faced with a problem, follow these five guidelines:\n\n1. **Define the problem clearly**\n2. **Identify constraints**\n3. **List candidate patterns**\n4. **Evaluate trade-offs**\n5. **prototype and test**\n\nSo next time, remember: your real goal isn&#39;t to use fancy patterns, but to solve the problem in front of you as directly as possible.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/the-tough-love-architecture-guide/1-the-truth.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/the-tough-love-architecture-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;28ba1db2cc4657c8&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;the-tough-love-architecture-guide/1-the-truth&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/0-introduction.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;introduction-computer-architecture&quot;],&quot;title&quot;:[0,&quot;Introduction: Computer Architecture&quot;],&quot;description&quot;:[0,&quot;Introduction for From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,0],&quot;shortname&quot;:[0,&quot;Intro&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-09-03T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Featured&quot;],[0,&quot;Computer Architecture&quot;],[0,&quot;Introduction&quot;]]],&quot;tags&quot;:[1,[[0,&quot;CPU&quot;],[0,&quot;Hardware&quot;],[0,&quot;Binary&quot;],[0,&quot;Logic Gates&quot;],[0,&quot;Digital Design&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;computer architecture basics&quot;],[0,&quot;how a cpu works&quot;],[0,&quot;transistor to system&quot;],[0,&quot;von neumann architecture introduction&quot;],[0,&quot;computer engineering fundamentals&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-09-03T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Every click, tap, and swipe hides a story. Beneath the apps, screens, and machines you use every day lies a hidden world of circuits, instructions, and clever engineering. This series takes you on a bottom-up journey from bits and bytes, through memory, storage, and CPUs, to the input and output devices that bridge the digital and physical worlds. By the end, computers won&#39;t feel like mysterious black boxes, but like beautifully crafted, logical machines.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/0-introduction.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;1e78949c794e3d11&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/0-introduction&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;the-tough-love-architecture-guide/2-bad-design.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;refactoring-a-bad-design-a-step-by-step-example&quot;],&quot;title&quot;:[0,&quot;The Rescue Mission: Refactoring a &#39;Monster&#39; Class Step-by-Step&quot;],&quot;description&quot;:[0,&quot;Don&#39;t delete that messy code just yet. Learn how to transform a tightly coupled Java class into a clean, testable architecture using incremental refactoring and abstractions.&quot;],&quot;collection&quot;:[0,&quot;The Tough Love Architecture Guide&quot;],&quot;chapter&quot;:[0,2],&quot;shortname&quot;:[0,&quot;Bad Design&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-09-21T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Software Design Patterns&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Java&quot;],[0,&quot;Clean Code&quot;],[0,&quot;SRP&quot;],[0,&quot;Unit Testing&quot;],[0,&quot;Technical Debt&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how to refactor a big class&quot;],[0,&quot;single responsibility principle java example&quot;],[0,&quot;incremental refactoring vs rewrite&quot;],[0,&quot;decoupling java classes&quot;],[0,&quot;unit testing refactored code&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/the-tough-love-architecture-guide.CA6T7i7D.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for the \&quot;The Tough Love Architecture Guide\&quot; series&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-09-21T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Let&#39;s be honest: most projects don&#39;t start with a bad design. Over time, requirements change or new ones get added. Perhaps we have a tight deadline to meet, or maybe we&#39;re just being lazy, but often we will just hack the new requirements into the original design. We just tell ourselves that we&#39;ll refactor this later, but of course, we never do. Imagine a scenario where a seemingly minor piece of unrefactored code caused a system outage. This can result in thousands of dollars in lost revenue within a matter of hours. Such real-world setbacks underscore the critical importance of timely refactoring and set the stage for understanding the need to manage technical debt responsibly.\n\nA few deadlines later, and that “*temporary*” architecture has roots so deep that nobody wants to touch it. The code works..., but it&#39;s very fragile, hard to test, and impossible to extend without breaking something.\n\nIn this article, I will present a small example of a single class that started with good intentions but over time has grown into a monster that only a few of us dare tame. You will be able to translate the techniques I use to tame the beast to your real-life projects.\n\n## A bad design - (Setting the scene)\n\nLet me introduce you to our scene: the UserManager class.\n\nRight now, this class does a bit of everything:\n\n- Creating user\n- Saving them to the database\n- Sending welcome emails\n- Logging user actions\n\n```java\npublic class UserManager {\n    private Database database;\n    private EmailService emailService;\n    private Logger logger;\n\n    public UserManager(Database database, EmailService emailService, Logger logger) {\n        this.database = database;\n        this.emailService = emailService;\n        this.logger = logger;\n    }\n\n    public void createUser(String name, String email) {\n        // Save user to database\n        database.save(new User(name, email));\n\n        // Send welcome email\n        emailService.send(email, \&quot;Welcome to our app!\&quot;);\n\n        // Log the action\n        logger.log(\&quot;User created: {}\&quot;, name);\n    }\n\n    public void deleteUser(String email) {\n        database.delete(email);\n        logger.log(\&quot;User deleted: {}\&quot;, email);\n    }\n}\n\n```\n\nInitially, this class was created simply for storing users in a database. Even though you might initially think this class looks fine. It has become a sleeping monster; this class has too many responsibilities, is tightly coupled, and is hard to test. Now, let&#39;s break down the steps needed to start and refactor this.\n\n## Step 1 - Identify the problem\n\nMany developers will see some \&quot;Messy\&quot; code and think, \&quot;I need to rewrite this.\&quot; However, let me explain why this could be dangerous. Bad code can still contain a significant amount of business logic, undocumented hacks, and edge cases. For instance, there might be a seemingly redundant line of code that checks if a user&#39;s name is exactly 13 characters long. Although it may seem unusual, it could be an essential edge case for handling inputs from a specific legacy system that incorrectly handles overflow. If you simply rewrite the code, there is a significant chance you will lose the hidden knowledge buried inside the existing codebase.\n\nInstead of immediately rewriting the code, you can better ask yourself the following questions:\n\n- ***What exactly is wrong?***\n- ***Why is it wrong?***\n\nIf we answer these questions for our class, it comes down to the problem that the class has too many responsibilities (user creation, persistence, logging, and email sending). This is incorrect because the class has too many responsibilities, creating the risk that changing one thing will break another.\n\nRefactoring this class isn&#39;t just about style. It&#39;s about concerns related to testability and tight coupling.\n\n## Step 2 - Choose a better solution\n\nLet&#39;s not be like most developers and skip this part. But instead of just starting and rewriting the code, trying to hammer out a better solution, ask these three questions first:\n\n- *What role should this class or component actually play?*\n- *Which responsibility should it keep and which should it rewrite?*\n- *What are the most important constraints right now? (testability, performance, maintainability)*\n\nIn our case, the **UserManager** is overwhelmed with various responsibilities, including persistence, notifications, and logging. We need to shift the role of our class from “doing everything” to an orchestrator.\n\nIt means the **UserManager** class should just coordinate the workflow. However, it should not be aware of how each specific activity is implemented and executed. We need to create specific classes for each task (**UserRepository, EmailNotifier**).\n\nWhen we actually try to find a better solution, we&#39;re not just cleaning up the code; we&#39;re also redefining the architecture around the identified problem. We&#39;re not worried about choosing the coolest pattern, but we actually try to find a good solution.\n\n## Step 3 - Create an implementation plan\n\nNow, for this step, we need some real discipline. We identified the problem and found a solution to fix it. What is left but deleting this class and writing it again right!? With this approach, we risk losing working code, introducing new bugs, or even reintroducing the same design problems.\n\nInstead, let&#39;s take a safer approach; think *incremental refactoring.* We make small and safe changes one at a time, which we can test individually. This approach will give us a better chance of retaining all functionality.\n\nThis is a simple playbook you can follow once ready to implement your solution:\n\n1. **Introduce abstraction early.** Try to define your interfaces and abstract classes before implementing them. This approach will help you determine if your predefined contracts are sensible without having to implement them. For our piece of code, we need to create the interfaces ***UserRepository** and **EmailNotifier.***\n2. **Start with one responsibility.** Don&#39;t take out everything at once. For example, in the **UserManager** class, it would make sense to first extract the logic for the **UserRepository** while keeping the rest of the code intact.\n3. **Strangle old code.** Don&#39;t delete existing methods straight away. Let the abstraction take over piece by piece. Only remove the old code once you&#39;re confident that you haven&#39;t broken anything and have covered all the expected behaviours.\n4. **Test as you go.** After adding the abstraction, write unit tests for it. Verify your implementation and ensure that your refactoring hasn&#39;t broken anything.\n5. **Keep the class working at every stage.** Even when you&#39;re in the middle of refactoring, your code should still work. If you follow this approach, it will be easy to identify when and where you could have made a possible mistake.\n\nThe primary concept of this playbook is straightforward: each step should be reversible. If something fails, you can rollback that step instead of needing to do a massive rewrite and spend a lot of time refactoring.\n\n## Step 4 - Implement the changes\n\nWe have finally arrived at the fun part, and we can start refactoring. Let&#39;s refactor our **UserManager** class using the playbook defined in step 3.\n\n### Step 4.1 Introduce the abstraction\n\nLet&#39;s start by introducing some abstraction to the UserManager. Our class does not need to know how the underlying implementation works. We will create two interfaces: one for the EmailManager and one for the UserRepository. The heaviest dependency is the UserRepository, so let&#39;s start removing the direct coupling between the UserManager and the UserRepository.\n\nThe UserRepository interface will have two methods: one to save a user and one to delete a user from the database. Our UserRepository will look like this:\n\n```java\npublic interface UserRepository {\n    void save(User user);\n    void deleteById(String emailAddress);\n}\n```\n\nNext, we need to extract the email sending logic. We will follow the same approach where the UserManager class does not need to know the underlying implementation. Our EmailNotifier interface will have one method that allows us to send a welcome email.\n\n```java\npublic interface EmailNotifier {\n    void sendWelcomeEmail(String emailAddress);\n}\n```\n\n### Step 4.2 Extract the persistence logic\n\nNow that we have defined our interfaces, let&#39;s write an implementation. We will implement the persistence logic in the UserRepository, like this:\n\n```java\npublic class UserRepositoryImpl implements UserRepository {\n    private final Database database;\n\n    public UserRepositoryImpl(Database database) {\n        this.database = database;\n    }\n\n    @Override\n    public void save(User user) {\n        database.save(user);\n    }\n\n    @Override\n    public void deleteById(String emailAddress) {\n        database.delete(emailAdress);\n    }\n}\n```\n\n### Step 4.3 Extract the notification logic\n\nLastly, we need to implement our second interface: the EmailNotifier. Our implementation will look like this:\n\n```java\npublic class EmailNotifierImpl implements EmailNotifier {\n    private final EmailService emailService;\n\n    public EmailNotifierImpl(EmailService emailService) {\n        this.emailService = emailService;\n    }\n\n    @Override\n    public void sendWelcomeEmail(String emailAdress) {\n        emailService.send(emailAdress, \&quot;Welcome to our app!\&quot;);\n    }\n}\n```\n\n### Step 4.4. Update UserManager to orchestration\n\nNow, let&#39;s add these interfaces to the UserManager class.\n\n```java\npublic class UserManager {\n    private final UserRepository userRepository;\n    private final EmailNotifier emailNotifier;\n    private final Logger logger;\n\n    public UserManager(UserRepository userRepository, EmailNotifier emailNotifier, Logger logger) {\n        this.userRepository = userRepository;\n        this.emailNotifier = emailNotifier;\n        this.logger = logger;\n    }\n\n    public void createUser(String name, String email) {\n        userRepository.save(new User(name, email));\n        emailNotifier.sendWelcomeEmail(email);\n        logger.log(\&quot;User created: {}\&quot;, name);\n    }\n\n    public void deleteUser(String emailAddress) {\n        userRepository.deleteById(emailAddress);\n        logger.log(\&quot;User deleted: {}\&quot;, emailAddress);\n    }\n}\n```\n\n## Step 4.5. Unit tests\n\nTo ensure the robustness and functionality after our refactoring, it is crucial to implement unit tests consistently. Our primary goal is to verify key functionality, such as user management, and confirm that these aspects function without errors. We begin by writing unit tests for each of our interfaces and the UserManager. This involves verifying the core operations of saving and deleting users, ensuring data integrity, and operational reliability.\n\n```java\nclass UserRepositoryTests {\n    private UserRepository userRepo;\n    private Database database;\n\t\n    @BeforeEach\n    void init() {\n        database = mock(Database.class);\n        userRepo = new UserRepositoryImpl(database);\n    }\n\t\n    @Test\n    void testSaveUser() {\n        User user = new User(\&quot;testUser\&quot;, \&quot;testUser@mail.com\&quot;);\n        userRepo.save(user);\n    \n        verify(database, times(1)).save(user);\n    }\n\t\n    @Test\n    void testDeleteById() {\n        String email = \&quot;testUser@mail.com\&quot;;\n        userRepo.deleteById(email);\n    \n        verify(database, times(1)).delete(email);\n    }\n}\n```\n\nNext, we need to verify the EmailNotifier. For this interface, we need to check if we can successfully send a welcome email.\n\n```java\nclass EmailNotifierTests {\n    private EmailNotifier emailNotifier;\n    private EmailService emailService;\n\n    @BeforeEach\n    void init() {\n        emailService = mock(EmailService.class);\n        emailNotifier = new EmailNotifierImpl(emailService);\n    }\n\n    @Test\n    void testSendWelcomeEmail() {\n        String email = \&quot;testUser@mail.com\&quot;;\n        emailNotifier.sendWelcomeEmail(email);\n    \n        verify(emailService, times(1)).send(email, \&quot;Welcome to our app!\&quot;);\n    }\n}\n```\n\nThe final part we need to verify is the UserManager, where we bring it all together. For our UserManager unit tests, we should focus on verifying collaboration, ensuring it effectively delegates tasks to the appropriate interfaces, such as UserRepository and EmailNotifier. Instead of checking internal state changes, we should utilise mocks for these components. This approach will allow us to assert that the interfaces were indeed called, indicating successful orchestration.\n\n```java\nclass UserManagerTests {\n    private UserManager userManager;\n    private UserRepository userRepository;\n    private EmailNotifier emailNotifier;\n    private Logger logger;\n\n    @BeforeEach\n    void init() {\n        userRepository = mock(UserRepository.class);\n        emailNotifier = mock(EmailNotifier.class);\n        logger = mock(Logger.class);\n        \n        userManager = new UserManager(userRepository, emailNotifier, logger);\n    }\n\n    @Test\n    void testCreateUser() {\n        userManager.createUser(\&quot;testUser\&quot;, \&quot;testUser@mail.com\&quot;);\n        \n        verify(userRepository, times(1)).save(any(User.class));\n        verify(emailNotifier, times(1)).sendWelcomeEmail(\&quot;testUser@mail.com\&quot;);\n        verify(logger, times(1)).log(\&quot;User created: {}\&quot;, \&quot;testUser\&quot;);\n    }\n\n    @Test\n    void testDeleteUser() {\n        userManager.deleteUser(\&quot;testUser@mail.com\&quot;);\n        \n        verify(userRepository, times(1)).deleteById(\&quot;testUser@mail.com\&quot;);\n        verify(logger, times(1)).log(\&quot;User deleted: {}\&quot;, \&quot;testUser@mail.com\&quot;);\n    }\n}\n```\n\n## Refactoring is About Purpose, Not Perfection\n\nRefactoring isn&#39;t about making code look pretty. It&#39;s about making the design better fit the problem. In our original **UserManager** class, the code worked, but it was brittle and hard to maintain. By separating the functionality, introducing abstraction, and refactoring in safe steps, we made it flexible, testable, and ready for future maintenance. Next time your face badly designed code, remember:\n\n- Identify the problem before you touch code.\n- Select a more suitable direction based on the constraints.\n- Break the refactoring down into safe, reversible steps.\n\nSmall, deliberate changes beat a massive rewrite every time. To put this into practice, I challenge you: identify one overgrown class in your codebase today and apply Step 1—Identify the problem. This initial step can pave the way for meaningful refactoring and tangible improvements in your projects.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/the-tough-love-architecture-guide/2-bad-design.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/the-tough-love-architecture-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;da52e57515d54420&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;the-tough-love-architecture-guide/2-bad-design&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;the-tough-love-architecture-guide/3-performance.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;design-patterns-for-performance-what-actually-works-in-high-performance-systems&quot;],&quot;title&quot;:[0,&quot;Design Patterns for Performance: Scaling Beyond Simple Code&quot;],&quot;description&quot;:[0,&quot;Master high-performance architecture. Explore the Reactor pattern, Actor model, and resilience strategies like Circuit Breakers and Load Shedding for low-latency systems.&quot;],&quot;collection&quot;:[0,&quot;The Tough Love Architecture Guide&quot;],&quot;chapter&quot;:[0,3],&quot;shortname&quot;:[0,&quot;Performance&quot;],&quot;difficulty&quot;:[0,&quot;Beginner&quot;],&quot;author&quot;:[0,&quot;Quinn&quot;],&quot;date&quot;:[3,&quot;2025-10-26T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Software Design Patterns&quot;]]],&quot;tags&quot;:[1,[[0,&quot;High Performance&quot;],[0,&quot;Latency&quot;],[0,&quot;Concurrency&quot;],[0,&quot;Scalability&quot;],[0,&quot;Resilience&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;reactor pattern vs actor model&quot;],[0,&quot;fan-out fan-in pattern&quot;],[0,&quot;circuit breaker vs bulkhead&quot;],[0,&quot;zero-copy memory mapping&quot;],[0,&quot;high throughput system design&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/the-tough-love-architecture-guide.CA6T7i7D.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Banner for the \&quot;The Tough Love Architecture Guide\&quot; series&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-10-26T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;You might think to yourself that we don’t need design patterns when building high-frequency trading systems. This might be true for fun hobby projects, but in the real world, it will lead to a codebase that is hard to maintain and prone to subtle bugs like race conditions and deadlocks. Engineers often face challenges such as these when concurrent processes inadvertently interfere with each other. Debugging and resolving such issues can be time-consuming and complex. By using the right design patterns, we can maintain a readable codebase while mitigating these issues and still achieving the required performance.\n\nIn this post, I will present several different patterns that can be utilized for high-performance systems. With these patterns, we will reduce latency and utilize resources more efficiently, such as CPU, memory, and I/O. For a better understanding of computer resources, you can read an [introduction to computer architecture](/posts/introduction-computer-architecture/). To choose the right pattern, it is crucial to understand the specific bottlenecks of your system. For instance, if your system&#39;s primary challenge is I/O-bound operations, the Reactor pattern can be beneficial as it efficiently handles multiple concurrent requests. On the other hand, if the bottleneck is computational, patterns focusing on concurrency, like the Actor model, might be more suitable. Thus, determining whether you are optimizing the code for compute, data, networking, or concurrency is the key to selecting the most effective design pattern.\n\n## Why patterns matter for high-performance systems\n\nDesign patterns enforce structure and best practices, allowing for better code quality and maintainability. Even in high-performance systems, it&#39;s still important to have maintainable and quality code. When building a system optimized for speed, complexity, and readability can quickly spiral out of control. Luckily, there are also patterns that enable us to reason about and build quality, maintainable code. By combining the right patterns, we can build a high-performance system and keep some sanity in our code.\n\n## Concurrency and I/O patterns\n\nLet&#39;s first discuss patterns for Concurrency and I/O. These patterns will help us to structure multi-threaded and asynchronous code. We will discuss three popular patterns used in industry to solve these issues.\n\n### Reactor pattern\n\nThe reactor pattern is an event-driven design with a single-threaded event loop that demultiplexes I/O events and dispatches them to handlers. By using non-blocking I/O, a reactor can handle many concurrent I/O-bound requests with minimal threads and low latency. This pattern is widely used in high-performance servers and networking frameworks because it avoids the overhead of blocking threads or one-thread-per-connection models.\n\n### Actor model\n\nThis pattern treats each concurrent entity as an *actor* with its own private state, processing incoming messages one at a time. Actors communicate only via asynchronous message passing, eliminating the need for locks or shared-memory synchronization. Actors are extremely lightweight (far lighter than OS threads) and can be spawned in large numbers, allowing millions of actors to run concurrently. This leads to low-latency, high-throughput processing, as each actor handles its tasks independently.\n\n### Fan out / Fan in\n\nThe Fan out / Fan in pattern distributes workloads to multiple parallel workers (“fan-out”) and then aggregates their results (“fan-in”). A large computation can be split into many subtasks that run concurrently on multiple threads or machines; once all tasks complete, their partial results are combined. This pattern efficiently scales out computation across multicore CPUs or distributed systems. By parallelizing independent work, it can drastically reduce total processing time.\n\n## Data Access and Memory Optimization\n\nWhen writing high-performance systems, it&#39;s important to efficiently handle reading, writing, and managing data. This could be handling data from a database, a file, or a remote API. In this section, we will discuss patterns that help us separate data logic from business logic. Making data operations consistent and reusable, and supporting multiple data sources without rewriting core logic.\n\n### Cache-Aside pattern\n\nThe cash-Aside pattern lets an application explicitly check an external cache before the primary data store. On a cache miss, it loads the data from the datastore and populates the cache. This lazy-loading strategy means frequently accessed data is served from fast memory rather than slower databases. As a result, repeated reads of the same data have much lower latency, and overall throughput improves, since the database sees fewer queries.\n\n### Sharding pattern\n\nThe Sharing pattern enables the horizontal partitioning of a dataset across multiple database instances (shards) to improve performance. By splitting data (for example, by key range or hash) into separate shards, each shard handles only a subset of the workload, reducing contention and enabling parallel queries. Sharding boosts throughput and also enhances availability: a failure or maintenance on one shard affects only that partition, not the entire dataset.\n\n### Zero-Copy and Memory-Mapped files\n\nThe Zero-Copy and Memory-Mapped file pattern uses techniques to eliminate extra data copying between the user space and kernel. Zero-copy I/O lets devices transfer data directly between kernel and user memory without CPU intervention. Memory-mapped files (mmap) puts the content into the process’s address space, so after the initial mapping, reads and writes go straight to RAM without an extra copy or system calls. In effect, the file data becomes part of memory, dramatically speeding up file and network I/O for large transfers.\n\n## Resilience and Stability\n\nAny high-performance system should be resilient and fault-tolerant. For high-performance systems to be useful, we need to make sure they&#39;re consistent and predictable under high workloads. When the system is fast, when everything is perfect, but it crashes in the real world, it is not truly high performance. Therefore, in this section, we will discuss some patterns that can help to have a stable and reliable system while still being high-performance.\n\n### Circuit breaker\n\nThe circuit breaker pattern wraps calls to an external service and trips (opens) if failures exceed a threshold. When open, calls fail immediately instead of waiting on timeouts or retry loops. This prevents resource exhaustion (threads, connections) from constantly retrying a dead service, giving the system time to recover. In short, a circuit breaker preserves stability by failing fast on unhealthy dependencies.\n\n### Bulkhead pattern\n\nWith the bulkhead pattern, we can isolate components or consumers into separate pools so that a failure in one does not bring down others. Like compartments in a ship, this means partitioning services or resources (for example, with dedicated thread pools or separate processes). If one partition fails or overloads, it does not deplete shared resources, and the rest of the system can continue operating. Bulkheads confine failures, preventing cascading outages across the entire system\n\n### Load sheading\n\nThe load-shading pattern intentionally drops or delays lower-priority requests when the load is too high. Rather than letting queues grow and latencies spike to the point of collapse, the system sheds excess work to stay responsive. For example, a server might reject additional requests once CPU or memory utilization crosses a threshold. This trade-off sacrifices some non-critical throughput to guarantee that high-priority tasks still meet latency targets.\n\n## Next steps\n\nThe right design patterns are a powerful tool for building high-performance systems in a maintainable way. When used correctly, these patterns will make our code more maintainable, scalable, and improve code quality. In practice, we first need to identify our system&#39;s bottlenecks before choosing the right pattern. Once you know these bottlenecks, you can apply the right pattern and address them. For example, choose the reactor pattern if your application has I/O heavy services, or integrate cash-aside and sharding to speed up data access. As you apply these patterns, continuously monitor the effects of the changes. Watch the latency and throughput, and adjust the time-out and load-shedding thresholds as needed. With careful testing and incremental adoption, you can dramatically improve performance while preserving code quality and maintainability.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/the-tough-love-architecture-guide/3-performance.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/the-tough-love-architecture-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;396c2ed92aaade9a&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;the-tough-love-architecture-guide/3-performance&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/2-logic.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;logic-gates-adders-flip-flops&quot;],&quot;title&quot;:[0,&quot;Logic Gates &amp; Memory: Building an 8-Bit Adder from Scratch&quot;],&quot;description&quot;:[0,&quot;How do transistors become logic? Learn about AND, OR, XOR gates, and how to build 8-bit adders and SR flip-flops for computer memory.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,2],&quot;shortname&quot;:[0,&quot;Logic&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-09-14T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;],[0,&quot;Electrical Engineering&quot;],[0,&quot;Logic&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Logic Gates&quot;],[0,&quot;Binary Adder&quot;],[0,&quot;Flip-Flops&quot;],[0,&quot;Combinational Logic&quot;],[0,&quot;Sequential Logic&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how an AND gate works&quot;],[0,&quot;half-adder vs full-adder&quot;],[0,&quot;binary 8-bit adder circuit&quot;],[0,&quot;SR flip-flop explained&quot;],[0,&quot;logic gate symbols guide&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-09-14T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In this chapter, we&#39;ll see how simple switches can be combined into **logic gates**, the fundamental building blocks of digital circuits. Step by step, we&#39;ll explore the basic gates: **NOT, AND, OR**, and a few useful variations. Then we move on to how they can be wired together into larger circuits, known as **combinational logic**. Along the way, we&#39;ll build our first arithmetic circuits: starting with the **Half-Adder**, then extending it into a **Full-Adder**, and finally chaining them together to create an **8-bit adder**, capable of adding entire binary numbers. Finally, we&#39;ll wrap up with an introduction to **sequential logic**, where circuits gain memory and can remember past states.\n\n## From switches to logic gates\n\nAs we saw in the previous chapter, a transistor at its core acts like an electronic switch: it either allows current to flow (on) or prevents it from flowing (off). In the simple circuit shown below, an AA battery provides the power, the LED indicates the on/off states, a transistor controls the electric flow, and the resistors limit the current to protect the transistor and LED from damaging or even exploding (which can happen to the best of us). \n\n![Simple battery-powered circuit with a transistor and an LED in serial](assets/how-a-simple-switch-works-logic-circuit-animation.gif)\n\nBy looking at the circuit, you will notice that it is just an LED that turns on and off depending on the transistor&#39;s state. It may not look impressive, but here is the idea: by cleverly wiring transistors,  we can make them do more than switch a light on/off. We can make them process information. This is where **logic gates** come in. The very first one we&#39;ll explore is the **NOT gate**, followed by the **AND** and **OR** **gates**.\n\n### NOT Gate\n\nYou may wonder: What happens if we change how the LED is connected? Instead of wiring it directly to the transistor&#39;s output path, we place it on the transistor&#39;s collector side (the path to ground). Now something interesting happens: the circuit **flips the signal**. The output becomes the opposite of the input. This is called an **inverter**, or more commonly, a **NOT gate**.\n\n![Simple battery-powered circuit with a transistor and an LED in parallel](assets/how-an-not-gate-works-logic-circuit-animation.gif)\n\nWhen the transistor is **on**, current flows from the battery&#39;s positive side through the resistor and straight to its negative (ground) via the transistor. In this state, the transistor acts almost like a short circuit, providing a low-resistance path for the current to follow. A transistor isn&#39;t a perfect wire, and even when fully on, there is still a tiny “resistance” inside it. This tiny resistance causes a small [voltage drop](https://en.wikipedia.org/wiki/Voltage_drop) across the transistor. Most of the battery&#39;s voltage now falls across the first resistor and the transistor. Very little voltage is left for the LED, so it stays off. In digital circuits, this small leftover voltage is still seen as a logical **0 (low)**. When the transistor switches **off**, the path to ground vanishes. Current can now flow through the LED instead. The LED lights up, which we read as a logical **1 (high)**.\n\nIn logic and computer science, outputs for every possible input are often shown in a **truth table**. A truth table lists all input values (true or false) and the corresponding outputs. Below is the truth table for a NOT gate. In math, the output is often labelled as **Y**.\n\n| **Input (A)** | **Output (Y)** |\n| --- | --- |\n| 0 | 1 |\n| 1 | 0 |\n\n### AND Gate\n\nOf course, we are not limited to using just one transistor. By combining multiple, we can build circuits that only give an output under specific conditions. One of them is the **AND gate**. In the example below, we connect two transistors in series (one after the other) to create an AND gate. \nCurrent from the source must pass through both transistors before it can reach the output. In other words, the output will be high if and only if both inputs (A and B) are high. If either transistor is off, the path is broken, and no current reaches the output.\n\n![Animation of how an AND gate works](assets/how-an-and-gate-works-logic-circuit-animation.gif)\n\nHere follows the truth table for the AND gate:\n\n| **A** | **B** | **Y** |\n| --- | --- | --- |\n| 0 | 0 | 0 |\n| 1 | 0 | 0 |\n| 0 | 1 | 0 |\n| 1 | 1 | 1 |\n\n### OR Gate\n\nNow let&#39;s try wiring the transistors in **parallel** instead of in series. This gives us the **OR gate**. In this case, the current only needs one path or the other to reach the output to make it high, hence the name OR. \n\n![Animation of how an OR gate works](assets/how-an-or-gate-works-logic-circuit-animation.gif)\n\nHere is the truth table for the OR gate:\n\n| **A** | **B** | **Y** |\n| --- | --- | --- |\n| 0 | 0 | 0 |\n| 1 | 0 | 1 |\n| 0 | 1 | 1 |\n| 1 | 1 | 1 |\n\n### XOR Gate\n\nThere is one more gate that deserves special attention, and because we will need it later: the **XOR gate,** short for **exclusive OR**. This gate is like the regular OR gate, but with a twist. As we know from the previous section, with an OR, the output is high if one or both inputs are high. With the XOR, the output is only high if exactly one input is high. This behaviour makes XOR extremely useful, because it can act as a simple difference detector: output is 1 if the inputs are different, and 0 if they&#39;re the same. You&#39;ll see XOR pop up again in circuits that perform arithmetic, like adders.\n\nHere&#39;s the truth table for XOR:\n\n| **A** | **B** | **Y** |\n| --- | --- | --- |\n| 0 | 0 | 0 |\n| 1 | 0 | 1 |\n| 0 | 1 | 1 |\n| 1 | 1 | 0 |\n\n### NAND and NOR Gate\n\nBy combining the previous gates, we can create even more variations. For example, if we take the output of an AND gate and run it through a NOT gate, we get a **NAND gate** (short for NOT AND). Likewise, combining an OR with a NOT gives a **NOR gate** (NOT OR). NAND and NOR are called **universal gates** because you can combine them to perform any Boolean function and thus create any other type of logic gate (AND, OR, NOT, XOR, XNOR). \n\n## Combinational Logic\n\nSo far, we&#39;ve looked at the basic gates: NOT, AND, OR, and a few variations like XOR, NAND, and NOR. Each one flips, combines, or filters signals simply. The real magic begins when we **connect them**. By combining gates, we can create circuits that solve more interesting problems. These are called **combinational logic circuits**. The key feature of combinational logic is that the **output depends only on the current inputs**. These circuits are stateless. They don&#39;t remember what happened before, meaning that if you give the same inputs, you&#39;ll always get the same outputs. With enough combinational logic, we can create circuits that add, subtract, increment, and do much more. In fact, arithmetic operations in a computer all come down to cleverly arranging these simple logic gates.\n\nTo see combinational logic in action, let&#39;s build something practical: a circuit that can add two numbers together, the **Half-Adder**, a simple circuit that can add two binary bits together. Before we dive in, here&#39;s a quick overview of the most common gate symbols:\n\n![Overview of the most common gate symbols](assets/all-logic-gate-symbols.jpg)\n\nOkay, let&#39;s start small. Suppose we add two **1-bit numbers**, A and B. The result of this addition has two parts: the **sum** and the **carry**. The sum is the result of the addition, and the carry is like the overflowing bit, indicating that the result is greater than we can hold in the sum. For example:\n\n| A | B | Sum | Carry |\n| --- | --- | --- | --- |\n| 0 | 0 | 0 | 0 |\n| 1 | 0 | 1 | 0 |\n| 0 | 1 | 1 | 0 |\n| 1 | 1 | 0 | 1 |\n\nHere&#39;s the fun part: we can build this exact behaviour using only the gates we&#39;ve already learned! When we look at the table above, we can see that the sum is only 1 when either A or B is 1. This exactly matches the behaviour of an XOR. Now for the carry, it is only 1 when both A and B are 1. This is the behaviour of the AND gate. If we put that together, it will look something like this:\n\n![Animation of how a Half-Adder works](assets/how-a-half-adder-works-logic-circuit-animation.gif)\n\nThis little circuit is our first glimpse at how computers perform arithmetic operations. It&#39;s neat, but it has a limitation: it doesn&#39;t handle a carry input. In real arithmetic, numbers don&#39;t exist in isolation. When adding multi-bit numbers, each column might produce a carry that needs to be added to the next. This is where the bigger brother of the Half-Adders comes into play, the Full-Adder. A Full-Adder extends the Half-Adder by including a **carry-in** bit (C-in). Its outputs are the **sum** and a **carry-out** bit (C-out). How do we build one? Well, it&#39;s surprisingly easy! Here is how it works:\n\n1. Take **two Half-Adders**.\n2. Feed the sum from the first into one input of the second (together with C-in).\n3. Use an extra **OR gate** to combine the carry outputs from both Half-Adders.\n\nThe result is a Full-Adder!\n\n![Overview of a Full-Adder circuit](assets/full-adder-logic-circuit.jpg)\n\nNow, here&#39;s where it gets exciting: if we chain multiple Full-Adders together by feeding each carry-out into the next carry-in, we can add multi-bit numbers. For example, connecting 8 Full-Adders creates an **8-bit adder**, capable of adding two 8-bit numbers. The final carry-out then serves as an overflow signal if the result doesn&#39;t fit into 8 bits.\n\n![Overview of an 8-bit adder circuit](assets/8-bit-adder-logic-circuit-animation.gif)\n\n## Sequential Logic\n\nSo far, we&#39;ve only looked at combinational logic circuits, circuits whose output only depends on the current inputs. Change an input, and the output changes immediately. A computer needs more than just that. If you write a sentence and your computer instantly forgets the previous letter every time you press a new key, it would not be useful. It also needs **memory**. This is where **sequential logic** comes in. Sequential circuits combine logic gates with feedback loops so that the output doesn&#39;t just depend on the current inputs, but also on the circuit&#39;s **previous state**. In other words, they have memory.\n\nA simple example of sequential logic in action is a flip-flop, a tiny circuit that can store a single bit of information. The information is stored until a new input changes it. One of the simplest types is the SR flip-flop (Set-Reset flip-flop). It has two inputs:\n\n- **S (Set)**: tells the circuit to remember a `1`\n- **R (Reset)**: tells the circuit to remember a `0`\n\nAnd it has the outputs **Q** and **Q′**, which hold the stored value and its inverse.\n\n![Overview of an SR Flip-Flop circuit](assets/sr-flip-flop-logic-circuit.jpg)\n\nThe truth table looks like this:\n\n| **S (Set)** | **R (Reset)** | **Q (next state)** | **Q′ (next state)** | Notes |\n| --- | --- | --- | --- | --- |\n| 0 | 0 | No change (holds previous value) | Opposite of Q | Memory condition |\n| 0 | 1 | 0 | 1 | Reset state |\n| 1 | 0 | 1 | 0 | Set state |\n| 1 | 1 | Undefined (invalid) | Undefined (invalid) | Not allowed |\n\nNotice something new here: when **S = 0** and **R = 0**, the output doesn&#39;t change. The flip-flop remembers whatever it was holding before. That&#39;s memory in action. As you can see, this simple flip-flop has a flaw: the state where both S and R are 1 is undefined. Luckily, engineers have designed improved versions like the JK and D flip-flops that avoid the undefined state, making them more reliable building blocks for memory.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/2-logic.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;3f72aa2eb13b019a&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/2-logic&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/1-transistor.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;transistors-the-tiny-switch-that-powers-modern-computers&quot;],&quot;title&quot;:[0,&quot;Transistors: The Tiny Switch That Powers Modern Computers&quot;],&quot;description&quot;:[0,&quot;Learn how transistors work from the ground up. Explore semiconductors, N-type vs P-type doping, and the MOSFET drawbridge analogy in this beginner-friendly guide.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,1],&quot;shortname&quot;:[0,&quot;Transistor&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-09-07T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;],[0,&quot;Electrical Engineering&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Transistors&quot;],[0,&quot;Semiconductors&quot;],[0,&quot;MOSFET&quot;],[0,&quot;Binary&quot;],[0,&quot;CPU Basics&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how transistors work&quot;],[0,&quot;n-type vs p-type doping&quot;],[0,&quot;what is a MOSFET&quot;],[0,&quot;computer architecture basics&quot;],[0,&quot;vacuum tubes vs transistors&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-09-07T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In this chapter, we will uncover the details of the tiny switch at the heart of modern computing: the transistor. We start with an introduction to electricity, then dive into the details of the transistor. We finish with a brief history lesson.\n\n## Electricity Basics\n\nBefore we begin with transistors, here is a quick introduction to electricity concepts: voltage, current, and binary states. Feel free to skip this section if you are already familiar with them. In electricity, voltage is the pressure from a power source that pushes electric charges through the circuit. You can see it as a water hose; the higher the pressure, the harder the water wants to flow. Current is the flow or movement of electric charge. More current means more charges are moving through the wire per second, like the flow of water through a hose. Binary states are fairly simple. There are two states:\n\n- Off, no current or below a threshold.\n- On, current is flowing or above a threshold.\n\n## What Exactly Is a Transistor?\n\nLet&#39;s begin with a question, the answer to which is simple, but the specifics can become quite complicated: *What is a transistor?* At its core, a **transistor** is an electronic switch that regulates the flow of electrical current. When the switch is **on**, it represents a `1`. When it&#39;s **off**, it represents a `0`. This simple on/off behaviour is the foundation on which digital computing is built. Everything inside your computer, like images, sounds, and programs, can be reduced to tiny switches flipping between `0` and `1`.  They are essentially little bits of electric current. You may be asking whether this is the origin of the word “bits” in the context of digital computing. No, don&#39;t get confused; the abbreviation “bits” stands for “binary digits.”\n\n## Semiconductors\n\nUnlike the light switch on your wall, a **transistor** is microscopic, has no moving parts, and requires no human hand to operate. This is made possible by the physics of **semiconductors**. A semiconductor is a material whose electrical conductivity lies between that of an insulator and a conductor. To put it more simply, it conducts electricity better than something like plastic, but not as well as metals like copper. Silicon is one of these semiconducting materials, and the most important in modern electronics. Fun fact: about 28% of Earth&#39;s crust is silicon, so we&#39;re not running out anytime soon. Silicon atoms have four outer ([valence](https://en.wikipedia.org/wiki/Valence_electron)) electrons, allowing each atom to bond with four neighbours in a rigid, tetrahedral crystal structure. In pure silicon, nearly all electrons are locked in these bonds, so only a small fraction gains enough energy to move freely through the lattice. This limited number of mobile charges is what makes silicon a semiconductor. On its own, pure silicon isn&#39;t useful for building a transistor. Luckily for us, there is a process called **doping**. In doping, a small amount of impurity atoms (atoms of a different element) is injected, changing the electrical behaviour. There are two types of doping, N and P-type: \n\n- **N-type doping**: Atoms with three valence electrons (e.g. [boron](https://en.wikipedia.org/wiki/Boron)) are added. This creates “holes”, electron vacancies that behave like positive charge carriers.\n- **P-type doping**: Atoms with five valence electrons (e.g. [phosphorus](https://en.wikipedia.org/wiki/Phosphorus)) are added. The extra electron becomes free to move, creating an excess of negative charge carriers.\n\nTogether, N-type and P-type semiconductors form the foundation of transistors, and by combining them, we can create devices like the **MOSFET**, the tiny transistor at the heart of modern computing.\n\n![Examples of no, P-type, and N-type doping of silicon](assets/no-vs-p-vs-n-type-semiconductor-doping.jpg)\n\n## Meet the MOSFET&#39;s three players\n\nA common type of transistor used in digital circuits is the **MOSFET** (Metal-Oxide-Semiconductor Field-Effect Transistor). It has three electrical contacts:\n\n- Source (where current enters)\n- Drain (where the current exits)\n- Gate (which controls the flow)\n\nThe gate is separated from the semiconductor by a thin oxide layer, allowing it to control current without direct contact. At rest, electrons from the N-type regions naturally diffuse into the P-type region, filling holes and creating a **depletion region**. In this region, mobile charges are absent, forming a barrier that blocks current flow. This is the **off state** of the transistor. When a small positive voltage is applied to the gate, it attracts electrons toward the channel. This reduces the depletion region, allowing current to pass freely from source to drain. This is the **on state**, where the transistor acts as a closed switch. You can think of the gate like a **drawbridge**: when it&#39;s up, nothing can cross (off state). When it&#39;s lowered with a voltage, it creates a path across the channel, letting current flow (on state). However, the MOSFET wasn&#39;t always there.\n\n&gt; If you like a more visual explanation, I highly recommend watching [this](https://youtu.be/IcrBqCFLHIY?si=-onKV_bj28ybzD6Z) YouTube video by [Veritasium](https://www.youtube.com/@veritasium)\n\n![Example of a MOSFET circuit](assets/mosfet-transistor-layout.jpg)\n\n## In The Beginning\n\nIt is hard to imagine, but before transistors were a thing, early computers relied on **vacuum tubes**. Glass tubes which looked a lot like light bulbs. As the name suggests, it uses vacuums to control the flow of electrical current. The vacuum inside the tube removes all materials, even air, that could conduct electricity. The story begins on **16 November 1904,** when British electrical engineer and physicist Sir John Ambrose Fleming patented the first vacuum tube. By **1939**, they were being demonstrated for computation, and in **1946**, the famous [**ENIAC**](https://en.wikipedia.org/wiki/ENIAC) computer was built with more than **17,000 tubes**. It was ground-breaking, but also fragile. Vacuum tubes often burned out, and when one failed, it could take 15 minutes to locate and two days to replace. Not ideal if you ask me. On the upside, it made electronic computing possible for the first time in history. The next leap came in **1947**, when **John Bardeen, Walter Brattain, and William Shockley** at AT&amp;T&#39;s Bell Labs invented the first working **transistor**. Smaller, faster, and more reliable than vacuum tubes. Then, between **1955 and 1960**, Bell Labs revolutionised and invented the **MOSFET**. This design became the backbone of microchips, and it still powers every smartphone, laptop, and server in use today.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/1-transistor.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;79c2956faedb3f5e&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/1-transistor&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/3-alu.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;how-alu-works-arithmetic-logic-unit-explained&quot;],&quot;title&quot;:[0,&quot;How an ALU Works: The Heart of CPU Architecture Explained&quot;],&quot;description&quot;:[0,&quot;Don&#39;t just read about the ALU, learn how to build one. Let&#39;s break down the Arithmetic Logic Unit into 1-bit circuits using adders, multiplexers, and logic gates. Perfect for students of computer architecture.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,3],&quot;shortname&quot;:[0,&quot;ALU&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-11-02T00:00:00.000Z&quot;],&quot;update&quot;:[3,&quot;2026-02-08T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;],[0,&quot;Electrical Engineering&quot;],[0,&quot;Logic&quot;]]],&quot;tags&quot;:[1,[[0,&quot;ALU&quot;],[0,&quot;CPU Architecture&quot;],[0,&quot;Multiplexer&quot;],[0,&quot;Binary Arithmetic&quot;],[0,&quot;Logic Gates&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how an ALU works&quot;],[0,&quot;arithmetic logic unit logic gates&quot;],[0,&quot;1-bit ALU design tutorial&quot;],[0,&quot;cpu control unit vs alu&quot;],[0,&quot;binary adder subtractor circuit&quot;],[0,&quot;multiplexer in cpu architecture&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;steps&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Receive Input Data&quot;],&quot;text&quot;:[0,&quot;Binary data is pulled from CPU registers and fed into the ALU&#39;s input lines (A and B).&quot;]}],[0,{&quot;name&quot;:[0,&quot;Apply Control Signals&quot;],&quot;text&quot;:[0,&quot;The Control Unit sends an opcode (binary signal) to define the operation, such as ADD, SUB, or AND.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Parallel Processing&quot;],&quot;text&quot;:[0,&quot;All internal circuits (Adders, Logic Gates) calculate their results simultaneously.&quot;]}],[0,{&quot;name&quot;:[0,&quot;Output Selection&quot;],&quot;text&quot;:[0,&quot;A multiplexer (MUX) uses the control signal to select exactly one result and forward it to the output.&quot;]}]]]}],&quot;body&quot;:[0,&quot;In the previous chapter, we constructed circuits that could add numbers together, starting from a simple **Half-Adder** and progressing to an **8-bit Adder**. That was our first glimpse into how arithmetic happens inside a computer. But adders are just the beginning. Computers need a dedicated component to perform not only arithmetic but also logical decisions. This is where the **Arithmetic Logic Unit**, or **ALU**, comes in.\n\n## Arithmetic… what?\n\nAs mentioned earlier, the 8-bit adder was our first look at how arithmetic works inside a computer. However, a computer needs to do much more than add two numbers. With some clever circuitry, we can extend our 8-bit adder to create an 8-bit Subtractor. If we fix the B input of both the adder and subtractor to 00000001, we effectively create an incrementer and a decrementer, respectively.\n\nInside the CPU, all these circuits live together inside a special component called the **Arithmetic Logic Unit (ALU)**. Whenever your computer needs to perform arithmetic operations or make logical decisions, the ALU is the one doing the work. You can think of it as the calculator at the heart of the processor, but one that also understands logic.\n\nAt its core, an ALU is a circuit that takes **inputs**, performs an **operation**, and produces an **output**. The inputs usually come from CPU registers, and the operation performed depends on what the program instructs. The result is then sent back to a register or to memory for later use.\n\nWhat makes the ALU powerful is its **flexibility**. The same hardware can perform many different tasks depending on the **control signals** it receives. Depending on the control signals it receives, the ALU might add two numbers one moment, compare them the next, or even shift bits left or right.\n\nModern ALUs are often divided into two main parts:\n\n- **Arithmetic Unit (AU):** Handles mathematical operations such as addition, subtraction, multiplication, and division.\n- **Logic Unit (LU):** Handles logical operations such as AND, OR, XOR, and NOT.\n\nSome CPUs take this even further, including separate arithmetic units for different types of data. For example, one unit might handle **fixed-point arithmetic** (integers), while another handles **floating-point arithmetic** (real numbers used in scientific or graphics calculations).\n\n## Arithmetic\n\nLet’s take a closer look at how the circuits inside the ALU perform arithmetic. We already know from the previous chapter how an 8-bit adder works, and how to build incrementers and decrementers from it. But we haven’t yet seen how a **subtractor** works. Let’s start there.\n\nJust like before, we begin small, with the **Half-Subtractor**. The Half-Subtractor is similar to the Half-Adder: it has two inputs and two outputs. However, unlike the Half-Adder, it includes a **NOT gate** before the AND gate.\n\nThe first output, referred to as the **Difference**, represents the result of subtracting B from A. The second output, the **Borrow**, indicates that when the digit being subtracted (B) is larger than the digit it’s subtracted from (A), we need to “borrow” from the next higher bit, just like in decimal subtraction.\n\nHere’s how that looks conceptually:\n\n![Animated schematic diagram of a Half-Subtractor](assets/how-a-half-subtractor-works-logic-circuit-animation.gif)\n\nThe truth table for a Half-Subtractor is as follows:\n\n| A | B | Difference | Borrow |\n| --- | --- | --- | --- |\n| 0 | 0 | 0 | 0 |\n| 1 | 0 | 1 | 0 |\n| 0 | 1 | 1 | 1 |\n| 1 | 1 | 0 | 0 |\n\nTo create the **Full-Subtractor**, we combine two Half-Subtractors. It has three inputs: **A**, **B**, and **B-in** (the borrow from a previous stage).\n\n- **A** and **B** connect to the first Half-Subtractor.\n- Its **Difference** output connects to the A input of the second Half-Subtractor, while **B-in** connects to its B input.\n- The two **Borrow** outputs are combined through an **OR gate** to produce a single **Borrow out (B-out)**.\n\nConnecting eight of these Full-Subtractors forms an **8-bit Subtractor**, which can handle multi-bit binary subtraction, just like the adder, but with borrowing instead of carrying.\n\n![Schematic diagram of Full-Subtractor](assets/full-subtractor-logic-circuit.jpg)\n\n## Logic\n\nArithmetic is only half the story; the other half is logic. The **Logic Unit** performs operations like **AND**, **OR**, **XOR**, and **NOT** directly on the binary bits of its inputs. Why does this matter? Because logic operations allow a computer to **make decisions**. For example, the ALU can compare two numbers to determine whether they are equal, greater, or less than. This information is critical for loops, conditional statements, and control flow in programs. Without logic operations, a computer could calculate, but it couldn’t **decide**.\n\n## Control Signals\n\nNow that we understand what the ALU does and how it works, one question remains: how does it know what operation to perform? The ALU doesn’t choose its operation on its own. Instead, it listens to instructions sent by the Control Unit in the form of binary codes known as opcodes (operation codes). Depending on the opcode, the ALU may perform addition, subtraction, comparison, or logical operations.\n\nHere’s an example of how different control signals might map to ALU operations:\n\n| **Opcode (Control Signal)** | **Operation** |\n| --- | --- |\n| 000 | ADD |\n| 001 | SUBTRACT |\n| 010 | AND |\n| 011 | OR |\n| 100 | XOR |\n| 101 | NOT |\n\nInside the ALU, a **multiplexer (MUX)** acts as a selector. You can think of a multiplexer as an electronic switch that selects one of many inputs and forwards it to a single output, based on control signals. Each operation (add, subtract, AND, and so on) is implemented as a separate circuit. All of them receive the same inputs and produce their results **in parallel**, but only one of these outputs is actually sent to the final result line. The MUX, controlled by the opcode, chooses which one.\n\nThis approach might seem inefficient at first; after all, why compute everything if you only need one result? But it makes the ALU incredibly fast. Since all operations are ready at once, the MUX can instantly select the required output without waiting for a circuit to “turn on.”\n\n## Building a Simple 1-bit ALU\n\nNow that we know how arithmetic and logic operations work, and how control signals tell the ALU what to do, let’s put everything together and build a simple **1-bit ALU**.\n\nA 1-bit ALU performs one operation on one pair of input bits (**A** and **B**) at a time. When we combine multiple 1-bit ALUs (usually eight, sixteen, or thirty-two of them), we get a multi-bit ALU capable of handling entire binary numbers. But just like before, we’ll start small.\n\n### The Idea\n\nOur ALU will have:\n\n- Two **inputs**: A and B\n- One **control signal**: to decide what operation to perform\n- One **output**: the result of that operation\n\nFor simplicity, let’s say our ALU only performs the following operations:\n\n| **Opcode** | **Operation** | **Description** |\n| --- | --- | --- |\n| 00 | AND | Logical AND between A and B |\n| 01 | OR | Logical OR between A and B |\n| 10 | XOR | Logical XOR between A and B |\n| 11 | ADD | Adds A and B (with carry out) |\n\nIn circuit form, we’ll have **four separate mini-circuits,** one for each operation. The outputs from all four will feed into a **multiplexer (MUX)**. The MUX then selects which output to send to the final result, based on the control signal.\n\n![Schematic of a simple 1 bit ALU](assets/1-bit-alu-logic-circuit.jpg)\n\n### How it works step-by-step\n\n1. **Inputs arrive:** Two bits, A and B, are fed into the ALU.\n2. **Each mini-circuit does its job:**\n    - The **AND gate** outputs `A AND B`.\n    - The **OR gate** outputs `A OR B`.\n    - The XOR gate outputs `A XOR B`.\n    - The **adder** calculates `A + B`.\n3. **Opcode is set:** The CPU sends a 2-bit control signal telling the ALU which operation to use.\n4. **MUX selects output:** The multiplexer chooses the correct result and passes it to the output line.\n\nHere’s an example of how it behaves:\n\n| **A** | **B** | **Control** | **Operation** | **Result** |\n| --- | --- | --- | --- | --- |\n| 0 | 1 | 00 | AND | 0 |\n| 0 | 1 | 01 | OR | 1 |\n| 1 | 1 | 10 | XOR | 0 |\n| 0 | 1 | 11 | ADD | 1 |\n\n## Summary\n\n- The **ALU** is the part of the CPU that performs both arithmetic and logic operations.\n- It consists of an **Arithmetic Unit (AU)** and a **Logic Unit (LU)**.\n- **Control signals** from the control unit tell the ALU which operation to perform.\n- A **multiplexer (MUX)** selects the correct operation result.\n\nIn the next chapter, we’ll meet the **Control Unit,** the “brain” of the CPU that tells the ALU and every other component exactly what to do and when to do it.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/3-alu.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;2bdcbb414de79fc4&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/3-alu&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/4-control-unit.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;how-cpu-control-unit-works-decoding-microcode&quot;],&quot;title&quot;:[0,&quot;The Control Unit: How CPUs Decode and Execute Instructions&quot;],&quot;description&quot;:[0,&quot;Learn how the CPU Control Unit acts as a conductor. Explore the differences between hardwired and microprogrammed control, and how instruction decoding works.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,4],&quot;shortname&quot;:[0,&quot;Control Unit&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-11-16T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Control Unit&quot;],[0,&quot;Instruction Decoding&quot;],[0,&quot;Microcode&quot;],[0,&quot;CPU Clock&quot;],[0,&quot;Fetch-Decode-Execute&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how a control unit works&quot;],[0,&quot;hardwired vs microprogrammed control unit&quot;],[0,&quot;instruction decoder explained&quot;],[0,&quot;fetch decode execute cycle basics&quot;],[0,&quot;CPU microcode vs hardware&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-11-16T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In the previous chapter, we explored the **Arithmetic Logic Unit (ALU)**, the part of the CPU responsible for performing arithmetic and logic operations. We saw how it can add, subtract, compare, and make decisions depending on the **control signals** it receives. But this raises an important question: *where do those control signals come from?* That’s the job of the **Control Unit (CU),** the CPU’s internal **orchestrator**.\n\n## The Control Unit’s Role\n\nIf we think of the CPU as a small orchestra, the **ALU** would be the musician, performing the actual notes (the operations). The **Control Unit**, on the other hand, is the **conductor**, ensuring that every part of the CPU works in perfect timing.\n\nThe Control Unit doesn’t do the math itself, but it **directs** all the components that do. It tells the ALU when to add or subtract, instructs the registers when to store or load data, and coordinates with memory to fetch new instructions.\n\nIn short, the Control Unit:\n\n- **Fetches** instructions from memory\n- **Decodes** what those instructions mean\n- **Directs** the ALU, registers, and memory to carry them out\n- **Repeats** this cycle continuously\n\nThis ongoing rhythm is known as the **Fetch–Decode–Execute cycle**. We’ll explore this cycle step by step in **Chapter 6**, once we’ve seen how the rest of the CPU fits together.\n\n## Hardwired vs. Microprogrammed Control\n\nNot all Control Units are built the same way. Broadly speaking, there are two main design approaches:\n\n### 1. Hardwired Control\n\nIn a **hardwired control unit**, control signals are generated directly by fixed electronic circuits made up of logic gates, flip-flops, and decoders. Each instruction’s behaviour is physically built into the hardware wiring.\n\n- **Pros:**\n    - Speedy (no intermediate steps).\n    - Efficient for simple instruction sets.\n- **Cons:**\n    - It is difficult to modify, as changing the instruction set requires redesigning the hardware.\n    - Complex to build for large CPUs with many instructions.\n\nThis approach is common in small or specialised processors (like microcontrollers), where performance and simplicity matter more than flexibility.\n\n### 2. Microprogrammed Control\n\nIn contrast, a **microprogrammed control unit** works more like a tiny interpreter.\n\nInstead of using fixed wiring, it stores small **microinstructions** in a special memory area called the **control store**. Each microinstruction tells the CPU which control signals to activate during one small step of an operation.\n\nSo when the CPU executes a normal instruction (like `ADD A, B`), the Control Unit actually runs a short **microprogram,** a sequence of low-level steps that cause the ALU, registers, and buses to act in the right order.\n\n- **Pros:**\n    - Easier to modify or extend (you can change the microcode instead of the hardware).\n    - Ideal for complex instruction sets (like in older CISC CPUs).\n- **Cons:**\n    - Slightly slower than hardwired designs, since each instruction is interpreted internally.\n\nModern CPUs often blend both approaches; some parts are hardwired for speed, while others use microcode for flexibility. x86 processors (like those from Intel and AMD) are a perfect example of a **hybrid** design:\n\n- **Simple instructions** such as `ADD`, `SUB`, `AND`, or `MOV` are usually handled by **hardwired control**.\n    \n    These can be executed in just a few fast steps, so wiring them directly into hardware keeps performance high.\n    \n- **Complex instructions**, like `REP MOVSB` (which copies an entire block of memory) or older instructions that are kept for backwards compatibility are handled using **microcode**.\n    \n    Internally, the CPU breaks these complicated instructions into a sequence of simpler micro-operations and runs them like a tiny program.\n    \n\nThis way, the CPU gets the **speed** of hardwired control for the common operations, and the **flexibility** of microcode for everything else, without needing to redesign the hardware every time the instruction set changes.\n\n## How Instructions Are Decoded\n\nEvery program you run, from a web browser to a video game, ultimately boils down to a long list of machine instructions. Each instruction is just a binary number. The Control Unit must interpret that number to decide what to do.\n\nLet’s look at a simplified example. Suppose we have an 8-bit CPU where each instruction is one byte (8 bits):\n\n```bash\nInstruction: 0100 1010\n```\n\nThe Control Unit might divide this instruction into parts like this:\n\n| **Bits** | **Meaning** |\n| --- | --- |\n| 0100 | Operation code (Opcode) → tells the CPU what to do |\n| 1010 | Operand → tells the CPU which register or memory address to use |\n\nWhen the Control Unit reads this instruction, it **decodes** the opcode (`0100`) to determine which control signals to activate.\n\nFor example, opcode `0100` might mean “ADD,” while `0101` could mean “SUBTRACT.”\n\nIt then sends the appropriate signals:\n\n- Enable the ALU’s “add” circuit.\n- Load inputs from registers A and B\n- Store the result back into register A\n\nAll of this happens automatically, step by step, as the CPU clock ticks.\n\n## Timing and Coordination\n\nComputers work in rhythm. Every operation, from fetching data to performing arithmetic, happens in sync with the **clock**. The clock doesn’t measure time like a wall clock; instead, it provides a steady beat that synchronises all internal actions.\n\nOn each clock pulse, the Control Unit advances the CPU to its next step: reading an instruction, setting up the ALU, or writing a result back to memory. This precise timing ensures that signals don’t collide and that every operation occurs exactly when it should.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/4-control-unit.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;89eb2d298f91b42a&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/4-control-unit&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/5-memory.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;computer-memory-hierarchy-registers-cache-ram-explained&quot;],&quot;title&quot;:[0,&quot;The Memory Hierarchy: From High-Speed Registers to Long-Term Storage&quot;],&quot;description&quot;:[0,&quot;A complete guide to computer memory. Explore the trade-offs between speed and capacity in registers, CPU cache (L1/L2/L3), RAM, and virtual memory.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,5],&quot;shortname&quot;:[0,&quot;Memory&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-11-30T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;]]],&quot;tags&quot;:[1,[[0,&quot;Memory Hierarchy&quot;],[0,&quot;CPU Cache&quot;],[0,&quot;Virtual Memory&quot;],[0,&quot;RAM&quot;],[0,&quot;MMU&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;memory hierarchy explained&quot;],[0,&quot;difference between L1 L2 L3 cache&quot;],[0,&quot;how virtual memory works&quot;],[0,&quot;physical vs virtual address translation&quot;],[0,&quot;CPU registers types&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-11-30T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;In earlier chapters, we have seen how the CPU performs operations and how the Control Unit coordinates them. But a CPU alone can’t do much without a place to keep data and instructions. A running program needs numbers to add, addresses to jump to, and instructions to execute, and all of that must live somewhere, and that is memory. Not all memory is created equal. Different parts of the computer store data at different speeds and capacities, depending on how quickly the CPU needs to access it.\n\n## Registers, Cache, RAM, and Storage\n\nComputers use several different types of memory and storage. Although all of these hold data, they serve very different roles. You can imagine them as a series of “shelves” at different distances from the CPU. \n\n### Registers\n\nContinuing our shelf analogy from earlier, **registers are the shelves closest to the CPU,** so close in fact that they are part of the CPU itself. Registers are tiny, high-speed storage locations used to hold data while instructions are being executed. They store things like ALU inputs, memory addresses, counters, temporary results, and more.\n\nRegisters are extremely small, usually holding between 1 and 8 bytes depending on the CPU architecture. Likewise for the number of registers. Modern CPUs also include special, wider registers (SIMD registers) for processing many values at once, useful for multimedia and scientific computing. When we say a processor is “32-bit” or “64-bit,” we’re usually referring to the size of its **general-purpose registers,** the amount of data it can work with in a single operation.\n\nCommon Register Sizes\n\n- **8-bit registers:** Seen in older or simpler CPUs; hold a single byte.\n- **16-bit registers:** Used in early PCs and many microcontrollers.\n- **32-bit registers:** Common in older x86 CPUs and ARMv7 processors.\n- **64-bit registers:** Found in modern x86-64 and ARM64 CPUs; allow larger numbers and bigger memory addresses.\n\nCommon Types of Registers:\n\n- **Program Counter (PC) / Instruction Pointer (IP):** Holds the address of the next instruction to fetch.\n- **Instruction Register (IR):** Stores the instruction currently being decoded or executed.\n- **Address Registers:** Hold memory addresses used for load/store operations.\n- **Stack Pointer (SP):** Points to the top of the stack used for function calls and temporary data.\n- **Data Registers (DR):** Store data loaded from memory or I/O devices.\n- **Accumulator (ACC):** A special register used for arithmetic; historically very important, still present conceptually.\n- **General-Purpose Registers (GPRs):** Flexible registers used for calculations and data manipulation (R0, R1, …).\n- **Status / Flags Register (SR):** Stores condition bits (Zero, Carry, Overflow, etc.) that reflect the result of ALU operations.\n\n&gt; See [Hello World in x86 Assembly: Step-by-Step Guide](/posts/hello-world-x86-64-assembly-docker/) if you want to play around with registers yourself.\n&gt; \n\n### Cache\n\nCPU cache sits just outside the core, but still very close. Cache is a small, extremely fast type of memory that stores data the CPU is likely to need in the near future. Its main purpose is to reduce the time the CPU spends waiting for data from slower main memory (RAM).\n\nThink of cache as a **“middle shelf”**: not as fast as registers, but much faster than RAM and far larger than the CPU’s limited registers.\n\nModern CPUs use several layers of cache:\n\n- **L1 Cache**: The smallest and fastest level, located inside each CPU core. Often split into separate instruction and data caches (L1i and L1d).\n- **L2 Cache**: Larger but slightly slower than L1. Still per-core.\n- **L3 Cache**: Much larger and slower than L1/L2, commonly shared across all CPU cores.\n\nThe deeper the level, the farther it is from the CPU core, and the slower it gets. However, even the slowest cache is far, *far* faster than RAM.\n\nCaches work using the principle of **locality**:\n\n- **Temporal locality** – if you use data once, you’ll probably use it again soon.\n- **Spatial locality** – if you use data at one address, nearby data is likely needed too.\n\nThe CPU automatically loads data into the cache based on these patterns, so the next time it needs that data, it&#39;s already nearby.\n\n### **RAM (Main Memory)**\n\nIf registers are the closest shelves and cache is the middle shelf, then **RAM** (Random Access Memory) is the large set of shelves farther away but still within quick reach. RAM stores the data and programs that the CPU is currently working on. It is much larger than cache, typically measured in gigabytes, but also much slower.\n\nWhen you open an application, load a file, or run code, the operating system places that data into RAM so the CPU can access it quickly. If data is not in RAM, the CPU would have to fetch it from storage, which is slower.\n\nRAM has several key characteristics:\n\n- **Volatile:** Its contents are lost when the computer powers off.\n- **Random access:** Any memory location can be accessed directly and in roughly the same amount of time.\n- **Shared resource:** All programs running on your system compete for space in RAM.\n- **Bigger but slower:** Tens of nanoseconds (50-100) to access data (compared to the cache’s sub-nanoseconds).\n\nBecause RAM is much slower than cache, the CPU relies heavily on caching to avoid waiting. A cache “miss” forces the CPU to fetch data from RAM, and this delay can hurt performance significantly.\n\nRAM exists in different forms and technologies:\n\n- **DRAM (Dynamic RAM):** The most common type. Needs to be refreshed constantly.\n- **SDRAM (Synchronous DRAM):** Works in sync with the CPU clock.\n- **DDR (Double Data Rate SDRAM):** Modern high-speed family of RAM (DDR3, DDR4, DDR5).\n\nYou don’t need to know the hardware details to understand its role: **RAM is the CPU’s working area**, holding the data and code currently in use.\n\n### Storage (HDD, SSD, and More)\n\nIf registers are the closest shelves and RAM is the workspace, then **storage** is the large warehouse where data is kept long-term. Storage is where your computer permanently keeps files, programs, photos, operating system data, and everything else, even when the power is off.\n\nStorage is **much slower** than RAM, but it is also **much larger**, typically measured in hundreds of gigabytes or even terabytes.\n\nCommon types of storage include:\n\n- **HDD (Hard Disk Drive):** Uses spinning disks and a moving read/write head. Much slower than SSDs but still common for large, inexpensive storage.\n- **SSD (Solid-State Drive):** Has no moving parts. Much faster than HDDs, vastly improving boot times, loading times, and responsiveness.\n- **NVMe SSD:** A newer type of SSD that connects directly to the CPU via PCIe, offering extremely high read/write speeds.\n\nStorage is **non-volatile**, meaning data remains even when the computer is turned off, unlike RAM.\n\nWhenever the system needs something that’s not in RAM, it loads it from storage. If RAM fills up, the operating system may even use the storage device as “overflow,” in a process called *paging* or *swapping*, though this is much slower.\n\nStorage isn’t designed for speed; it’s designed for **capacity and long-term permanence**.\n\n## How Memory Addressing Works\n\nSo far, we’ve discussed different types of memory and storage, but how does the CPU locate specific data within all that memory? This is where **memory addressing** comes in.\n\nEvery byte in RAM has a unique identifier, known as a **memory address**. You can think of RAM as a long row of tiny mailboxes, each with its own number starting from 0. When the CPU needs data, it provides the address of the mailbox where that data lives.\n\nFor example, if the CPU wants the byte stored at address `1000`, it sends the number `1000` across the **address bus**. RAM receives this address, looks it up, and returns the correct data byte.\n\n### **Addresses Are Just Numbers**\n\nMemory addresses are simply binary numbers. A CPU with:\n\n- **32-bit addressing** can theoretically access up to 2³² bytes (4 GB) of memory\n- **64-bit addressing** can (in theory) access 2⁶⁴ bytes, an enormous number, far beyond what modern computers actually install\n\nThis is one of the big reasons 64-bit CPUs became standard: they can address far more memory than 32-bit systems.\n\n### **Addressing Words, Not Just Bytes**\n\nWhile every byte has an address, the CPU often works with bigger chunks called **words**. The word size usually matches the CPU’s register size, which is why “32-bit” and “64-bit” processors get their names, so:\n\n- A 32-bit CPU uses 32-bit (4-byte) words\n- A 64-bit CPU uses 64-bit (8-byte) words\n\nEven though memory is byte-addressable, the CPU might fetch a whole word at once. This is because the CPU usually operates on full words at a time, not single bytes.\n\n### **Pointers and Addresses**\n\nWhen a program stores a memory address in a variable, that variable is called a **pointer**. Instead of holding actual data, it holds the *location* of data. You’ll encounter pointers in low-level programming (like C or assembly), and understanding memory addresses is key to using them correctly.\n\n### **Sequential Instructions**\n\nInstructions stored in memory also have addresses. The **Program Counter (PC)** holds the address of the next instruction, and the CPU increases it as it executes instructions sequentially, unless a jump or branch changes it.\n\n### **How the CPU Sends and Receives Addresses**\n\nThe CPU communicates with memory through two main channels:\n\n- **Address bus:** sends *where* the data is\n- **Data bus:** sends the actual data back and forth\n\nWhen the CPU wants to read memory:\n\n1. It places an address on the address bus\n2. Memory responds by putting the requested data on the data bus\n3. The CPU reads that data\n\nWhen writing, the CPU places both the address *and* the data on the appropriate buses.\n\n### **Virtual vs. Physical Memory**\n\nModern operating systems do **not** let programs access physical RAM directly. Instead, every program sees its own private world of memory called **virtual memory**. When you see an address in a debugger like `0x00400000`, that number is **not** the true physical RAM location. It is a *virtual address* that must be translated.\n\nThis translation is handled by a small hardware unit inside the CPU called the **Memory Management Unit (MMU)**.\n\nWhy have virtual memory at all? Virtual memory gives us:\n\n- **Protection:** One program cannot read or overwrite another program.\n- **Simplicity:** Each program sees a nice, clean, continuous block of memory.\n- **Flexibility:** Programs can use more memory than physically installed (thanks to paging).\n- **Sharing:** System libraries can be shared between programs without copying them.\n\n### How the Mapping Works\n\nTo avoid translating billions of individual bytes, the OS divides memory into fixed-size blocks called **pages,** with a typical size of **4 KB**. Both *virtual memory* and *physical memory* are divided into these 4 KB units.\n\nSo instead of mapping:\n\n```\nVirtual → Physical\nByte 14 → Byte 214920\nByte 15 → Byte 214921\nByte 16 → Byte 214922\n```\n\nThe system maps at page granularity:\n\n```\nVirtual Page #12 → Physical Page #203\nVirtual Page #13 → Physical Page #8\nVirtual Page #14 → (not in RAM — stored on disk)\n```\n\nEach program has its own **page table**, which is a big list that tells the MMU: “If the CPU accesses virtual page X, it actually lives at physical page Y.”\n\n### What the MMU Does on Every Memory Access\n\nEvery time the CPU touches memory (load, store, fetch instruction), the MMU:\n\n1. **Takes the virtual address**\n2. **Splits it into:**\n    - a *page number*\n    - an *offset* inside the page\n3. **Looks up the page number in the page table**\n4. **Finds the physical page number**\n5. **Recombines**\n    \n    physical page number + offset\n    \n    to form the final **physical address**\n    \n6. RAM is accessed normally using that physical address.\n\nThis happens billions of times per second and is completely invisible to software. Additionally, to make these translations faster, the CPU stores recently used mappings in a tiny cache called the **TLB (Translation Lookaside Buffer)**, so most address translations never require a full page-table lookup.\n\n### Translation Example\n\nVirtual address:\n\n```\n0x00403A10\n```\n\nBreak into:\n\n- **Virtual page number** = 0x00403\n- **Offset** = 0xA10 (the location inside that 4 KB page)\n\nMMU finds:\n\n```\nVirtual page number = 0x00403  \nOffset inside the page = 0xA10\n```\n\nFinal physical address:\n\n```\nPhysical Page 0x1A2C + offset 0xA10 = 0x1A2CA10\n```\n\nThe CPU now reads or writes that physical address.\n\n### **What If a Page Isn’t in RAM? (Page Faults)**\n\nIf the MMU sees that a virtual page isn’t currently loaded in RAM, the CPU triggers a **page fault**.\n\nThe OS then:\n\n1. Pauses the program\n2. Loads the page from storage (SSD) into RAM\n3. Updates the page table\n4. Resumes the program\n\nThis is how the system gives programs more “memory” than you physically have.\n\n## Memory Hierarchy\n\nNow that we’ve seen *how* memory works at a low level, we can look at how all these different types fit together into the overall structure of a computer’s memory system. All these different kinds of memory (registers, cache, RAM, storage) form what’s known as the **memory hierarchy**. Each level trades capacity for speed:\n\n1. **Registers**: Tiny, fastest, directly inside the CPU\n2. **L1/L2/L3 Cache**: Small, fast, close to the CPU\n3. **RAM**: Large, much slower\n4. **Storage (SSD/HDD)**: **H**uge, much slower\n5. **External storage/cloud**: **G**igantic capacity, extremely slow\n\nAs we move down the hierarchy:\n\n- **Speed decreases**\n- **Size increases**\n- **Cost per byte decreases**\n\nThe CPU always tries to work with data from the **highest** (fastest) level available. If the data isn’t there, it must fetch it from a slower level in a process called a **memory access**, which takes more time the further down the hierarchy it goes.\n\nThis hierarchy is the backbone of computer performance. It allows the CPU to operate at high speed without needing gigabytes of expensive, ultra-fast memory.\n\nMuch of computer architecture, from cache design to operating systems, exists to make this hierarchy feel as fast as possible to software.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/5-memory.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;c4bb2055f4e91c23&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/5-memory&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/6-cpu.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;how-cpu-works-fetch-decode-execute-cycle&quot;],&quot;title&quot;:[0,&quot;The CPU in Action: Mastering the Fetch-Decode-Execute Cycle&quot;],&quot;description&quot;:[0,&quot;See the full picture of computer architecture. Learn how the ALU, Control Unit, and Registers coordinate to execute instructions in the Fetch-Decode-Execute cycle.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,6],&quot;shortname&quot;:[0,&quot;CPU&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-12-14T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;]]],&quot;tags&quot;:[1,[[0,&quot;CPU&quot;],[0,&quot;Instruction Cycle&quot;],[0,&quot;Fetch-Decode-Execute&quot;],[0,&quot;Machine Code&quot;],[0,&quot;System Design&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how a cpu works step by step&quot;],[0,&quot;fetch decode execute cycle explained&quot;],[0,&quot;instruction register vs program counter&quot;],[0,&quot;cpu execution walkthrough&quot;],[0,&quot;machine code execution process&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-12-14T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;Up to now, we’ve looked at the main pieces of the CPU separately: the ALU, the control unit, and the registers.\n\nIn this chapter, we bring everything together and see how they actually cooperate to run a program.\nThis is where the CPU stops being a set of parts and becomes a machine that thinks in a very mechanical way.\n\n## Bringing it all together\n\nA CPU is essentially a loop: it constantly fetches instructions from memory, interprets them, and performs the required actions.\n\nHere’s how the major components play their roles:\n\n### Registers\n\nThese are tiny, ultra-fast storage slots inside the CPU. They hold:\n\n- the current instruction (IR)\n- the next instruction’s address (PC or IP)\n- operands for the ALU\n- results produced by the ALU\n- temporary values the control unit needs\n\n### ALU (Arithmetic Logic Unit)\n\nThe ALU performs mathematical operations (like addition or subtraction) and logical operations (like AND, OR, or comparison).\n\nIt does nothing on its own; it is activated only when the control unit tells it to.\n\n### Control Unit\n\nThis is the “orchestrator.” It:\n\n- reads machine instructions\n- decodes what they mean\n- activates the right circuits (ALU, registers, memory access paths)\n- manages timing so each step happens in the correct order\n\nYou can imagine it as the conductor of an orchestra. Without it, the ALU and registers just sit there with nothing to do.\n\nWhen all three work together, the CPU becomes a general-purpose engine that can run any program expressed in machine code.\n\n## The Instruction Cycle (Fetch → Decode → Execute)\n\nEvery CPU instruction follows the same high-level process called the **instruction cycle**:\n\n### 1. Fetch\n\nThe control unit loads the next instruction from memory into the **Instruction Register (IR)**.\n\n- The **Program Counter (PC)** holds the address of that instruction.\n- After fetching, the PC is incremented to point to the next one.\n\n### 2. Decode\n\nThe control unit examines the bits of the instruction to figure out:\n\n- What operation it represents (e.g., ADD, LOAD, JUMP)\n- Which registers are involved\n- Whether a memory address or an immediate value is needed\n- What signals must be sent to the ALU, registers, and buses\n\nThis is where microcode (if used) may step in to guide complex instructions.\n\n### 3. Execute\n\nThe CPU performs the required action.\n\nThis may involve:\n\n- doing arithmetic in the ALU\n- moving data between registers\n- loading from or storing to memory\n- updating flags in the status register\n- changing the PC (for jumps and branches)\n\nSome instructions also include a **write-back** phase, where the result is stored in the destination register.\n\nAfter execution finishes, the cycle repeats with the next instruction, billions of times per second.\n\n![The CPUs fetch decode execute store cycle ](assets/fetch-decode-execute-store-cycle.jpg)\n\n## Example Walkthrough: Executing a Simple Instruction\n\nLet’s break down a simple machine instruction:\n\n```nasm\nADD R1, R2, R3\n```\n\nMeaning: R1 = R2 + R3\n\nHere’s how the CPU performs it:\n\n### Step 1: Fetch\n\n1. The PC holds the address of the `ADD` instruction in memory.\n2. The control unit loads this instruction into the Instruction Register (IR).\n3. The PC increments to point to the next instruction.\n\n### Step 2: Decode\n\nThe control unit decodes the bits stored in IR:\n\n- operation → ADD\n- source registers → R2 and R3\n- destination register → R1\n- ALU operation needed → addition\n\nThe control unit now prepares the ALU and registers.\n\n### Step 3: Execute\n\n1. The values inside registers R2 and R3 are sent to the ALU’s input lines.\n2. The ALU performs the addition.\n3. The result is placed on the CPU’s internal data bus.\n4. The control unit writes the result into register R1.\n5. The ALU updates the flags register (Zero, Carry, Overflow, etc.) if needed.\n\n### Step 4: Continue\n\nThe CPU goes back to the fetch stage, now using the updated PC to load the next instruction.\n\nThis process repeats endlessly, carrying out a program step by step.\n\n### Why This Matters\n\nUnderstanding this cycle is key to comprehending how all software works, from a simple “Hello World” program to a complex video game or operating system.\n\nEverything a computer does boils down to:\n\n1. **Fetch** an instruction\n2. **Decode** it\n3. **Execute** it\n4. Repeat\n\nAnd inside that cycle, the ALU, registers, and control unit cooperate with precise timing to turn binary instructions into meaningful work.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/6-cpu.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;3a42c69bfd8b00d2&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/6-cpu&quot;],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/7-beyond.mdx&quot;],&quot;data&quot;:[0,{&quot;urlSlug&quot;:[0,&quot;modern-computer-architecture-io-buses-multicore-gpu&quot;],&quot;title&quot;:[0,&quot;Beyond the CPU: How I/O, Buses, and Multicore Systems Work&quot;],&quot;description&quot;:[0,&quot;Explore computer architecture beyond the CPU. Learn about memory-mapped I/O, DMA, interrupts, the difference between CPUs and GPUs, and modern SoC design.&quot;],&quot;collection&quot;:[0,&quot;From Transistor to System: A Friendly Guide to Computer Architecture&quot;],&quot;chapter&quot;:[0,7],&quot;shortname&quot;:[0,&quot;Beyond&quot;],&quot;difficulty&quot;:[0,&quot;Intermediate&quot;],&quot;author&quot;:[0,&quot;Gianni&quot;],&quot;date&quot;:[3,&quot;2025-12-28T00:00:00.000Z&quot;],&quot;categories&quot;:[1,[[0,&quot;Computer Architecture&quot;]]],&quot;tags&quot;:[1,[[0,&quot;I/O Systems&quot;],[0,&quot;DMA&quot;],[0,&quot;Multicore&quot;],[0,&quot;GPU&quot;],[0,&quot;System Bus&quot;],[0,&quot;SoC&quot;]]],&quot;keywords&quot;:[1,[[0,&quot;how computer I/O works&quot;],[0,&quot;polling vs interrupts&quot;],[0,&quot;direct memory access explained&quot;],[0,&quot;CPU vs GPU architecture&quot;],[0,&quot;what is a system on a chip&quot;],[0,&quot;computer bus types&quot;]]],&quot;image&quot;:[0,{&quot;src&quot;:[0,&quot;/assets/computer-architecture-transistor-to-system-guide.BpaNqm1a.png&quot;],&quot;width&quot;:[0,1024],&quot;height&quot;:[0,1024],&quot;format&quot;:[0,&quot;png&quot;]}],&quot;alt&quot;:[0,&quot;Cover image for the blog series \&quot;From Transistor to System: A Friendly Guide to Computer Architecture,\&quot; showing various computer components in a cheerful, cartoon style&quot;],&quot;draft&quot;:[0,false],&quot;update&quot;:[3,&quot;2025-12-28T00:00:00.000Z&quot;]}],&quot;body&quot;:[0,&quot;By now, we have taken a deep look inside the CPU. We’ve seen how instructions are fetched, decoded, and executed, how registers and memory work together, and how the ALU performs calculations under the control of carefully timed signals.\n\nBut a CPU by itself is not very useful. A computer becomes powerful only when the CPU can **communicate with the outside world**, store data permanently, and share work with other processing units. In this chapter, we step beyond the CPU and look at the larger system that surrounds it.\n\n## The CPU Is Not Alone\n\nThe CPU is often called the “brain” of the computer, but even a brain cannot function in isolation. A CPU cannot directly:\n\n- Read keystrokes from a keyboard\n- Display pixels on a screen\n- Store files permanently\n- Send data over a network\n\nTo do any of these things, it must work together with other hardware components. These components are connected through communication pathways and coordinated by the operating system. Together, they form a **computer system**, not just a processor.\n\n## Input and Output (I/O) Systems\n\n**Input/Output (I/O)** refers to how a computer exchanges data with the outside world.\n\n- **Input devices** send data *into* the system (keyboard, mouse, camera, network).\n- **Output devices** receive data *from* the system (display, speakers, printer).\n- Many devices, like storage and network cards, do both.\n\n### How the CPU Talks to Devices\n\nFrom the CPU’s point of view, hardware devices look like special regions of memory. Each device exposes **control registers** that the CPU can read or write.\n\nFor example:\n\n- Writing a value to a graphics device register might change a pixel.\n- Reading a network register might return received data.\n\nThis approach is often called **memory-mapped I/O**. The CPU uses the same load and store instructions it already knows; it just talks to *devices* instead of RAM.\n\n### Polling vs. Interrupts\n\nThere are two main ways a CPU can interact with devices:\n\n- **Polling** (is like a chef walking to the front door every 30 seconds to see if a customer has arrived):\n    \n    The CPU repeatedly checks a device to see if it’s ready.\n    \n    - Simple, but inefficient.\n    - Wastes CPU time if nothing happens.\n- **Interrupts** (are like a doorbell. The chef can keep cooking until the bell rings):\n    \n    The device signals the CPU when it needs attention.\n    \n    - Much more efficient.\n    - Allows the CPU to work on other tasks meanwhile.\n\nModern systems rely heavily on interrupts to stay responsive.\n\n## Buses: The System’s Highways\n\nAll communication inside a computer happens over **buses**, shared pathways that carry signals between components.\n\nA bus typically carries three kinds of information:\n\n- **Data:** the actual values being transferred\n- **Addresses:** where the data should go. The \&quot;width\&quot; of this bus determines how much memory the CPU can \&quot;see.\&quot; For example, a 32-bit address bus can name about 4 billion unique locations, which is why many 32-bit systems are limited to around 4 GB of RAM without special techniques.\n- **Control signals:** read, write, interrupt, and timing information\n\nYou can think of buses as highways:\n\n- Wider highways move more data at once.\n- Faster highways deliver data more quickly.\n\n&gt; Did you know?: Modern smartphone and laptops like Apple’s M-series, often pack the CPU, GPU and RAM into a single **systems-on-chip** (SoC). This reduces the \&quot;highway distance\&quot; data has to travel, improving performance and energy efficiency.\n&gt; \n\n### Modern Interconnects\n\nIn modern computers, simple shared buses have largely been replaced by faster, point-to-point connections, such as:\n\n- **PCI Express (PCIe):** used for GPUs, SSDs, and expansion cards\n- **Memory buses:** dedicated links between CPU and RAM\n\nThe details are complex, but the idea is simple: **moving data efficiently is just as important as computing it.**\n\n## Peripheral Communication\n\nAny hardware device outside the CPU and main memory is called a **peripheral**.\n\nCommon peripherals include:\n\n- Storage devices\n- USB devices\n- Network interfaces\n- Graphics cards\n\n### The Role of the Operating System\n\nApplications do not talk to hardware directly. Instead, the **operating system** acts as an intermediary:\n\n- It provides **device drivers** for each type of hardware.\n- It presents a consistent interface to software.\n- It handles errors, timing, and security.\n\nThis is why the same program can run on many different machines without knowing the details of the hardware underneath.\n\n### Direct Memory Access (DMA)\n\nSome devices can transfer data directly to and from RAM without involving the CPU. This is called **Direct Memory Access (DMA)**.\n\nWith DMA:\n\n- The CPU sets up the transfer\n- The device moves the data itself\n- The CPU is interrupted when the transfer finishes\n\nDMA allows high-speed devices, like SSDs and network cards, to operate efficiently without slowing down the CPU. While the device moves data, the CPU remains free to run the OS or execute programs, which is why modern systems feel responsive even during large transfers.\n\n## Multicore Processors\n\nFor many years, CPUs became faster by increasing their clock speed. Eventually, this approach ran into physical limits related to power consumption and heat.\n\nThe solution was not one faster core, but **multiple cores**.\n\n### What Multicore Means\n\nA multicore processor contains:\n\n- Multiple independent CPU cores\n- Each has its own registers and execution units\n- Often sharing cache and main memory\n\nEach core runs its own instruction cycle, in parallel with the others.\n\n### Parallelism and Concurrency\n\n- **Concurrency** refers to managing multiple tasks simultaneously.\n- **Parallelism** means executing tasks simultaneously.\n\nMulticore processors make true parallel execution possible, but they also introduce challenges:\n\n- Coordinating shared memory\n- Avoiding race conditions\n- Keeping cores busy\n\nThese challenges are handled mostly by software (compilers, runtimes, and operating systems).\n\n## GPUs: Specialised Processors\n\nGraphics Processing Units (**GPUs**) were originally designed to draw images on screens. Over time, they evolved into powerful processors in their own right.\n\n### CPU vs. GPU\n\n- **CPU:**\n    - Few complex cores\n    - Complex control logic\n    - Excellent at decision-making and serial tasks\n- **GPU:**\n    - Thousands of simpler cores\n    - Designed for massive parallel workloads\n    - Excellent at doing the same operation on lots of data\n\nThis makes GPUs ideal for:\n\n- Graphics rendering\n- Image and video processing\n- Machine learning\n- Scientific simulations\n\nIn modern systems, the CPU often acts as a **coordinator**, handing large data-parallel tasks to the GPU.\n\n## The Modern Computer as a Team\n\nA modern computer is not a single processor working alone; it is a **team of specialised components**:\n\n- The **CPU** coordinates and executes instructions.\n- **Memory** holds active data and programs.\n- **Storage** keeps data long-term.\n- **Peripherals** handle input and output.\n- The **GPU** accelerates massively parallel workloads.\n- The **Operating System** orchestrates communication and resource sharing.\n\nEach part plays a role, and performance depends on how well they work together.\n\n![High-level diagram of a computer system illustrating how the CPU, main memory, GPU, and I/O devices communicate over a system bus.](assets/computer-system-architecture-cpu-memory-gpu-io-diagram.jpg)\n\n## Closing Thoughts\n\nWe started this journey at the lowest levels: bits, logic gates, and simple operations. From there, we built up to CPUs, memory, and instruction cycles. Now we see the bigger picture: a computer is a carefully orchestrated system designed to move and transform information efficiently.\n\nAt its core, everything still comes down to simple steps:\n\n- move data\n- perform operations\n- make decisions\n\nUnderstanding these foundations makes everything else, operating systems, compilers, graphics, and networking, easier to reason about.\n\nThe CPU may be the heart of the machine, but it is the **system around it** that brings computation to life.&quot;],&quot;filePath&quot;:[0,&quot;src/content/posts/from-transistor-to-system-a-friendly-guide-to-computer-architecture/7-beyond.mdx&quot;],&quot;assetImports&quot;:[1,[[0,&quot;./assets/computer-architecture-transistor-to-system-guide.png&quot;]]],&quot;digest&quot;:[0,&quot;d7e09a127423da84&quot;],&quot;deferredRender&quot;:[0,true],&quot;collection&quot;:[0,&quot;posts&quot;],&quot;slug&quot;:[0,&quot;from-transistor-to-system-a-friendly-guide-to-computer-architecture/7-beyond&quot;],&quot;render&quot;:[0,null]}]]]}" ssr client="only" opts="{&quot;name&quot;:&quot;SearchModal&quot;,&quot;value&quot;:true}"></astro-island> <div data-astro-transition-persist="find-me-on-the-other-side" id="cc-container"> <script type="module" src="/assets/CookieConsent.astro_astro_type_script_index_0_lang.BGS9yL8D.js"></script> <script>
                // Restore the `show--consent` class if it was present before the page swap
                document.addEventListener(
                    "astro:before-preparation",
                    (event) => {
                        const htmlClassName =
                            window.document.documentElement.className;
                        const consentClassPresent = htmlClassName.includes(
                            "show--consent",
                        )
                            ? true
                            : false;
                        window._showConsentClass = consentClassPresent;
                    },
                );

                document.addEventListener("astro:before-swap", (event) => {
                    const showConsent = window._showConsentClass
                        ? ` show--consent`
                        : "";
                    event.newDocument.documentElement.className += showConsent;
                });
            </script> </div> </body> </html>